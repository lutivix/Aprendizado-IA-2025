# ğŸŒ³ Ãrvores de DecisÃ£o: ExplicaÃ§Ã£o LÃºdica e DidÃ¡tica

**Data:** 11/11/2025  
**Tema:** Como funcionam as Ãrvores de DecisÃ£o e seus hiperparÃ¢metros

---

## ğŸ¯ O que Ã© uma Ãrvore de DecisÃ£o?

Imagine que vocÃª Ã© um **mÃ©dico veterinÃ¡rio** tentando identificar se um animal Ã© um **cachorro** ou um **gato** fazendo perguntas do tipo "sim/nÃ£o":

```
                    [Tem mais de 10kg?]
                          /        \
                       SIM          NÃƒO
                        /              \
              [Late muito?]        [Ronrona?]
                /      \              /      \
             SIM      NÃƒO          SIM      NÃƒO
              |        |            |        |
          CACHORRO  CACHORRO      GATO    GATO
```

Cada **pergunta** Ã© um **nÃ³** (node), cada **resposta** Ã© um **ramo** (branch), e cada **conclusÃ£o** Ã© uma **folha** (leaf).

---

## ğŸ® Exemplo LÃºdico: "Devo Jogar Futebol Hoje?"

Vamos criar uma Ã¡rvore de decisÃ£o para decidir se vocÃª deve jogar futebol:

### ğŸ“Š Dataset de Exemplo:

| Clima | Temperatura | Vento | Cansado? | **Jogar?** |
|-------|-------------|-------|----------|---------|
| Sol   | 25Â°C        | Fraco | NÃ£o      | âœ… SIM  |
| Chuva | 15Â°C        | Forte | NÃ£o      | âŒ NÃƒO  |
| Sol   | 30Â°C        | Fraco | Sim      | âŒ NÃƒO  |
| Nublado | 20Â°C      | Fraco | NÃ£o      | âœ… SIM  |
| Chuva | 10Â°C        | Forte | Sim      | âŒ NÃƒO  |
| Sol   | 28Â°C        | MÃ©dio | NÃ£o      | âœ… SIM  |

### ğŸŒ³ Ãrvore de DecisÃ£o Resultante:

```
                    [EstÃ¡ chovendo?]
                         /        \
                      SIM          NÃƒO
                       |             \
                   âŒ NÃƒO        [Temperatura > 28Â°C?]
                                      /           \
                                   SIM            NÃƒO
                                    |               \
                                âŒ NÃƒO          [Estou cansado?]
                                                   /         \
                                                SIM          NÃƒO
                                                 |            |
                                             âŒ NÃƒO        âœ… SIM
```

---

## ğŸ›ï¸ HiperparÃ¢metros Explicados de Forma LÃºdica

### 1ï¸âƒ£ **max_depth** (Profundidade MÃ¡xima)

**Analogia:** Quantas perguntas vocÃª pode fazer antes de tomar uma decisÃ£o?

#### Exemplo: "Escolher um Filme"

**max_depth = 1** (Muito simples - UNDERFITTING)
```
        [Ã‰ aÃ§Ã£o?]
         /     \
      SIM      NÃƒO
       |        |
   Vingadores Romance
```
âŒ **Problema:** Muito genÃ©rico! E se eu gosto de comÃ©dia de aÃ§Ã£o?

---

**max_depth = 3** (Balanceado)
```
                [Ã‰ aÃ§Ã£o?]
                 /     \
              SIM      NÃƒO
               |         \
        [Tem humor?]   [Ã‰ romance?]
           /    \         /      \
        SIM    NÃƒO     SIM      NÃƒO
         |      |       |        |
      DeadPool Matrix  Titanic  Drama
```
âœ… **Ideal:** Detalhado o suficiente, mas nÃ£o exagerado!

---

**max_depth = 10** (Muito complexo - OVERFITTING)
```
[Ã‰ aÃ§Ã£o?] â†’ [Tem humor?] â†’ [Ano > 2015?] â†’ [Diretor Ã© X?] 
  â†’ [Ator principal Ã© Y?] â†’ [OrÃ§amento > 100M?] â†’ ...
```
âŒ **Problema:** TÃ£o especÃ­fico que sÃ³ funciona para os filmes que vocÃª JÃ viu! NÃ£o generaliza para filmes novos.

#### ğŸ“Š ComparaÃ§Ã£o Visual:

```
max_depth = 1:  ğŸŒ± (plantinha)     â†’ Simples demais
max_depth = 5:  ğŸŒ³ (Ã¡rvore)        â†’ Equilibrado âœ…
max_depth = 20: ğŸŒ´ğŸŒ¿ğŸƒ (floresta) â†’ Complexo demais
```

---

### 2ï¸âƒ£ **min_samples_split** (MÃ­nimo de Amostras para Dividir)

**Analogia:** Quantas pessoas devem votar antes de vocÃª fazer uma nova pergunta?

#### Exemplo: "Escolher MÃºsica na Festa"

VocÃª tem 100 pessoas na festa e quer escolher mÃºsica.

**min_samples_split = 2** (Divide muito!)
```
[Rock?] â†’ [Hard Rock?] â†’ [Metal progressivo?] â†’ [Death Metal sueco?]
```
âŒ **Problema:** VocÃª faz perguntas tÃ£o especÃ­ficas que cada pessoa tem seu prÃ³prio gÃªnero! Festa vira um caos.

---

**min_samples_split = 20** (Balanceado)
```
[Rock?] â†’ [Pesado ou Leve?]
          âœ… Para aqui se menos de 20 pessoas
```
âœ… **Ideal:** SÃ³ divide em subgrupos se tiver gente suficiente para justificar.

---

**min_samples_split = 50** (Muito conservador)
```
[Rock ou Pop?]
  âœ… NÃ£o divide mais (menos de 50 por grupo)
```
âš ï¸ **Problema:** Ã€s vezes, deixa grupos muito genÃ©ricos (underfitting).

#### ğŸ¯ Regra de Ouro:
- Dataset pequeno (< 1000)? â†’ Use 10-20
- Dataset grande (> 10k)? â†’ Use 50-100

---

### 3ï¸âƒ£ **min_samples_leaf** (MÃ­nimo de Amostras por Folha)

**Analogia:** Quantas pessoas devem concordar com a decisÃ£o final?

#### Exemplo: "Classificar Frutas"

VocÃª tem 1000 frutas para classificar.

**min_samples_leaf = 1** (Aceita qualquer decisÃ£o!)
```
Resultado: "Esta maÃ§Ã£ Ã© especial porque tem exatamente 3 manchas e 2cm!"
```
âŒ **Problema:** DecisÃµes baseadas em frutas individuais (overfitting!). NÃ£o generaliza.

---

**min_samples_leaf = 5** (Balanceado)
```
Resultado: "MaÃ§Ã£s sÃ£o frutas vermelhas, mÃ©dias, com casca lisa"
```
âœ… **Ideal:** DecisÃ£o baseada em pelo menos 5 frutas similares.

---

**min_samples_leaf = 50** (Muito genÃ©rico)
```
Resultado: "Tudo Ã© fruta redonda"
```
âš ï¸ **Problema:** Perde detalhes importantes (underfitting).

#### ğŸ“Š Impacto Visual:

```
min_samples_leaf = 1:  ğŸ (cada fruta Ã© Ãºnica)      â†’ Overfitting
min_samples_leaf = 5:  ğŸğŸğŸğŸğŸ (grupo pequeno)  â†’ Equilibrado âœ…
min_samples_leaf = 50: ğŸx50 (grupo grande)         â†’ Underfitting
```

---

### 4ï¸âƒ£ **n_estimators** (NÃºmero de Ãrvores) - Para Random Forest

**Analogia:** Quantos especialistas vocÃª consulta antes de decidir?

#### Exemplo: "DiagnÃ³stico MÃ©dico"

**n_estimators = 1** (OpiniÃ£o de 1 mÃ©dico)
```
MÃ©dico: "Ã‰ gripe!"
```
âš ï¸ **Problema:** E se ele errou? Sem segunda opiniÃ£o.

---

**n_estimators = 10** (OpiniÃ£o de 10 mÃ©dicos)
```
8 mÃ©dicos: "Gripe"
2 mÃ©dicos: "Resfriado"
Resultado: GRIPE (maioria vota)
```
âœ… **Melhor:** Mais confiÃ¡vel, mas ainda pode melhorar.

---

**n_estimators = 100** (OpiniÃ£o de 100 mÃ©dicos)
```
85 mÃ©dicos: "Gripe"
12 mÃ©dicos: "Resfriado"
3 mÃ©dicos: "Alergia"
Resultado: GRIPE (forte consenso)
```
âœ… **Ideal:** Muito confiÃ¡vel! Mas leva mais tempo.

---

**n_estimators = 1000** (OpiniÃ£o de 1000 mÃ©dicos)
```
Resultado: Mesmo que 100, mas MUITO LENTO ğŸŒ
```
âš ï¸ **Custo-benefÃ­cio:** Ganho marginal nÃ£o compensa o tempo.

#### ğŸ“Š RelaÃ§Ã£o Performance vs Tempo:

```
n_estimators:   10    50    100   200   500   1000
Performance:    â˜…â˜…â˜†   â˜…â˜…â˜…   â˜…â˜…â˜…â˜…  â˜…â˜…â˜…â˜…  â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜…
Tempo:          âš¡    âš¡âš¡   âš¡âš¡âš¡  âš¡âš¡âš¡âš¡ ğŸŒ    ğŸŒğŸŒ
```

**Sweet spot:** 100-300 (bom equilÃ­brio)

---

### 5ï¸âƒ£ **max_features** (MÃ¡ximo de Features por Split)

**Analogia:** Quantas caracterÃ­sticas vocÃª considera em cada decisÃ£o?

#### Exemplo: "Escolher um Carro"

VocÃª tem 10 caracterÃ­sticas: cor, preÃ§o, marca, ano, potÃªncia, consumo, portas, ar-condicionado, direÃ§Ã£o, cÃ¢mbio.

**max_features = None** (Considera TODAS)
```
DecisÃ£o: Considera todos os 10 fatores a cada pergunta
```
âš ï¸ **Problema:** Ãrvores muito similares em Random Forest (menos diversidade).

---

**max_features = 'sqrt'** (âˆš10 â‰ˆ 3 features)
```
Ãrvore 1: [PreÃ§o, Marca, Ano]
Ãrvore 2: [Consumo, PotÃªncia, Portas]
Ãrvore 3: [Cor, Ar-condicionado, CÃ¢mbio]
```
âœ… **Ideal:** Cada Ã¡rvore Ã© diferente! Mais diversidade = melhor votaÃ§Ã£o.

---

**max_features = 'log2'** (logâ‚‚(10) â‰ˆ 3.3 features)
```
Similar ao sqrt, mas um pouco mais conservador
```
âœ… **Bom para:** Datasets com muitas features correlacionadas.

#### ğŸ¯ Quando usar cada um:

- **'sqrt'**: ClassificaÃ§Ã£o (padrÃ£o) âœ…
- **'log2'**: Muitas features correlacionadas
- **None**: Quando vocÃª tem poucas features (<10)

---

## ğŸ—ï¸ Como uma Ãrvore Aprende? (Passo a Passo)

### Exemplo: "Aprovar EmprÃ©stimo BancÃ¡rio"

**Dataset:**
| SalÃ¡rio | Idade | HistÃ³rico | **Aprovado?** |
|---------|-------|-----------|--------------|
| Alto    | 35    | Bom       | âœ… SIM       |
| Baixo   | 25    | Ruim      | âŒ NÃƒO       |
| MÃ©dio   | 45    | Bom       | âœ… SIM       |
| Alto    | 22    | Ruim      | âŒ NÃƒO       |
| Baixo   | 55    | Bom       | âœ… SIM       |

### ğŸ” Passo 1: Escolher a Melhor Pergunta

A Ã¡rvore testa **todas** as possibilidades:
- "SalÃ¡rio > 50k?"
- "Idade > 30?"
- "HistÃ³rico = Bom?"

**CritÃ©rio:** Qual pergunta **separa melhor** os dados?

#### Medindo "SeparaÃ§Ã£o" com Gini Impurity:

```python
# Antes de dividir:
Mistura: [âœ…âœ…âœ… âŒâŒ] = 60% SIM, 40% NÃƒO
Gini = 0.48 (muito misturado! ğŸ”€)

# Depois de perguntar "HistÃ³rico = Bom?":
Ramo SIM: [âœ…âœ…âœ…] = 100% SIM â†’ Gini = 0.0 (perfeito! âœ¨)
Ramo NÃƒO: [âŒâŒ] = 100% NÃƒO â†’ Gini = 0.0 (perfeito! âœ¨)

ğŸ¯ Ganho = 0.48 - 0.0 = MÃXIMO! (melhor split possÃ­vel)
```

### ğŸ” Passo 2: Dividir os Dados

```
                [HistÃ³rico = Bom?]
                     /        \
                  SIM          NÃƒO
                   |            |
              âœ… APROVAR    âŒ NEGAR
```

### ğŸ” Passo 3: Repetir nos Sub-ramos

Se ainda houver **mistura** (Gini > 0), continua dividindo:

```
                [HistÃ³rico = Bom?]
                     /        \
                  SIM          NÃƒO
                   |            |
            [SalÃ¡rio > 50k?]  âŒ NEGAR
               /        \
            SIM         NÃƒO
             |           |
        âœ… APROVAR   [Idade > 30?]
                        /      \
                     SIM       NÃƒO
                      |         |
                 âœ… APROVAR  âŒ NEGAR
```

### ğŸ›‘ Quando Parar?

A Ã¡rvore para de crescer quando **qualquer um** acontece:

1. âœ… **Pureza total:** Todos sÃ£o SIM ou NÃƒO (Gini = 0)
2. ğŸ›ï¸ **max_depth:** Atingiu profundidade mÃ¡xima
3. ğŸ›ï¸ **min_samples_split:** NÃ£o hÃ¡ amostras suficientes para dividir
4. ğŸ›ï¸ **min_samples_leaf:** DivisÃ£o criaria folhas muito pequenas

---

## ğŸ“ RelaÃ§Ã£o entre HiperparÃ¢metros e Overfitting/Underfitting

### ğŸŒ¡ï¸ TermÃ´metro do Modelo:

```
â„ï¸ UNDERFITTING          ğŸŒ¡ï¸ IDEAL          ğŸ”¥ OVERFITTING
(Muito Simples)       (Balanceado)       (Muito Complexo)
     
     ğŸŒ±                    ğŸŒ³                  ğŸŒ´ğŸŒ¿ğŸƒ
   1 folha             10 folhas            1000 folhas
   
Erro Alto             Erro Baixo           Train: Erro 0%
Treino: 70%           Treino: 85%          Test: 60% âš ï¸
Teste: 65%            Teste: 83% âœ…        
```

### ğŸ›ï¸ Como Ajustar:

#### Se vocÃª tem UNDERFITTING (modelo simples demais):
```python
# âŒ Antes:
model = RandomForestClassifier(
    max_depth=3,          # Muito raso
    min_samples_leaf=20   # Muito conservador
)

# âœ… Depois:
model = RandomForestClassifier(
    max_depth=10,         # â†‘ Mais profundo
    min_samples_leaf=5    # â†“ Menos restritivo
)
```

#### Se vocÃª tem OVERFITTING (modelo complexo demais):
```python
# âŒ Antes:
model = RandomForestClassifier(
    max_depth=None,       # Sem limite
    min_samples_leaf=1    # Aceita tudo
)

# âœ… Depois:
model = RandomForestClassifier(
    max_depth=15,         # â†“ Limitar profundidade
    min_samples_leaf=5,   # â†‘ Mais amostras por folha
    min_samples_split=10  # â†‘ Mais amostras para dividir
)
```

---

## ğŸ® Exemplo Completo: "Prever se Aluno Passa de Ano"

### ğŸ“Š Dataset:

| Horas Estudo/dia | Faltas | Nota Anterior | **Passou?** |
|------------------|--------|---------------|-------------|
| 5                | 2      | 8.5           | âœ… SIM      |
| 1                | 10     | 5.0           | âŒ NÃƒO      |
| 3                | 5      | 7.0           | âœ… SIM      |
| 0.5              | 15     | 4.0           | âŒ NÃƒO      |
| 4                | 3      | 8.0           | âœ… SIM      |
| 2                | 8      | 6.0           | âŒ NÃƒO      |

### ğŸŒ³ Ãrvore Treinada (max_depth=2):

```
                    [Horas Estudo > 2.5?]
                         /            \
                      SIM              NÃƒO
                       |                 \
                  [Faltas < 6?]      [Nota Anterior > 5.5?]
                    /      \              /           \
                 SIM      NÃƒO          SIM           NÃƒO
                  |        |            |             |
              âœ… SIM    âŒ NÃƒO       âœ… SIM         âŒ NÃƒO
```

### ğŸ¯ InterpretaÃ§Ã£o:

1. **Primeira pergunta:** "Estuda mais de 2.5h/dia?"
   - Se SIM â†’ Bom comeÃ§o! PrÃ³xima pergunta...
   - Se NÃƒO â†’ Depende da nota anterior

2. **Segunda pergunta (se estuda):** "Faltou menos de 6 vezes?"
   - Se SIM â†’ âœ… PASSA (estudioso + presente)
   - Se NÃƒO â†’ âŒ REPROVA (estuda mas falta muito)

3. **Segunda pergunta (se nÃ£o estuda):** "Nota anterior > 5.5?"
   - Se SIM â†’ âœ… PASSA (talvez seja inteligente)
   - Se NÃƒO â†’ âŒ REPROVA (nÃ£o estuda E vai mal)

### ğŸ“Š Performance:

```python
# Modelo com hiperparÃ¢metros ruins:
model_ruim = RandomForestClassifier(max_depth=1, n_estimators=10)
# Resultado: 70% accuracy (UNDERFITTING)

# Modelo balanceado:
model_bom = RandomForestClassifier(max_depth=5, n_estimators=100)
# Resultado: 85% accuracy âœ…

# Modelo overfitted:
model_complexo = RandomForestClassifier(max_depth=None, n_estimators=500)
# Resultado: Train 98%, Test 75% (OVERFITTING)
```

---

## ğŸ“ Resumo Final: Cheat Sheet

### ğŸ¯ Valores Recomendados por Tamanho de Dataset:

| Dataset        | max_depth | min_samples_leaf | n_estimators |
|----------------|-----------|------------------|--------------|
| Pequeno (<1k)  | 5-10      | 5-10             | 100          |
| MÃ©dio (1k-10k) | 10-20     | 2-5              | 100-200      |
| Grande (>10k)  | 15-30     | 1-2              | 200-300      |

### ğŸ›ï¸ Ajuste RÃ¡pido:

#### ğŸ“‰ Seu modelo estÃ¡ RUIM no TREINO? (Underfitting)
```
âœ… Aumentar: max_depth, n_estimators
âŒ Diminuir: min_samples_leaf, min_samples_split
```

#### ğŸ“‰ Seu modelo estÃ¡ BOM no TREINO mas RUIM no TESTE? (Overfitting)
```
âŒ Diminuir: max_depth, n_estimators (ou parar de aumentar)
âœ… Aumentar: min_samples_leaf, min_samples_split
âœ… Adicionar: max_features='sqrt'
```

### ğŸ’¡ Dica de Ouro:

> **"Comece simples, aumente aos poucos!"**
>
> 1. Inicie com valores padrÃ£o
> 2. Avalie train vs test accuracy
> 3. Se underfitting â†’ â†‘ complexidade
> 4. Se overfitting â†’ â†“ complexidade
> 5. Use Grid Search para automatizar

---

## ğŸ”— Recursos Adicionais

- ğŸ“˜ **Visualizar Ã¡rvores:** Use `plot_tree()` do scikit-learn
- ğŸ® **Interactive decision tree:** [R2D3 Visual Intro](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
- ğŸ“Š **Curso recomendado:** Veja notebook `01-ml-supervisionado-avancado.ipynb`

---

**Criado por:** GitHub Copilot  
**Data:** 11/11/2025  
**Projeto:** Aprendizado-IA-2025 - Semana 3

# ğŸ”§ Feature Engineering na PrÃ¡tica

**Semana 4 - Dia 2**  
**Objetivo:** Dominar tÃ©cnicas de transformaÃ§Ã£o de dados e criaÃ§Ã£o de features  
**Data:** 10 Dez 2025

---

## ğŸ“‹ Ãndice

1. [O que Ã© Feature Engineering?](#o-que-Ã©-feature-engineering)
2. [Por que Feature Engineering Importa?](#por-que-feature-engineering-importa)
3. [Tipos de Features](#tipos-de-features)
4. [TÃ©cnicas de Feature Engineering](#tÃ©cnicas-de-feature-engineering)
5. [TransformaÃ§Ãµes NumÃ©ricas](#transformaÃ§Ãµes-numÃ©ricas)
6. [TransformaÃ§Ãµes CategÃ³ricas](#transformaÃ§Ãµes-categÃ³ricas)
7. [Feature Creation](#feature-creation)
8. [Feature Selection](#feature-selection)
9. [Boas PrÃ¡ticas](#boas-prÃ¡ticas)
10. [Armadilhas Comuns](#armadilhas-comuns)
11. [Casos PrÃ¡ticos](#casos-prÃ¡ticos)
12. [Checklist de Feature Engineering](#checklist-de-feature-engineering)

---

## ğŸ¯ O que Ã© Feature Engineering?

**Feature Engineering** Ã© o processo de usar o conhecimento de domÃ­nio para criar, transformar ou selecionar variÃ¡veis (features) que tornam os algoritmos de ML mais eficazes.

### ğŸ“Š Analogia

Imagine que vocÃª estÃ¡ preparando ingredientes para uma receita:
- **Dados brutos** = ingredientes na feira
- **Feature Engineering** = lavar, descascar, picar, temperar
- **Modelo ML** = cozinhar
- **Resultado** = prato final

> "Features melhores > Algoritmos melhores"
> 
> â€“ Andrew Ng

### ğŸ” Exemplo PrÃ¡tico

**Dado bruto:**
```
data_compra = "2025-12-10 14:30:00"
```

**Features criadas:**
```
ano = 2025
mes = 12
dia_semana = "terÃ§a"
hora = 14
periodo = "tarde"
fim_semana = False
fim_mes = True
```

---

## ğŸ’¡ Por que Feature Engineering Importa?

### ğŸ“ˆ Impacto Real

| Aspecto | Sem FE | Com FE | Melhoria |
|---------|--------|--------|----------|
| **Accuracy** | 75% | 89% | +14% |
| **Training Time** | 10 min | 3 min | -70% |
| **Interpretabilidade** | Baixa | Alta | +++++ |
| **GeneralizaÃ§Ã£o** | Regular | Boa | +++++ |

### ğŸ¯ BenefÃ­cios

1. **Performance:** Modelos mais precisos
2. **Velocidade:** Treinamento mais rÃ¡pido
3. **Interpretabilidade:** Features mais claras
4. **GeneralizaÃ§Ã£o:** Melhor em dados novos
5. **Simplicidade:** Modelos mais simples funcionam melhor

---

## ğŸ·ï¸ Tipos de Features

### 1ï¸âƒ£ Features NumÃ©ricas

**ContÃ­nuas:**
- Valores em escala contÃ­nua
- Exemplo: peso, altura, temperatura, preÃ§o

**Discretas:**
- Valores inteiros
- Exemplo: idade, nÃºmero de filhos, quantidade de produtos

### 2ï¸âƒ£ Features CategÃ³ricas

**Nominais:**
- Sem ordem natural
- Exemplo: cor, cidade, categoria de produto

**Ordinais:**
- Com ordem natural
- Exemplo: tamanho (P, M, G), nÃ­vel de escolaridade

### 3ï¸âƒ£ Features Temporais

- Datas e horÃ¡rios
- Exemplo: data_compra, timestamp

### 4ï¸âƒ£ Features de Texto

- Texto livre
- Exemplo: comentÃ¡rios, descriÃ§Ãµes, reviews

### 5ï¸âƒ£ Features Booleanas

- Verdadeiro/Falso
- Exemplo: Ã©_cliente_vip, aceita_marketing

---

## ğŸ› ï¸ TÃ©cnicas de Feature Engineering

### ğŸ“Š VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Feature Engineering             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  1. TransformaÃ§Ãµes NumÃ©ricas        â”‚
â”‚     - Scaling/Normalization         â”‚
â”‚     - Log Transform                 â”‚
â”‚     - Binning                       â”‚
â”‚                                     â”‚
â”‚  2. TransformaÃ§Ãµes CategÃ³ricas      â”‚
â”‚     - One-Hot Encoding              â”‚
â”‚     - Label Encoding                â”‚
â”‚     - Target Encoding               â”‚
â”‚                                     â”‚
â”‚  3. Feature Creation                â”‚
â”‚     - CombinaÃ§Ãµes                   â”‚
â”‚     - AggregaÃ§Ãµes                   â”‚
â”‚     - Features de Tempo             â”‚
â”‚                                     â”‚
â”‚  4. Feature Selection               â”‚
â”‚     - CorrelaÃ§Ã£o                    â”‚
â”‚     - Feature Importance            â”‚
â”‚     - Recursive Feature Elimination â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ TransformaÃ§Ãµes NumÃ©ricas

### 1ï¸âƒ£ Scaling e NormalizaÃ§Ã£o

#### **StandardScaler (Z-score)**

Transforma para mÃ©dia 0 e desvio padrÃ£o 1.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# FÃ³rmula: z = (x - Î¼) / Ïƒ
```

**Quando usar:**
- âœ… Features com distribuiÃ§Ã£o normal
- âœ… Algoritmos sensÃ­veis a escala (SVM, KNN, Redes Neurais)
- âŒ Ãrvores de decisÃ£o (nÃ£o precisam)

**Exemplo:**
```
Original: [100, 200, 150, 180]
Scaled:   [-1.5, 1.1, -0.2, 0.6]
```

---

#### **MinMaxScaler**

Transforma para range [0, 1].

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# FÃ³rmula: x_scaled = (x - min) / (max - min)
```

**Quando usar:**
- âœ… Quando precisa range especÃ­fico [0, 1]
- âœ… Redes Neurais com ativaÃ§Ã£o sigmoid/tanh
- âœ… Algoritmos de distÃ¢ncia (KNN)

**Exemplo:**
```
Original: [10, 20, 15, 18]
MinMax:   [0.0, 1.0, 0.5, 0.8]
```

---

#### **RobustScaler**

Usa mediana e IQR, resistente a outliers.

```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

# FÃ³rmula: x_scaled = (x - median) / IQR
```

**Quando usar:**
- âœ… Dados com muitos outliers
- âœ… DistribuiÃ§Ãµes nÃ£o-normais
- âœ… Dados com valores extremos

---

### 2ï¸âƒ£ TransformaÃ§Ãµes de DistribuiÃ§Ã£o

#### **Log Transform**

Reduz skewness (assimetria).

```python
import numpy as np

# Log natural
X_log = np.log1p(X)  # log(1 + x) - evita log(0)

# Log base 10
X_log10 = np.log10(X + 1)
```

**Quando usar:**
- âœ… DistribuiÃ§Ã£o muito assimÃ©trica (positiva)
- âœ… Valores em escala exponencial (salÃ¡rios, preÃ§os)
- âœ… VariÃ¢ncia cresce com mÃ©dia

**Exemplo:**
```
Original: [1, 10, 100, 1000, 10000]
Log:      [0, 1,  2,   3,    4]
```

---

#### **Box-Cox Transform**

TransformaÃ§Ã£o paramÃ©trica que encontra melhor lambda.

```python
from scipy.stats import boxcox

X_boxcox, lambda_param = boxcox(X)
```

**Quando usar:**
- âœ… Quando quer normalizar distribuiÃ§Ã£o
- âœ… Dados sÃ³ positivos
- âœ… Otimizar transformaÃ§Ã£o automaticamente

---

#### **Yeo-Johnson Transform**

Similar a Box-Cox, mas aceita valores negativos.

```python
from sklearn.preprocessing import PowerTransformer

pt = PowerTransformer(method='yeo-johnson')
X_transformed = pt.fit_transform(X)
```

**Quando usar:**
- âœ… Box-Cox mas com valores negativos
- âœ… Normalizar distribuiÃ§Ã£o geral

---

### 3ï¸âƒ£ Binning (DiscretizaÃ§Ã£o)

Transforma features contÃ­nuas em categÃ³ricas.

```python
import pandas as pd

# Binning com intervalos iguais
pd.cut(df['idade'], bins=5, labels=['Muito Jovem', 'Jovem', 'Adulto', 'Maduro', 'Idoso'])

# Binning com quantis (mesma quantidade em cada bin)
pd.qcut(df['salario'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])

# Binning manual
bins = [0, 18, 30, 50, 100]
labels = ['Menor', 'Jovem', 'Adulto', 'Senior']
pd.cut(df['idade'], bins=bins, labels=labels)
```

**Quando usar:**
- âœ… Capturar relaÃ§Ãµes nÃ£o-lineares
- âœ… Reduzir impacto de outliers
- âœ… Criar regras de negÃ³cio claras

**Exemplo:**
```
Original: [5000, 8000, 12000, 25000, 45000]
Binned:   ['Baixo', 'Baixo', 'MÃ©dio', 'Alto', 'Muito Alto']
```

---

## ğŸ·ï¸ TransformaÃ§Ãµes CategÃ³ricas

### 1ï¸âƒ£ One-Hot Encoding

Cria coluna binÃ¡ria para cada categoria.

```python
import pandas as pd

# Com Pandas
df_encoded = pd.get_dummies(df, columns=['cor', 'tamanho'])

# Com Scikit-learn
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False, drop='first')  # drop='first' evita multicolinearidade
X_encoded = encoder.fit_transform(X[['cor', 'tamanho']])
```

**Quando usar:**
- âœ… Features categÃ³ricas nominais (sem ordem)
- âœ… Poucas categorias (< 10-15)
- âœ… Todos os algoritmos ML

**Exemplo:**
```
Original:
  cor
  vermelho
  azul
  verde

One-Hot:
  cor_azul  cor_verde  cor_vermelho
     0         0          1
     1         0          0
     0         1          0
```

**âš ï¸ Cuidado:**
- Muitas categorias â†’ muitas colunas (curse of dimensionality)
- Use `drop='first'` para evitar multicolinearidade

---

### 2ï¸âƒ£ Label Encoding

Atribui nÃºmero inteiro a cada categoria.

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['tamanho_encoded'] = le.fit_transform(df['tamanho'])
```

**Quando usar:**
- âœ… Features ordinais (com ordem natural)
- âœ… Target variable (y)
- âœ… Ãrvores de decisÃ£o
- âŒ RegressÃ£o Linear (cria ordem artificial)

**Exemplo:**
```
Original: ['P', 'M', 'G', 'M', 'P']
Encoded:  [0, 1, 2, 1, 0]
```

---

### 3ï¸âƒ£ Ordinal Encoding

Similar a Label, mas vocÃª define a ordem.

```python
from sklearn.preprocessing import OrdinalEncoder

categories = [['P', 'M', 'G', 'GG']]
encoder = OrdinalEncoder(categories=categories)
df['tamanho_ord'] = encoder.fit_transform(df[['tamanho']])
```

**Quando usar:**
- âœ… Ordem Ã© importante para o modelo
- âœ… VocÃª conhece a ordem lÃ³gica

---

### 4ï¸âƒ£ Target Encoding (Mean Encoding)

Substitui categoria pela mÃ©dia do target.

```python
# Manual
target_mean = df.groupby('categoria')['target'].mean()
df['categoria_encoded'] = df['categoria'].map(target_mean)

# Com category_encoders
from category_encoders import TargetEncoder

te = TargetEncoder()
df['categoria_encoded'] = te.fit_transform(df['categoria'], df['target'])
```

**Quando usar:**
- âœ… Muitas categorias (> 15)
- âœ… Forte relaÃ§Ã£o entre categoria e target
- âš ï¸ Cuidado com overfitting!

**Exemplo:**
```
categoria  |  target  â†’  categoria_encoded
-----------+---------     -----------------
A          |    1              0.8
A          |    0              0.8
B          |    1              0.5
B          |    0              0.5
C          |    0              0.3
```

---

### 5ï¸âƒ£ Frequency Encoding

Substitui pela frequÃªncia da categoria.

```python
freq = df['categoria'].value_counts(normalize=True)
df['categoria_freq'] = df['categoria'].map(freq)
```

**Quando usar:**
- âœ… FrequÃªncia Ã© informativa
- âœ… Reduzir dimensionalidade

---

## ğŸ¨ Feature Creation

### 1ï¸âƒ£ CombinaÃ§Ãµes de Features

#### **OperaÃ§Ãµes MatemÃ¡ticas**

```python
# Criar novas features combinando existentes
df['preco_por_m2'] = df['preco'] / df['area']
df['imc'] = df['peso'] / (df['altura'] ** 2)
df['razao_receita_despesa'] = df['receita'] / df['despesa']
df['lucro'] = df['receita'] - df['custo']
```

#### **InteraÃ§Ãµes Polinomiais**

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# Cria: x1, x2, x1Â², x1*x2, x2Â²
```

**Quando usar:**
- âœ… Capturar relaÃ§Ãµes nÃ£o-lineares
- âœ… RegressÃ£o Polinomial
- âš ï¸ Aumenta muito a dimensionalidade

---

### 2ï¸âƒ£ Features de Tempo

```python
import pandas as pd

# Converter para datetime
df['data'] = pd.to_datetime(df['data'])

# Extrair componentes
df['ano'] = df['data'].dt.year
df['mes'] = df['data'].dt.month
df['dia'] = df['data'].dt.day
df['dia_semana'] = df['data'].dt.dayofweek  # 0=Segunda, 6=Domingo
df['hora'] = df['data'].dt.hour
df['minuto'] = df['data'].dt.minute

# Features derivadas
df['trimestre'] = df['data'].dt.quarter
df['dia_do_ano'] = df['data'].dt.dayofyear
df['semana_do_ano'] = df['data'].dt.isocalendar().week
df['fim_semana'] = df['dia_semana'].isin([5, 6]).astype(int)
df['fim_mes'] = df['data'].dt.is_month_end.astype(int)

# PerÃ­odos do dia
df['periodo'] = pd.cut(df['hora'], 
                       bins=[0, 6, 12, 18, 24], 
                       labels=['Madrugada', 'ManhÃ£', 'Tarde', 'Noite'])

# DiferenÃ§as temporais
df['dias_desde_primeira_compra'] = (df['data'] - df['data'].min()).dt.days
df['meses_cliente'] = ((df['data'] - df['data_cadastro']).dt.days / 30).astype(int)
```

---

### 3ï¸âƒ£ Features de AgregaÃ§Ã£o

```python
# EstatÃ­sticas por grupo
df_agg = df.groupby('cliente_id').agg({
    'valor_compra': ['mean', 'sum', 'std', 'min', 'max', 'count'],
    'desconto': 'mean',
    'data': ['min', 'max']
}).reset_index()

# Renomear colunas
df_agg.columns = ['cliente_id', 'ticket_medio', 'valor_total', 
                  'std_compras', 'min_compra', 'max_compra', 'num_compras',
                  'desconto_medio', 'primeira_compra', 'ultima_compra']

# Features derivadas
df_agg['frequencia_compra'] = (
    (df_agg['ultima_compra'] - df_agg['primeira_compra']).dt.days / df_agg['num_compras']
)
```

---

### 4ï¸âƒ£ Features de Texto

```python
# Features bÃ¡sicas de texto
df['len_descricao'] = df['descricao'].str.len()
df['num_palavras'] = df['descricao'].str.split().str.len()
df['num_caracteres_maiusculos'] = df['descricao'].str.count(r'[A-Z]')
df['tem_numero'] = df['descricao'].str.contains(r'\d').astype(int)

# TF-IDF para anÃ¡lise mais avanÃ§ada
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=100)
X_tfidf = tfidf.fit_transform(df['descricao'])
```

---

### 5ï¸âƒ£ Features GeogrÃ¡ficas

```python
import numpy as np

# DistÃ¢ncia entre dois pontos (Haversine)
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Raio da Terra em km
    
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    
    return R * c

df['distancia_centro'] = haversine(
    df['lat'], df['lon'], 
    -23.550520, -46.633308  # Coordenadas do centro (exemplo: SÃ£o Paulo)
)

# RegiÃ£o geogrÃ¡fica
df['regiao'] = pd.cut(df['lat'], bins=5, labels=['Norte', 'Nordeste', 'Centro', 'Sudeste', 'Sul'])
```

---

## ğŸ¯ Feature Selection

### 1ï¸âƒ£ CorrelaÃ§Ã£o

Remove features altamente correlacionadas.

```python
import pandas as pd
import numpy as np

# Matriz de correlaÃ§Ã£o
corr_matrix = df.corr()

# Encontrar pares com correlaÃ§Ã£o > 0.9
high_corr = []
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > 0.9:
            colname = corr_matrix.columns[i]
            high_corr.append(colname)

# Remover features correlacionadas
df_reduced = df.drop(columns=high_corr)
```

---

### 2ï¸âƒ£ Feature Importance

```python
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Treinar modelo
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Feature importance
importances = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# Selecionar top features
top_features = importances.head(10)['feature'].tolist()
X_selected = X[top_features]
```

---

### 3ï¸âƒ£ Recursive Feature Elimination (RFE)

```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# Selecionar 10 melhores features
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rfe = RFE(estimator=rf, n_features_to_select=10)
rfe.fit(X_train, y_train)

# Features selecionadas
selected_features = X_train.columns[rfe.support_].tolist()
X_selected = X_train[selected_features]
```

---

### 4ï¸âƒ£ SelectKBest

```python
from sklearn.feature_selection import SelectKBest, f_classif

# Selecionar k melhores features baseado em ANOVA F-value
selector = SelectKBest(score_func=f_classif, k=10)
X_selected = selector.fit_transform(X_train, y_train)

# Nomes das features selecionadas
selected_features = X_train.columns[selector.get_support()].tolist()
```

---

## âœ… Boas PrÃ¡ticas

### 1ï¸âƒ£ Sempre Separe Train/Test ANTES

```python
from sklearn.model_selection import train_test_split

# CERTO: Separar primeiro
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit scaler apenas no train
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Usa parÃ¢metros do train!

# ERRADO: Fit em todo dataset
scaler.fit(X)  # âŒ Data leakage!
```

---

### 2ï¸âƒ£ Use Pipelines

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# Pipeline automÃ¡tico
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier())
])

# Fit e predict automÃ¡tico
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
```

---

### 3ï¸âƒ£ ColumnTransformer para Features Mistas

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Separar colunas por tipo
numeric_features = ['idade', 'salario', 'experiencia']
categorical_features = ['cidade', 'nivel_educacao']

# Transformador
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ])

# Pipeline completo
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

pipeline.fit(X_train, y_train)
```

---

### 4ï¸âƒ£ ValidaÃ§Ã£o Cruzada com Feature Engineering

```python
from sklearn.model_selection import cross_val_score

# CV com pipeline completo
scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')
print(f"CV Accuracy: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

---

### 5ï¸âƒ£ Documente suas Features

```python
# Criar dicionÃ¡rio de features
feature_dict = {
    'preco_por_m2': 'PreÃ§o dividido pela Ã¡rea em mÂ²',
    'dias_desde_compra': 'Dias desde a Ãºltima compra',
    'ticket_medio': 'Valor mÃ©dio das compras do cliente',
    'fim_semana': '1 se sÃ¡bado/domingo, 0 caso contrÃ¡rio'
}

# Salvar metadados
import json
with open('features_metadata.json', 'w') as f:
    json.dump(feature_dict, f, indent=2)
```

---

## âš ï¸ Armadilhas Comuns

### 1ï¸âƒ£ Data Leakage

**Problema:** InformaÃ§Ã£o do futuro "vaza" para o passado.

```python
# âŒ ERRADO: Usar informaÃ§Ã£o do teste no treino
scaler.fit(X)  # Fit em todo dataset
X_scaled = scaler.transform(X)
X_train, X_test = train_test_split(X_scaled, ...)

# âœ… CERTO: Fit apenas no treino
X_train, X_test = train_test_split(X, ...)
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

---

### 2ï¸âƒ£ Target Leakage

**Problema:** Feature contÃ©m informaÃ§Ã£o sobre o target.

```python
# âŒ ERRADO: Feature que sÃ³ existe quando y=1
df['fraude_confirmada']  # SÃ³ existe apÃ³s investigaÃ§Ã£o (target)

# âœ… CERTO: Features disponÃ­veis antes da decisÃ£o
df['valor_transacao'], df['hora_transacao'], df['localizacao']
```

---

### 3ï¸âƒ£ Overfitting em Feature Engineering

**Problema:** Muitas features criam modelo complexo demais.

```python
# âŒ ERRADO: Criar centenas de features sem critÃ©rio
for col1 in df.columns:
    for col2 in df.columns:
        df[f'{col1}_x_{col2}'] = df[col1] * df[col2]  # ExplosÃ£o combinatÃ³ria!

# âœ… CERTO: Criar features com significado
df['preco_por_unidade'] = df['preco_total'] / df['quantidade']
```

---

### 4ï¸âƒ£ Ignorar Missing Values

```python
# âŒ ERRADO: Scaler quebra com NaN
X_scaled = scaler.fit_transform(X)  # Error se tem NaN

# âœ… CERTO: Tratar NaN antes
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)
X_scaled = scaler.fit_transform(X_imputed)
```

---

### 5ï¸âƒ£ NÃ£o Testar Impacto

```python
# âŒ ERRADO: Criar features sem validar
df['nova_feature'] = ...
# Assumir que melhora modelo

# âœ… CERTO: Validar com CV
from sklearn.model_selection import cross_val_score

# Sem nova feature
score_before = cross_val_score(model, X, y, cv=5).mean()

# Com nova feature
X_new = X.copy()
X_new['nova_feature'] = ...
score_after = cross_val_score(model, X_new, y, cv=5).mean()

print(f"Melhoria: {score_after - score_before:.3f}")
```

---

## ğŸ“š Casos PrÃ¡ticos

### ğŸ  Caso 1: PrevisÃ£o de PreÃ§o de ImÃ³veis

```python
import pandas as pd
import numpy as np

# Features originais
df = pd.DataFrame({
    'area': [80, 120, 100, 90],
    'quartos': [2, 3, 3, 2],
    'banheiros': [1, 2, 2, 1],
    'idade': [5, 10, 2, 8],
    'preco': [200000, 350000, 280000, 220000]
})

# Feature Engineering
df['preco_por_m2'] = df['preco'] / df['area']
df['comodos_total'] = df['quartos'] + df['banheiros']
df['razao_quartos_area'] = df['quartos'] / df['area']
df['imovel_novo'] = (df['idade'] < 3).astype(int)
df['area_log'] = np.log1p(df['area'])

# Binning idade
df['categoria_idade'] = pd.cut(df['idade'], 
                                bins=[0, 3, 7, 100], 
                                labels=['Novo', 'Seminovo', 'Antigo'])
```

**Resultado:** +15% accuracy

---

### ğŸ’³ Caso 2: DetecÃ§Ã£o de Fraude

```python
# Features temporais
df['hora'] = df['timestamp'].dt.hour
df['dia_semana'] = df['timestamp'].dt.dayofweek
df['transacao_noturna'] = (df['hora'].between(0, 6)).astype(int)

# Features de agregaÃ§Ã£o
df_user = df.groupby('user_id').agg({
    'valor': ['mean', 'std', 'max'],
    'timestamp': 'count'
}).reset_index()

df_user.columns = ['user_id', 'ticket_medio', 'std_valor', 
                   'max_valor', 'num_transacoes']

# Anomalias
df = df.merge(df_user, on='user_id')
df['valor_atipico'] = (df['valor'] > df['max_valor'] * 2).astype(int)
df['frequencia_alta'] = (df['num_transacoes'] > 50).astype(int)
```

**Resultado:** +22% precision em fraudes

---

### ğŸ›’ Caso 3: Churn de Clientes

```python
# Features de comportamento
df['dias_ultima_compra'] = (pd.Timestamp.now() - df['ultima_compra']).dt.days
df['frequencia_compra'] = df['num_compras'] / df['dias_cliente']
df['ticket_crescente'] = (df['ticket_medio_3m'] > df['ticket_medio_6m']).astype(int)

# Features de engajamento
df['abre_email'] = df['emails_abertos'] / df['emails_enviados']
df['clica_link'] = df['clicks'] / df['emails_abertos']
df['converte'] = df['compras'] / df['visitas']

# RFM (Recency, Frequency, Monetary)
df['rfm_score'] = (
    df['dias_ultima_compra'].rank(ascending=False) +
    df['num_compras'].rank() +
    df['valor_total'].rank()
) / 3
```

**Resultado:** +18% recall em churn

---

## ğŸ“‹ Checklist de Feature Engineering

### âœ… Antes de ComeÃ§ar

- [ ] Entendi o problema de negÃ³cio?
- [ ] Entendi o significado de cada feature?
- [ ] Fiz EDA (Exploratory Data Analysis)?
- [ ] Identifiquei tipos de features (numÃ©ricas, categÃ³ricas, etc.)?
- [ ] Verifiquei missing values?
- [ ] Verifiquei outliers?

### âœ… Durante Feature Engineering

- [ ] Separei train/test ANTES de qualquer transformaÃ§Ã£o?
- [ ] Tratei missing values apropriadamente?
- [ ] Escalei features numÃ©ricas (se necessÃ¡rio)?
- [ ] Encodei features categÃ³ricas corretamente?
- [ ] Criei features de tempo relevantes?
- [ ] Criei features de agregaÃ§Ã£o Ãºteis?
- [ ] Criei features de interaÃ§Ã£o (se necessÃ¡rio)?
- [ ] Documentei cada feature criada?

### âœ… Feature Selection

- [ ] Removi features altamente correlacionadas?
- [ ] Usei feature importance para selecionar?
- [ ] Validei impacto com cross-validation?
- [ ] Removi features com baixa variÃ¢ncia?

### âœ… ValidaÃ§Ã£o

- [ ] Testei modelo com e sem novas features?
- [ ] Validei que nÃ£o hÃ¡ data leakage?
- [ ] Validei que nÃ£o hÃ¡ target leakage?
- [ ] Features fazem sentido de negÃ³cio?
- [ ] Modelo generalizou bem no teste?

### âœ… ProduÃ§Ã£o

- [ ] Criei pipeline reproduzÃ­vel?
- [ ] Salvei scalers/encoders treinados?
- [ ] Documentei processo completo?
- [ ] CÃ³digo estÃ¡ versionado?

---

## ğŸ“ Resumo Executivo

### ğŸ”‘ Pontos-Chave

1. **Features > Algoritmos:** Boas features melhoram mais que trocar algoritmo
2. **ConheÃ§a seus dados:** EDA Ã© essencial antes de FE
3. **Evite data leakage:** Fit apenas em train, transform em test
4. **Use pipelines:** Automatiza e previne erros
5. **Valide tudo:** CV para medir impacto real
6. **Documente:** VocÃª (e outros) vÃ£o agradecer depois

### ğŸ“Š Impacto Real

| TÃ©cnica | Dificuldade | Impacto | Quando Usar |
|---------|-------------|---------|-------------|
| **Scaling** | FÃ¡cil | Alto | SVM, KNN, Neural Nets |
| **One-Hot Encoding** | FÃ¡cil | Alto | Features categÃ³ricas |
| **Log Transform** | FÃ¡cil | MÃ©dio | DistribuiÃ§Ãµes assimÃ©tricas |
| **Features de Tempo** | MÃ©dio | Alto | Dados temporais |
| **AggregaÃ§Ãµes** | MÃ©dio | Alto | MÃºltiplas linhas/cliente |
| **InteraÃ§Ãµes Polinomiais** | MÃ©dio | MÃ©dio | RelaÃ§Ãµes nÃ£o-lineares |
| **Target Encoding** | DifÃ­cil | Alto | Muitas categorias |
| **Feature Selection** | MÃ©dio | MÃ©dio | Muitas features |

### ğŸ¯ PrÃ³ximos Passos

1. âœ… Praticar no notebook `02-feature-engineering.ipynb`
2. âœ… Aplicar em projeto real (Dia 3)
3. âœ… Criar seu prÃ³prio checklist de FE
4. âœ… Documentar aprendizados

---

## ğŸ“š Recursos Adicionais

### ğŸ“– Leitura Recomendada

- [Feature Engineering for Machine Learning (O'Reilly)](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)
- [Scikit-learn Preprocessing Guide](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Kaggle Feature Engineering](https://www.kaggle.com/learn/feature-engineering)

### ğŸ¥ VÃ­deos

- [Feature Engineering by Andrew Ng](https://www.youtube.com/watch?v=bmjamLZ3v8A)
- [Applied ML 2020 - Feature Engineering](https://www.youtube.com/watch?v=82I5zTdS7E8)

### ğŸ› ï¸ Ferramentas

- **category_encoders:** Encoders avanÃ§ados
- **featuretools:** AutoML para feature engineering
- **tsfresh:** Features automÃ¡ticas para sÃ©ries temporais

---

## ğŸ‰ ConclusÃ£o

Feature Engineering Ã© uma **arte e ciÃªncia**:
- **Arte:** Requer criatividade e conhecimento de domÃ­nio
- **CiÃªncia:** Deve ser validado com mÃ©tricas objetivas

> "Feature engineering is the key to applied machine learning."
> 
> â€“ Pedro Domingos

**Lembre-se:**
- Comece simples, complexifique gradualmente
- Valide cada mudanÃ§a com CV
- Documente tudo
- Aprenda com cada projeto

---

**PrÃ³ximo passo:** Pratique tudo isso no notebook! ğŸš€

**Boa sorte e bom cÃ³digo!** ğŸ’ª

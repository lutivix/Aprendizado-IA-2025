# ğŸ›ï¸ Semana 3, Dia 2: Hyperparameter Tuning e Cross-Validation

**Data:** 11 de Novembro de 2025  
**DuraÃ§Ã£o:** 4-5 horas  
**Objetivo:** Otimizar modelos ML atravÃ©s de ajuste de hiperparÃ¢metros

---

## ğŸ¯ **Objetivos do Dia**

### Aprendizado
- [x] Entender o que sÃ£o hiperparÃ¢metros
- [x] Compreender Grid Search vs Random Search
- [x] Dominar tÃ©cnicas de Cross-Validation
- [x] Interpretar Learning Curves
- [x] Aplicar Pipelines ML completos
- [x] Feature Selection e PCA

### ImplementaÃ§Ã£o
- [x] Grid Search CV
- [x] Random Search CV
- [x] K-Fold Cross-Validation
- [x] Learning Curves
- [x] Pipeline completo (preprocessamento + modelo)
- [x] Feature Selection (SelectKBest, RFE)
- [x] ReduÃ§Ã£o de dimensionalidade (PCA)

### EntregÃ¡veis
- [x] Notebook completo com otimizaÃ§Ã£o de 4 modelos
- [x] ComparaÃ§Ã£o de estratÃ©gias de busca
- [x] VisualizaÃ§Ãµes de Learning Curves
- [x] Pipeline produtivo
- [x] DocumentaÃ§Ã£o lÃºdica em Markdown

---

## ğŸ“š **Conceitos TeÃ³ricos**

### 1. O que sÃ£o HiperparÃ¢metros?

**DefiniÃ§Ã£o:** ParÃ¢metros definidos ANTES do treinamento que controlam como o modelo aprende.

#### DiferenÃ§a: ParÃ¢metros vs HiperparÃ¢metros

| Aspecto | ParÃ¢metros | HiperparÃ¢metros |
|---------|-----------|-----------------|
| **DefiniÃ§Ã£o** | Aprendidos durante treino | Definidos antes do treino |
| **Exemplo (Linear)** | Coeficientes (w, b) | Taxa de aprendizado |
| **Exemplo (Ãrvore)** | Splits das Ã¡rvores | `max_depth`, `min_samples_split` |
| **OtimizaÃ§Ã£o** | Gradient descent | Grid/Random Search |
| **Controle** | AutomÃ¡tico (algoritmo) | Manual (cientista de dados) |

#### Analogia LÃºdica ğŸ®

Imagine que vocÃª estÃ¡ treinando para jogar futebol:

- **ParÃ¢metros:** TÃ©cnica de chute, posicionamento â†’ vocÃª **aprende jogando**
- **HiperparÃ¢metros:** Quantos treinos/semana, duraÃ§Ã£o dos treinos â†’ vocÃª **decide antes**

---

### 2. Principais HiperparÃ¢metros por Modelo

#### ğŸŒ² Random Forest

```python
RandomForestClassifier(
    n_estimators=100,      # Quantas Ã¡rvores (mais = melhor, mas mais lento)
    max_depth=10,          # Profundidade mÃ¡xima (limita overfitting)
    min_samples_split=20,  # MÃ­nimo de amostras para dividir nÃ³
    min_samples_leaf=5,    # MÃ­nimo de amostras em folha
    max_features='sqrt',   # Features consideradas por split
    random_state=42        # Reprodutibilidade
)
```

**Impacto visual:**
- ğŸ”¼ `max_depth` alto â†’ Overfitting (Ã¡rvores profundas decoram dados)
- ğŸ”½ `max_depth` baixo â†’ Underfitting (Ã¡rvores rasas, simples demais)

#### âš¡ XGBoost

```python
XGBClassifier(
    n_estimators=100,       # NÃºmero de boosting rounds
    learning_rate=0.1,      # Taxa de aprendizado (0.01-0.3)
    max_depth=6,            # Profundidade por Ã¡rvore
    subsample=0.8,          # FraÃ§Ã£o de amostras (0.5-1.0)
    colsample_bytree=0.8,   # FraÃ§Ã£o de features (0.3-1.0)
    gamma=0,                # RegularizaÃ§Ã£o (>0 reduz overfitting)
    reg_alpha=0,            # L1 regularization
    reg_lambda=1,           # L2 regularization
    random_state=42
)
```

**Regra de ouro:**
- ğŸ¯ `learning_rate` baixo + `n_estimators` alto = melhor generalizaÃ§Ã£o (mais lento)
- âš¡ `learning_rate` alto + `n_estimators` baixo = mais rÃ¡pido (risco de overfitting)

#### ğŸ”µ Support Vector Machine (SVM)

```python
SVC(
    C=1.0,              # RegularizaÃ§Ã£o (menor = mais regularizaÃ§Ã£o)
    kernel='rbf',       # 'linear', 'rbf', 'poly', 'sigmoid'
    gamma='scale',      # InfluÃªncia de cada ponto (auto, scale, ou valor)
    degree=3,           # Grau do polinÃ´mio (se kernel='poly')
    random_state=42
)
```

**Trade-off C:**
- ğŸ”¼ `C` alto â†’ Margem pequena, menos erros treino (overfitting)
- ğŸ”½ `C` baixo â†’ Margem grande, tolera erros (underfitting)

#### ğŸ§  Multi-Layer Perceptron (MLP)

```python
MLPClassifier(
    hidden_layer_sizes=(100, 50),  # Arquitetura: 2 camadas (100 e 50 neurÃ´nios)
    activation='relu',              # 'relu', 'tanh', 'logistic'
    solver='adam',                  # 'adam', 'sgd', 'lbfgs'
    alpha=0.0001,                   # RegularizaÃ§Ã£o L2
    learning_rate_init=0.001,       # Taxa de aprendizado inicial
    max_iter=200,                   # Ã‰pocas mÃ¡ximas
    early_stopping=True,            # Para quando nÃ£o melhora
    validation_fraction=0.1,        # Dados para validaÃ§Ã£o (se early_stopping=True)
    random_state=42
)
```

**Arquitetura:**
- `(100,)` â†’ 1 camada escondida, 100 neurÃ´nios
- `(100, 50)` â†’ 2 camadas: 100 â†’ 50 neurÃ´nios
- `(64, 32, 16)` â†’ 3 camadas: 64 â†’ 32 â†’ 16 neurÃ´nios

---

### 3. Grid Search vs Random Search

#### Grid Search (Busca Exaustiva)

**Como funciona:**
Testa TODAS as combinaÃ§Ãµes de hiperparÃ¢metros.

```python
param_grid = {
    'n_estimators': [50, 100, 200],      # 3 valores
    'max_depth': [5, 10, 15],            # 3 valores
    'min_samples_split': [2, 5, 10]      # 3 valores
}
# Total: 3 Ã— 3 Ã— 3 = 27 combinaÃ§Ãµes
```

**Vantagens:**
- âœ… Garante encontrar a melhor combinaÃ§Ã£o dentro do grid
- âœ… Mais previsÃ­vel (sabe quantas iteraÃ§Ãµes terÃ¡)

**Desvantagens:**
- âŒ Lento (cresce exponencialmente)
- âŒ Pode desperdiÃ§ar tempo em regiÃµes ruins

**Quando usar:**
- Poucos hiperparÃ¢metros (<4)
- EspaÃ§o de busca pequeno
- Quando performance Ã© crÃ­tica

---

#### Random Search (Busca AleatÃ³ria)

**Como funciona:**
Testa N combinaÃ§Ãµes ALEATÃ“RIAS de hiperparÃ¢metros.

```python
param_distributions = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10, 20]
}
# Testa 20 combinaÃ§Ãµes aleatÃ³rias (n_iter=20)
```

**Vantagens:**
- âœ… Mais rÃ¡pido (vocÃª controla n_iter)
- âœ… Explora melhor o espaÃ§o (pode encontrar combinaÃ§Ãµes nÃ£o Ã³bvias)
- âœ… Escala melhor com muitos hiperparÃ¢metros

**Desvantagens:**
- âŒ NÃ£o garante encontrar o Ã³timo global
- âŒ Pode ter sorte (ou azar) na amostragem

**Quando usar:**
- Muitos hiperparÃ¢metros (>4)
- EspaÃ§o de busca grande
- Tempo/recursos limitados

---

#### ComparaÃ§Ã£o Visual

```
Grid Search:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ âœ“   â”‚ âœ“   â”‚ âœ“   â”‚  depth=5
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ âœ“   â”‚ âœ“   â”‚ âœ“   â”‚  depth=10
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ âœ“   â”‚ âœ“   â”‚ âœ“   â”‚  depth=15
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
  n=50  n=100 n=200
(Testa TODAS: 9 combinaÃ§Ãµes)

Random Search:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚     â”‚ âœ“   â”‚     â”‚  depth=5
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ âœ“   â”‚     â”‚ âœ“   â”‚  depth=10
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚ âœ“   â”‚     â”‚  depth=15
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
  n=50  n=100 n=200
(Testa 5 aleatÃ³rias se n_iter=5)
```

---

### 4. Cross-Validation (ValidaÃ§Ã£o Cruzada)

**Problema:** Se treinar/testar sempre na mesma divisÃ£o, pode ter sorte/azar.

**SoluÃ§Ã£o:** K-Fold Cross-Validation

#### K-Fold BÃ¡sico

```
Dataset dividido em 5 partes (K=5):

Fold 1: [TEST] [TRAIN] [TRAIN] [TRAIN] [TRAIN]
Fold 2: [TRAIN] [TEST] [TRAIN] [TRAIN] [TRAIN]
Fold 3: [TRAIN] [TRAIN] [TEST] [TRAIN] [TRAIN]
Fold 4: [TRAIN] [TRAIN] [TRAIN] [TEST] [TRAIN]
Fold 5: [TRAIN] [TRAIN] [TRAIN] [TRAIN] [TEST]

Resultado final: MÃ©dia das 5 acurÃ¡cias Â± desvio padrÃ£o
```

**CÃ³digo:**
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"AcurÃ¡cia: {scores.mean():.3f} Â± {scores.std():.3f}")
```

#### Stratified K-Fold (Recomendado)

MantÃ©m a proporÃ§Ã£o de classes em cada fold.

**Exemplo:** Dataset com 70% classe 0, 30% classe 1
- Cada fold terÃ¡ ~70% classe 0 e ~30% classe 1

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
```

**Quando usar:**
- âœ… **Sempre** em classificaÃ§Ã£o desbalanceada
- âœ… Default recomendado para classificaÃ§Ã£o

---

### 5. Learning Curves (Curvas de Aprendizado)

**Objetivo:** Diagnosticar overfitting vs underfitting.

#### InterpretaÃ§Ã£o Visual

```
AcurÃ¡cia
   â†‘
100%â”‚            â•±â”€â”€â”€â”€â”€â”€â”€ Train
    â”‚          â•±
 80%â”‚    â•±â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Test
    â”‚  â•±
 60%â”‚â•±
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Tamanho do dataset
```

**CenÃ¡rios:**

#### 1. Overfitting (Sobreajuste)
```
Train: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta, estÃ¡vel)
Test:  â”€â”€â”€â”€â”€â•±â•±â•±â•±â•±â•±  (baixa, crescendo lentamente)
```
**DiagnÃ³stico:** Grande gap entre train e test  
**SoluÃ§Ã£o:** 
- RegularizaÃ§Ã£o (â†‘ `min_samples_split`, â†“ `max_depth`)
- Mais dados de treino
- Feature selection

#### 2. Underfitting (Subajuste)
```
Train: â”€â”€â”€â”€â”€â”€â”€â”€ (baixa, estÃ¡vel)
Test:  â”€â”€â”€â”€â”€â”€â”€â”€ (baixa, estÃ¡vel)
```
**DiagnÃ³stico:** Ambas acurÃ¡cias baixas  
**SoluÃ§Ã£o:**
- Modelo mais complexo (â†‘ `max_depth`, â†‘ `n_estimators`)
- Mais features
- Feature engineering

#### 3. Modelo Ideal
```
Train: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta)
Test:  â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta, prÃ³xima do train)
```
**DiagnÃ³stico:** Gap pequeno, ambas altas  
**AÃ§Ã£o:** Modelo pronto! ğŸ‰

---

### 6. Pipeline ML

**Objetivo:** Encapsular preprocessamento + modelo em um Ãºnico objeto.

**Vantagens:**
- âœ… Previne data leakage (fit apenas no train)
- âœ… CÃ³digo limpo e reutilizÃ¡vel
- âœ… FÃ¡cil de usar com GridSearchCV
- âœ… Deploy simplificado

#### Pipeline BÃ¡sico

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

pipeline = Pipeline([
    ('scaler', StandardScaler()),       # Passo 1: normalizar
    ('classifier', RandomForestClassifier())  # Passo 2: treinar
])

# Usar como um modelo normal
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
```

#### Pipeline com Grid Search

```python
param_grid = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [5, 10]
}

grid = GridSearchCV(pipeline, param_grid, cv=5)
grid.fit(X_train, y_train)
```

**Nota:** Use `nome_do_passo__parametro` para acessar hiperparÃ¢metros.

---

### 7. Feature Selection (SeleÃ§Ã£o de Features)

**Objetivo:** Remover features irrelevantes para melhorar performance e velocidade.

#### MÃ©todos Principais

##### 1. SelectKBest (Univariado)

Seleciona K melhores features baseado em testes estatÃ­sticos.

```python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=5)
X_new = selector.fit_transform(X, y)
```

**FunÃ§Ãµes de score:**
- `f_classif`: ANOVA F-value (classificaÃ§Ã£o)
- `chi2`: Chi-quadrado (dados nÃ£o-negativos)
- `mutual_info_classif`: InformaÃ§Ã£o mÃºtua

##### 2. RFE (Recursive Feature Elimination)

Elimina features recursivamente treinando o modelo.

```python
from sklearn.feature_selection import RFE

rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=5)
X_new = rfe.fit_transform(X, y)
```

**Como funciona:**
1. Treina modelo com todas features
2. Ranqueia por importÃ¢ncia
3. Remove a menos importante
4. Repete atÃ© ter K features

##### 3. Feature Importance (Ãrvores)

```python
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Manter top 5
top_features = [feature_names[i] for i in indices[:5]]
```

---

### 8. PCA (Principal Component Analysis)

**Objetivo:** Reduzir dimensionalidade mantendo variÃ¢ncia.

**Conceito:** Cria novas features (componentes) que sÃ£o combinaÃ§Ãµes lineares das originais.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)  # Reduz para 2 dimensÃµes
X_pca = pca.fit_transform(X)

print(f"VariÃ¢ncia explicada: {pca.explained_variance_ratio_}")
```

**Exemplo de Output:**
```
VariÃ¢ncia explicada: [0.65, 0.25]
```
â†’ PC1 explica 65% da variÃ¢ncia, PC2 explica 25% (90% total)

**Quando usar:**
- âœ… Muitas features correlacionadas
- âœ… VisualizaÃ§Ã£o (reduzir para 2D/3D)
- âœ… Reduzir ruÃ­do
- âŒ Perda de interpretabilidade (componentes sÃ£o abstratos)

---

## ğŸ› ï¸ **ImplementaÃ§Ã£o PrÃ¡tica**

### Estrutura do Notebook

O notebook `02-hyperparameter-tuning.ipynb` contÃ©m:

1. **Setup e Imports**
   - Bibliotecas: scikit-learn, xgboost, pandas, matplotlib, seaborn
   - ConfiguraÃ§Ã£o de visualizaÃ§Ãµes

2. **Carregamento de Dados**
   - Dataset Titanic (mesmo da Semana 2)
   - URLs com fallback (Stanford CS109 + Pandas GitHub)

3. **ExplicaÃ§Ãµes Educativas**
   - DataFrame (conceito + 7 exemplos prÃ¡ticos)
   - MÃ©todos Pandas (fillna, dropna, str, cut, qcut, apply, etc.)
   - HiperparÃ¢metros (teoria + demonstraÃ§Ã£o prÃ¡tica)

4. **Preprocessamento**
   - Feature engineering
   - Tratamento de valores faltantes
   - Encoding de variÃ¡veis categÃ³ricas
   - NormalizaÃ§Ã£o (quando necessÃ¡rio)

5. **Grid Search**
   - Random Forest otimizado
   - ComparaÃ§Ã£o com baseline
   - VisualizaÃ§Ã£o de resultados

6. **Random Search**
   - XGBoost otimizado
   - ComparaÃ§Ã£o Grid vs Random
   - Tempo de execuÃ§Ã£o

7. **Cross-Validation**
   - K-Fold comparativo
   - Stratified K-Fold
   - AnÃ¡lise de estabilidade

8. **Learning Curves**
   - DiagnÃ³stico de overfitting
   - Curvas por modelo
   - InterpretaÃ§Ã£o visual

9. **Pipeline Completo**
   - Preprocessamento + modelo
   - Grid Search com pipeline
   - ValidaÃ§Ã£o

10. **Feature Selection**
    - SelectKBest
    - RFE
    - Feature Importance
    - ComparaÃ§Ã£o de performance

11. **PCA**
    - ReduÃ§Ã£o de dimensionalidade
    - VisualizaÃ§Ã£o 2D
    - AnÃ¡lise de variÃ¢ncia explicada

12. **ComparaÃ§Ã£o Final**
    - Todos os modelos otimizados
    - Tabela comparativa
    - Melhor modelo
    - RecomendaÃ§Ãµes

---

## ğŸ“Š **Resultados Esperados**

### MÃ©tricas de Sucesso

#### Baseline (sem tuning)
- Random Forest: ~82% accuracy
- XGBoost: ~83% accuracy
- SVM: ~80% accuracy

#### ApÃ³s OtimizaÃ§Ã£o
- Random Forest: ~85% accuracy (+3%)
- XGBoost: ~87% accuracy (+4%)
- SVM: ~84% accuracy (+4%)

### Insights Principais

1. **XGBoost** geralmente vence em performance bruta
2. **Random Forest** Ã© mais rÃ¡pido de treinar
3. **SVM** precisa de normalizaÃ§Ã£o (StandardScaler)
4. **Feature selection** pode melhorar velocidade sem perder acurÃ¡cia
5. **PCA** Ãºtil para visualizaÃ§Ã£o, mas pode reduzir performance

---

## ğŸ“ **Conceitos-Chave para Dominar**

### Checklist de Aprendizado

- [ ] Diferencio parÃ¢metros de hiperparÃ¢metros
- [ ] Sei quando usar Grid vs Random Search
- [ ] Entendo o que Ã© Cross-Validation e por que usar
- [ ] Interpreto Learning Curves (overfitting vs underfitting)
- [ ] Crio Pipelines para prevenir data leakage
- [ ] Aplico feature selection para melhorar modelos
- [ ] Uso PCA para reduÃ§Ã£o de dimensionalidade
- [ ] Comparo mÃºltiplos modelos de forma justa

### Perguntas de Auto-AvaliaÃ§Ã£o

1. **Por que nÃ£o usar `scaler.fit_transform()` no test set?**
   <details>
   <summary>Ver resposta</summary>
   
   **Data leakage!** O test set representa dados futuros/desconhecidos. Se vocÃª `fit` no test, estÃ¡ usando informaÃ§Ãµes (mÃ©dia, desvio) que nÃ£o teria na produÃ§Ã£o. Use apenas `transform()` com o scaler jÃ¡ fitado no train.
   </details>

2. **Grid Search com 5 hiperparÃ¢metros (cada um com 4 valores) e CV=5. Quantos modelos sÃ£o treinados?**
   <details>
   <summary>Ver resposta</summary>
   
   **1280 modelos!**
   - CombinaÃ§Ãµes: 4^5 = 1024
   - Com CV=5: 1024 Ã— 5 = 5120 treinos (mas 1024 modelos Ãºnicos)
   
   Por isso Random Search Ã© preferÃ­vel com muitos hiperparÃ¢metros.
   </details>

3. **Quando usar feature selection?**
   <details>
   <summary>Ver resposta</summary>
   
   **Use quando:**
   - Muitas features (>50)
   - Features correlacionadas
   - Overfitting persistente
   - Velocidade Ã© crÃ­tica
   
   **Cuidado:** Pode remover features Ãºteis! Sempre valide com CV.
   </details>

---

## ğŸ”§ **Troubleshooting Comum**

### Problema 1: Grid Search muito lento

**Sintoma:** Leva horas para terminar

**SoluÃ§Ãµes:**
```python
# 1. Reduzir espaÃ§o de busca
param_grid = {
    'n_estimators': [100],  # Em vez de [50, 100, 200]
    'max_depth': [5, 10]    # Em vez de [5, 10, 15, 20]
}

# 2. Usar Random Search
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(
    model, param_distributions, 
    n_iter=20,  # Limite de iteraÃ§Ãµes
    cv=3        # Menos folds
)

# 3. Usar menos folds no CV
grid = GridSearchCV(model, param_grid, cv=3)  # Em vez de cv=5

# 4. Paralelizar
grid = GridSearchCV(model, param_grid, n_jobs=-1)  # Usa todos os cores
```

### Problema 2: Overfitting mesmo apÃ³s tuning

**Sintoma:** Train >> Test mesmo com hiperparÃ¢metros otimizados

**SoluÃ§Ãµes:**
```python
# 1. Aumentar regularizaÃ§Ã£o
RandomForestClassifier(
    max_depth=5,           # Mais raso
    min_samples_split=50,  # Mais restritivo
    min_samples_leaf=20
)

# 2. Feature selection
from sklearn.feature_selection import RFE
selector = RFE(model, n_features_to_select=10)

# 3. Mais dados (se possÃ­vel)
# 4. Ensemble de modelos simples
```

### Problema 3: PCA piora performance

**Sintoma:** AcurÃ¡cia cai apÃ³s aplicar PCA

**ExplicaÃ§Ã£o:** PCA descarta informaÃ§Ã£o (componentes com baixa variÃ¢ncia)

**SoluÃ§Ãµes:**
```python
# 1. Manter mais componentes
pca = PCA(n_components=0.95)  # 95% da variÃ¢ncia

# 2. Testar sem PCA
# PCA Ã© melhor para visualizaÃ§Ã£o que para performance

# 3. Usar PCA apenas se MUITAS features (>100)
```

---

## ğŸ“š **Recursos Adicionais**

### DocumentaÃ§Ã£o Complementar

1. **Material LÃºdico**
   - ğŸ“„ `docs/14-arvores-decisao-explicacao-ludica.md`
   - Exemplos prÃ¡ticos: futebol, filmes, frutas, carros
   - Analogias visuais com emojis

2. **Scikit-learn Docs**
   - [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
   - [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
   - [Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)

3. **Tutoriais Interativos**
   - Kaggle Learn: Feature Engineering
   - DataCamp: Hyperparameter Tuning

### Papers e Livros

- **"Random Search for Hyper-Parameter Optimization"** (Bergstra & Bengio, 2012)
  - Por que Random Search > Grid Search

- **"Hands-On Machine Learning"** (AurÃ©lien GÃ©ron)
  - CapÃ­tulos 2-3: Fine-tuning models

---

## âœ… **Checklist de ConclusÃ£o**

Antes de avanÃ§ar para o Dia 3, vocÃª deve:

- [ ] Executou todo o notebook `02-hyperparameter-tuning.ipynb`
- [ ] Entendeu a diferenÃ§a entre Grid e Random Search
- [ ] Aplicou Cross-Validation com sucesso
- [ ] Interpretou Learning Curves
- [ ] Criou pelo menos 1 Pipeline completo
- [ ] Experimentou feature selection
- [ ] Testou PCA e entendeu o trade-off
- [ ] Comparou 3+ modelos de forma justa
- [ ] Leu o material lÃºdico (`14-arvores-decisao-explicacao-ludica.md`)
- [ ] Preencheu auto-avaliaÃ§Ã£o

---

## ğŸš€ **PrÃ³ximos Passos**

### Dia 3: Dashboard React Interativo

**Objetivo:** Criar interface visual para o modelo treinado

**Tecnologias:**
- Python API (Flask/FastAPI)
- React + TypeScript
- VisualizaÃ§Ãµes D3.js/Recharts

**Preview:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Titanic Survival Predictor    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Age: [____]  Sex: [M] [F]      â”‚
â”‚  Class: [1] [2] [3]             â”‚
â”‚  Fare: [____]                   â”‚
â”‚                                  â”‚
â”‚  [PREDICT SURVIVAL]             â”‚
â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Survival Probability:   â”‚   â”‚
â”‚  â”‚        78%              â”‚   â”‚
â”‚  â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**EntregÃ¡veis:**
- API REST com modelo treinado
- Frontend React interativo
- Deploy local (opcional: cloud)

---

## ğŸ’¡ **Dicas Finais**

### EstratÃ©gia de OtimizaÃ§Ã£o

1. **Baseline primeiro**
   ```python
   # Sempre comece com padrÃµes
   model = RandomForestClassifier()
   model.fit(X_train, y_train)
   baseline_score = model.score(X_test, y_test)
   ```

2. **OtimizaÃ§Ã£o iterativa**
   ```
   1. Random Search (broad search)
      â†“
   2. Grid Search refinado (narrow search)
      â†“
   3. Manual fine-tuning
   ```

3. **ValidaÃ§Ã£o rigorosa**
   ```python
   # Use sempre CV para decisÃµes finais
   scores = cross_val_score(best_model, X, y, cv=5)
   print(f"CV Score: {scores.mean():.3f} Â± {scores.std():.3f}")
   ```

### Boas PrÃ¡ticas

```python
# âœ… BOM: ReprodutÃ­vel
GridSearchCV(model, param_grid, cv=5, random_state=42, n_jobs=-1)

# âœ… BOM: Valida com dados nÃ£o vistos
train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# âœ… BOM: Pipeline previne leakage
pipeline = Pipeline([('scaler', StandardScaler()), ('model', SVC())])

# âŒ RUIM: Sem random_state (nÃ£o reprodutÃ­vel)
RandomForestClassifier()

# âŒ RUIM: Fit scaler no test
scaler.fit_transform(X_test)
```

---

**ParabÃ©ns por completar o Dia 2! ğŸ‰**

VocÃª agora domina tÃ©cnicas profissionais de otimizaÃ§Ã£o de modelos ML. Continue praticando e explorando!

_DÃºvidas? Consulte o material lÃºdico ou refaÃ§a as seÃ§Ãµes que nÃ£o ficaram claras._

---

**Ãšltima atualizaÃ§Ã£o:** 11/11/2025  
**PrÃ³xima revisÃ£o:** ApÃ³s Dia 3  
**VersÃ£o:** 1.0

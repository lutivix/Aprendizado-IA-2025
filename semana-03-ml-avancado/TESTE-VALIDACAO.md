# ğŸ§ª Teste de ValidaÃ§Ã£o - Semana 3: ML Supervisionado AvanÃ§ado

## â±ï¸ Tempo estimado total: 40 minutos
- Dia 1 (Modelos AvanÃ§ados): 20 minutos
- Dia 2 (Hyperparameter Tuning): 20 minutos

---

# ğŸ“… DIA 1: Modelos AvanÃ§ados

## ğŸ“‹ Parte 1: Conceitos (mÃºltipla escolha)

### QuestÃ£o 1: Ensemble Methods
VocÃª tem um dataset pequeno (500 amostras) e precisa de alta acurÃ¡cia. Qual modelo seria mais adequado?

**A)** Decision Tree (Ã¡rvore Ãºnica)  
**B)** Random Forest  
**C)** XGBoost  
**D)** SVM  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Random Forest**

**Por quÃª:**
- Decision Tree: Tende a overfitting com poucos dados
- **Random Forest: Reduz overfitting usando vÃ¡rias Ã¡rvores, ideal para dados pequenos**
- XGBoost: Excelente, mas pode ser overkill e mais complexo de configurar
- SVM: Bom tambÃ©m, mas precisa de normalizaÃ§Ã£o e ajuste de hiperparÃ¢metros

**Conceito-chave:** Random Forest Ã© o "safe choice" para maioria dos problemas tabulares.
</details>

---

### QuestÃ£o 2: NormalizaÃ§Ã£o
VocÃª vai treinar 3 modelos: Random Forest, SVM e MLP. Para quais vocÃª DEVE aplicar StandardScaler?

**A)** Apenas Random Forest  
**B)** Apenas SVM  
**C)** SVM e MLP  
**D)** Todos os trÃªs  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: C) SVM e MLP**

**Por quÃª:**
- **Random Forest:** Baseado em Ã¡rvores, **nÃ£o Ã© afetado** por escala
- **SVM:** Usa distÃ¢ncias euclidianas, **sensÃ­vel Ã  escala**
- **MLP:** Usa gradientes, **sensÃ­vel Ã  escala**

**CÃ³digo correto:**
```python
# âŒ ERRADO
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
# Treinar todos os modelos com X_train_scaled

# âœ… CERTO
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Random Forest: usa X_train original
# SVM e MLP: usa X_train_scaled
```

**Conceito-chave:** Modelos baseados em distÃ¢ncia precisam de normalizaÃ§Ã£o.
</details>

---

### QuestÃ£o 3: Feature Importance
VocÃª treinou um Random Forest e a feature `age` tem importÃ¢ncia 0.35 e `fare` tem 0.05. O que isso significa?

**A)** `age` Ã© 7x mais importante que `fare`  
**B)** Devo remover `fare` do modelo  
**C)** `age` contribui 35% para as decisÃµes do modelo  
**D)** Devo normalizar `age` porque o valor Ã© muito alto  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: C) `age` contribui 35% para as decisÃµes do modelo**

**Por quÃª:**
- A) Incorreto: NÃ£o Ã© uma relaÃ§Ã£o linear direta de "vezes"
- B) Incorreto: Features com baixa importÃ¢ncia ainda podem ser Ãºteis
- **C) Correto: Feature importance soma 1.0 (100%), entÃ£o 0.35 = 35% de contribuiÃ§Ã£o**
- D) Incorreto: Valor alto de importance Ã© DESEJÃVEL, nÃ£o um problema

**Quando remover features:**
- ImportÃ¢ncia prÃ³xima de **zero** (<0.001)
- **Causa overfitting** (modelo muito complexo)
- **Custo de coleta** Ã© alto demais para o benefÃ­cio

**Conceito-chave:** Feature importance mostra contribuiÃ§Ã£o relativa, nÃ£o valor absoluto.
</details>

---

### QuestÃ£o 4: Overfitting
Seu modelo tem:
- **Train accuracy:** 98%
- **Test accuracy:** 75%

O que estÃ¡ acontecendo e como corrigir?

**A)** Underfitting - aumentar complexidade do modelo  
**B)** Overfitting - reduzir complexidade ou adicionar regularizaÃ§Ã£o  
**C)** Modelo perfeito - nenhuma aÃ§Ã£o necessÃ¡ria  
**D)** Dataset ruim - coletar mais dados  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Overfitting - reduzir complexidade ou adicionar regularizaÃ§Ã£o**

**Por quÃª:**
Grande diferenÃ§a entre train e test (23%) indica **overfitting** = modelo decorou os dados de treino.

**SoluÃ§Ãµes prÃ¡ticas:**

1. **Random Forest:**
```python
# âŒ Overfitting
RandomForestClassifier(n_estimators=200, max_depth=None)

# âœ… Melhor
RandomForestClassifier(
    n_estimators=100,
    max_depth=10,        # Limita profundidade
    min_samples_split=20 # Exige mais amostras para dividir
)
```

2. **Neural Network:**
```python
# âŒ Overfitting
MLPClassifier(hidden_layer_sizes=(200, 100, 50))

# âœ… Melhor
MLPClassifier(
    hidden_layer_sizes=(50, 25),  # Menos neurÃ´nios
    early_stopping=True,           # Para quando nÃ£o melhora
    validation_fraction=0.2        # Valida durante treino
)
```

**Conceito-chave:** Train >> Test = Overfitting. Simplificar ou regularizar.
</details>

---

## ğŸ–¥ï¸ Parte 2: PrÃ¡tica (cÃ³digo)

### Desafio: Completar o Pipeline

VocÃª tem este cÃ³digo com **3 erros**. Encontre e corrija:

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Carregar dados
df = pd.read_csv('dataset.csv')
X = df.drop('target', axis=1)
y = df['target']

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Normalizar
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)  # âŒ ERRO 1

# Treinar Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train_scaled, y_train)  # âŒ ERRO 2
y_pred_rf = rf_model.predict(X_test_scaled)

# Treinar SVM
svm_model = SVC()
svm_model.fit(X_train, y_train)  # âŒ ERRO 3
y_pred_svm = svm_model.predict(X_test_scaled)

# Avaliar
print(f"RF Accuracy: {accuracy_score(y_test, y_pred_rf)}")
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm)}")
```

<details>
<summary>ğŸ’¡ Ver resposta e explicaÃ§Ã£o</summary>

### Erros e CorreÃ§Ãµes:

#### âŒ ERRO 1: `scaler.fit_transform(X_test)`
```python
# ERRADO
X_test_scaled = scaler.fit_transform(X_test)

# CERTO
X_test_scaled = scaler.transform(X_test)
```
**Por quÃª:** `fit_transform` recalcula mÃ©dia/desvio no test set = **data leakage**!  
**Regra:** `fit` apenas no train, `transform` no test.

---

#### âŒ ERRO 2: Random Forest com dados normalizados
```python
# ERRADO
rf_model.fit(X_train_scaled, y_train)

# CERTO
rf_model.fit(X_train, y_train)  # Dados originais
```
**Por quÃª:** Random Forest nÃ£o precisa de normalizaÃ§Ã£o (baseado em thresholds, nÃ£o distÃ¢ncias).

---

#### âŒ ERRO 3: SVM sem normalizaÃ§Ã£o
```python
# ERRADO
svm_model.fit(X_train, y_train)

# CERTO
svm_model.fit(X_train_scaled, y_train)
```
**Por quÃª:** SVM Ã© **sensÃ­vel Ã  escala** das features (usa distÃ¢ncias).

---

### âœ… CÃ³digo Correto Completo:

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Carregar dados
df = pd.read_csv('dataset.csv')
X = df.drop('target', axis=1)
y = df['target']

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42  # âœ… Adicione random_state!
)

# Normalizar
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # âœ… transform, nÃ£o fit_transform

# Treinar Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)  # âœ… Dados originais
y_pred_rf = rf_model.predict(X_test)  # âœ… Dados originais

# Treinar SVM
svm_model = SVC(random_state=42)
svm_model.fit(X_train_scaled, y_train)  # âœ… Dados normalizados
y_pred_svm = svm_model.predict(X_test_scaled)  # âœ… Dados normalizados

# Avaliar
print(f"RF Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
```

### ğŸ“Š Checklist de ValidaÃ§Ã£o:

| Modelo | Dados de Treino | Dados de Teste | Por quÃª? |
|--------|----------------|----------------|----------|
| Random Forest | `X_train` | `X_test` | NÃ£o precisa normalizaÃ§Ã£o |
| XGBoost | `X_train` | `X_test` | NÃ£o precisa normalizaÃ§Ã£o |
| SVM | `X_train_scaled` | `X_test_scaled` | Precisa normalizaÃ§Ã£o |
| MLP | `X_train_scaled` | `X_test_scaled` | Precisa normalizaÃ§Ã£o |

</details>

---

## ğŸ“Š Parte 3: InterpretaÃ§Ã£o de Resultados

VocÃª treinou 4 modelos no Titanic e obteve:

| Modelo | Train Acc | Test Acc | CV Score | Tempo (s) |
|--------|-----------|----------|----------|-----------|
| Decision Tree | 98.5% | 76.2% | 75.1% Â± 3.2% | 0.05 |
| Random Forest | 95.3% | 83.8% | 82.9% Â± 2.1% | 1.2 |
| XGBoost | 96.8% | 85.1% | 84.3% Â± 1.8% | 2.5 |
| MLP | 89.2% | 81.5% | 80.2% Â± 4.5% | 3.8 |

### QuestÃ£o: Qual modelo vocÃª escolheria para produÃ§Ã£o? Por quÃª?

<details>
<summary>ğŸ’¡ Ver resposta e anÃ¡lise</summary>

### ğŸ† Resposta: **XGBoost**

### ğŸ“Š AnÃ¡lise Detalhada:

#### 1. **Decision Tree** âŒ
- **Train 98.5% vs Test 76.2%** = Overfitting severo (22.3% diferenÃ§a!)
- CV score baixo e alta variÃ¢ncia (Â±3.2%)
- **Veredicto:** NÃ£o confiÃ¡vel para produÃ§Ã£o

#### 2. **Random Forest** âœ… (2Âª opÃ§Ã£o)
- Train vs Test: 95.3% - 83.8% = 11.5% (razoÃ¡vel)
- Boa estabilidade (CV Â±2.1%)
- **RÃ¡pido** (1.2s)
- **Veredicto:** Excelente opÃ§Ã£o se tempo de inferÃªncia Ã© crÃ­tico

#### 3. **XGBoost** ğŸ† (MELHOR)
- **Melhor test accuracy:** 85.1%
- **Melhor CV score:** 84.3%
- **Menor variÃ¢ncia:** Â±1.8% (mais estÃ¡vel)
- Train vs Test: 96.8% - 85.1% = 11.7% (similar ao RF)
- Tempo aceitÃ¡vel (2.5s para treino)
- **Veredicto:** Melhor generalizaÃ§Ã£o + estabilidade

#### 4. **MLP** âš ï¸
- Performance OK (81.5%)
- **Problema:** Alta variÃ¢ncia (Â±4.5%) = instÃ¡vel
- Mais lento (3.8s)
- Train-Test gap pequeno (7.7%), mas CV ruim
- **Veredicto:** Necessita mais tuning de hiperparÃ¢metros

---

### ğŸ¯ CritÃ©rios de DecisÃ£o:

```
Escolha baseado em:

1. Test Accuracy (principal mÃ©trica)
   âœ… XGBoost: 85.1%

2. Estabilidade (CV variance)
   âœ… XGBoost: Â±1.8%

3. GeneralizaÃ§Ã£o (Train-Test gap)
   âœ… RF e XGBoost: ~11%

4. Tempo de treino/inferÃªncia
   âœ… RF: 1.2s (se crÃ­tico)

5. Interpretabilidade
   âœ… RF: Mais fÃ¡cil de explicar
```

### ğŸ’¼ DecisÃ£o para ProduÃ§Ã£o:

**CenÃ¡rio 1: Performance mÃ¡xima**
â†’ **XGBoost** (85.1% acc, mais estÃ¡vel)

**CenÃ¡rio 2: Tempo real crÃ­tico**
â†’ **Random Forest** (83.8% acc, 2x mais rÃ¡pido)

**CenÃ¡rio 3: Interpretabilidade crÃ­tica**
â†’ **Random Forest** (feature importance mais clara)

---

### âš ï¸ Red Flags a Observar:

1. **Train >> Test** (>15% diferenÃ§a) = Overfitting
2. **CV variance alta** (>3%) = Modelo instÃ¡vel
3. **Test < 80%** no Titanic = Modelo fraco
4. **Tempo > 10s** = Pode nÃ£o escalar

</details>

---

## ğŸ“ Gabarito de Auto-AvaliaÃ§Ã£o

### PontuaÃ§Ã£o:

- **Parte 1 (Conceitos):** 4 questÃµes Ã— 2 pontos = **8 pontos**
- **Parte 2 (CÃ³digo):** 3 erros Ã— 2 pontos = **6 pontos**  
- **Parte 3 (InterpretaÃ§Ã£o):** 1 questÃ£o Ã— 6 pontos = **6 pontos**

**TOTAL:** 20 pontos

---

### ğŸ“Š InterpretaÃ§Ã£o da sua nota:

#### ğŸ† 18-20 pontos: EXCELENTE!
**VocÃª estÃ¡ PRONTO para Semana 4!**

âœ… Domina os conceitos principais  
âœ… Identifica erros comuns  
âœ… Interpreta resultados corretamente  

**PrÃ³ximos passos:**
- AvanÃ§ar para Semana 4 com confianÃ§a
- Considere fazer um mini-projeto de consolidaÃ§Ã£o (opcional)

---

#### ğŸ’ª 14-17 pontos: BOM!
**VocÃª pode avanÃ§ar, mas revise alguns pontos.**

âœ… Entende a maioria dos conceitos  
âš ï¸ Pode ter dÃºvidas em situaÃ§Ãµes especÃ­ficas  

**PrÃ³ximos passos:**
- Revise as questÃµes que errou (leia as explicaÃ§Ãµes)
- Pratique um pouco mais os conceitos fracos
- AvanÃ§ar para Semana 4, mas mantenha o material da S3 Ã  mÃ£o

---

#### ğŸ”„ 10-13 pontos: PARCIAL
**Recomendado revisar antes de avanÃ§ar.**

âš ï¸ Alguns conceitos nÃ£o estÃ£o claros  
âš ï¸ Pode ter dificuldade na Semana 4  

**PrÃ³ximos passos:**
- RefaÃ§a o notebook da Semana 3
- Foque nos conceitos que errou aqui
- Tente o teste novamente apÃ³s 2-3 dias
- **SÃ³ avance quando se sentir mais confiante**

---

#### ğŸ“š 0-9 pontos: REVISAR
**Ã‰ importante refazer o conteÃºdo da Semana 3.**

âŒ Conceitos fundamentais precisam de atenÃ§Ã£o  

**PrÃ³ximos passos:**
1. **NÃ£o se desanime!** Isso Ã© aprendizado, nÃ£o prova
2. Releia o material da Semana 3
3. Execute o notebook cÃ©lula por cÃ©lula, entendendo cada parte
4. FaÃ§a anotaÃ§Ãµes com suas palavras
5. Retome este teste em 1 semana

**Dica:** Foque em entender O PORQUÃŠ, nÃ£o em memorizar cÃ³digo.

---

## ğŸ§© Teste BÃ´nus: CenÃ¡rio Real

### ğŸ’¼ Desafio Profissional (avanÃ§ado)

VocÃª Ã© contratado para prever churn (cancelamento) de clientes de uma empresa de streaming. O time te passa:

- **Dataset:** 10.000 clientes, 25 features
- **Target:** `churn` (0 = fica, 1 = cancela)
- **Deadline:** 1 semana
- **Requisito:** MÃ­nimo 80% de recall (nÃ£o perder clientes que vÃ£o sair)

**Seu plano de aÃ§Ã£o:**

```markdown
## Dia 1-2: EDA
- [ ] ___________
- [ ] ___________

## Dia 3-4: Feature Engineering
- [ ] ___________
- [ ] ___________

## Dia 5: Modelagem
- [ ] Modelos a testar: __________, __________, __________
- [ ] Por que esses modelos? ___________

## Dia 6: OtimizaÃ§Ã£o
- [ ] ___________

## Dia 7: Deploy
- [ ] ___________
```

<details>
<summary>ğŸ’¡ Ver exemplo de plano profissional</summary>

## âœ… Exemplo de Plano:

### Dia 1-2: EDA
- [ ] Verificar missing values e outliers
- [ ] Analisar distribuiÃ§Ã£o do target (balanceamento)
- [ ] CorrelaÃ§Ã£o entre features e churn
- [ ] Identificar features mais importantes visualmente

### Dia 3-4: Feature Engineering
- [ ] Criar features de comportamento (mÃ©dia de uso, frequÃªncia)
- [ ] Encoding de variÃ¡veis categÃ³ricas
- [ ] Tratar valores faltantes
- [ ] Normalizar features numÃ©ricas (se necessÃ¡rio)

### Dia 5: Modelagem
- **Modelos:** Random Forest, XGBoost, Logistic Regression
- **Por quÃª:**
  - RF: Baseline sÃ³lido, lida com imbalance
  - XGBoost: Melhor performance geralmente
  - LogReg: RÃ¡pido, interpretÃ¡vel para stakeholders

### Dia 6: OtimizaÃ§Ã£o
- [ ] Hyperparameter tuning (GridSearch/RandomSearch)
- [ ] Ajustar threshold para priorizar recall (>80%)
- [ ] Validar com cross-validation
- [ ] Analisar feature importance para insights de negÃ³cio

### Dia 7: Deploy
- [ ] Salvar modelo final (pickle/joblib)
- [ ] Criar script de prediÃ§Ã£o
- [ ] Documentar pipeline
- [ ] Apresentar insights para o time

### ğŸ“Š MÃ©tricas a Focar:
- **Recall:** >80% (requisito principal)
- **Precision:** Quanto maior melhor (evitar falsos positivos)
- **F1-Score:** Balancear recall e precision
- **ROC-AUC:** Avaliar separaÃ§Ã£o geral

### ğŸ’¡ Dica Profissional:
Em problemas de churn, **recall Ã© mais importante** que accuracy!
Ã‰ melhor contatar 100 clientes (10 falsos alarmes) que perder 10 clientes reais.

</details>

---

## ğŸ¯ ReflexÃ£o Final

Responda honestamente (sÃ³ para vocÃª):

1. **Consegui entender o propÃ³sito de cada modelo?**  
   [ ] Sim, totalmente  
   [ ] Mais ou menos  
   [ ] Preciso revisar  

2. **Sei quando usar normalizaÃ§Ã£o?**  
   [ ] Sim, com confianÃ§a  
   [ ] Tenho dÃºvidas  
   [ ] NÃ£o entendi bem  

3. **Interpreto matriz de confusÃ£o e feature importance?**  
   [ ] Sim, tranquilamente  
   [ ] Com ajuda do material  
   [ ] Ainda confuso  

4. **Me sinto confortÃ¡vel para tentar um problema novo similar?**  
   [ ] Sim, vou tentar!  
   [ ] Com o material ao lado, sim  
   [ ] Acho que nÃ£o ainda  

---

## âœ… DecisÃ£o Final

Com base nas suas respostas:

### â¡ï¸ AVANCE se:
- Acertou 14+ pontos no teste
- Respondeu "Sim" ou "Mais ou menos" na maioria das reflexÃµes
- Sente que, **com consulta ao material**, consegue fazer

### ğŸ”„ REVISE se:
- Acertou <10 pontos
- Respondeu "Preciso revisar" ou "NÃ£o entendi" em 3+ reflexÃµes
- Sente que **nÃ£o conseguiria nem com material ao lado**

---

---

# ğŸ“… DIA 2: Hyperparameter Tuning e Cross-Validation

## ğŸ“‹ Parte 1: Conceitos (mÃºltipla escolha)

### QuestÃ£o 1: ParÃ¢metros vs HiperparÃ¢metros
Qual das seguintes afirmaÃ§Ãµes estÃ¡ CORRETA?

**A)** ParÃ¢metros sÃ£o definidos antes do treino, hiperparÃ¢metros sÃ£o aprendidos durante  
**B)** HiperparÃ¢metros sÃ£o definidos antes do treino, parÃ¢metros sÃ£o aprendidos durante  
**C)** Ambos sÃ£o aprendidos durante o treino  
**D)** Ambos sÃ£o definidos antes do treino  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) HiperparÃ¢metros sÃ£o definidos antes do treino, parÃ¢metros sÃ£o aprendidos durante**

**Exemplos:**

| Modelo | ParÃ¢metros (aprendidos) | HiperparÃ¢metros (definidos antes) |
|--------|------------------------|-----------------------------------|
| Linear Regression | Coeficientes (w, b) | Taxa de aprendizado |
| Random Forest | Splits das Ã¡rvores | `max_depth`, `n_estimators` |
| Neural Network | Pesos das conexÃµes | Arquitetura, learning rate |

**Analogia:** 
- **HiperparÃ¢metros:** ConfiguraÃ§Ãµes do treino (quantos dias treinar por semana)
- **ParÃ¢metros:** O que vocÃª aprende durante o treino (tÃ©cnica de chute)

**Conceito-chave:** HiperparÃ¢metros controlam COMO o modelo aprende, parÃ¢metros sÃ£o O QUE ele aprende.
</details>

---

### QuestÃ£o 2: Grid Search vs Random Search
VocÃª tem 5 hiperparÃ¢metros, cada um com 4 possÃ­veis valores, e tempo limitado (30 minutos). O que fazer?

**A)** Grid Search com todos os valores (4^5 = 1024 combinaÃ§Ãµes)  
**B)** Random Search com n_iter=50  
**C)** Testar manualmente 10 combinaÃ§Ãµes  
**D)** Usar valores padrÃ£o (nÃ£o fazer tuning)  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Random Search com n_iter=50**

**Por quÃª:**

**Grid Search:**
- 1024 combinaÃ§Ãµes Ã— CV=5 = **5120 treinos**
- Tempo: InviÃ¡vel em 30 minutos

**Random Search (n_iter=50):**
- 50 combinaÃ§Ãµes Ã— CV=5 = **250 treinos**
- Tempo: ViÃ¡vel em 30 minutos
- Explora melhor o espaÃ§o (pode encontrar combinaÃ§Ãµes nÃ£o Ã³bvias)

**Manual (10 combinaÃ§Ãµes):**
- Muito poucas para espaÃ§o grande
- Depende de intuiÃ§Ã£o (pode errar)

**Valores padrÃ£o:**
- RÃ¡pido mas pode ter performance ruim

**Regra prÃ¡tica:**
```
HiperparÃ¢metros â‰¤ 3 â†’ Grid Search
HiperparÃ¢metros > 3 â†’ Random Search
Tempo muito limitado â†’ Valores padrÃ£o + tuning simples
```

**Conceito-chave:** Random Search Ã© mais eficiente para espaÃ§os grandes de hiperparÃ¢metros.
</details>

---

### QuestÃ£o 3: Cross-Validation
VocÃª tem um dataset com 1000 amostras (700 classe 0, 300 classe 1). Qual estratÃ©gia de CV usar?

**A)** K-Fold com k=5 (sem stratify)  
**B)** Stratified K-Fold com k=5  
**C)** Leave-One-Out CV  
**D)** Holdout (train/test Ãºnico)  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Stratified K-Fold com k=5**

**Por quÃª:**

Dataset desbalanceado (70% vs 30%) precisa de **Stratified K-Fold**.

**ComparaÃ§Ã£o:**

| MÃ©todo | Classe 0 | Classe 1 | Problema |
|--------|----------|----------|----------|
| K-Fold normal | Varia por fold | Varia por fold | âŒ Alguns folds podem ter poucos exemplos da classe 1 |
| **Stratified K-Fold** | **~70% em cada fold** | **~30% em cada fold** | âœ… ProporÃ§Ã£o mantida |
| Leave-One-Out | - | - | âŒ Muito lento (1000 iteraÃ§Ãµes!) |
| Holdout | 70% | 30% | âŒ NÃ£o valida variÃ¢ncia |

**CÃ³digo correto:**
```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf)
```

**Conceito-chave:** Use SEMPRE Stratified K-Fold em classificaÃ§Ã£o, especialmente se desbalanceado.
</details>

---

### QuestÃ£o 4: Learning Curves
VocÃª treinou um modelo e viu estas curvas:

```
Train accuracy: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95%
Test accuracy:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’ 60%
```

O que fazer?

**A)** Aumentar complexidade (mais features, modelo maior)  
**B)** Reduzir complexidade (regularizaÃ§Ã£o, menos features)  
**C)** Coletar mais dados  
**D)** B ou C (ambas podem ajudar)  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: D) B ou C (ambas podem ajudar)**

**DiagnÃ³stico: OVERFITTING**
- Train >> Test (35% gap)
- Modelo decorou os dados de treino

**SoluÃ§Ãµes:**

**1. Reduzir complexidade (B):**
```python
# Random Forest
RandomForestClassifier(
    max_depth=5,           # â†“ profundidade
    min_samples_split=50,  # â†‘ amostras mÃ­nimas
    min_samples_leaf=20    # â†‘ amostras por folha
)

# XGBoost
XGBClassifier(
    max_depth=3,           # â†“ profundidade
    learning_rate=0.01,    # â†“ taxa aprendizado
    reg_alpha=1.0,         # â†‘ regularizaÃ§Ã£o L1
    reg_lambda=1.0         # â†‘ regularizaÃ§Ã£o L2
)
```

**2. Mais dados (C):**
- Mais exemplos â†’ modelo generaliza melhor
- Se possÃ­vel, sempre ajuda

**3. Feature Selection:**
```python
from sklearn.feature_selection import RFE
selector = RFE(model, n_features_to_select=10)
```

**CenÃ¡rios de Learning Curves:**

```
1. Overfitting:
   Train: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta)
   Test:  â”€â”€â”€â”€â–â–â–â–â–â–  (baixa)
   â†’ SoluÃ§Ã£o: Regularizar ou mais dados

2. Underfitting:
   Train: â”€â”€â”€â”€â”€â”€â”€â”€ (baixa)
   Test:  â”€â”€â”€â”€â”€â”€â”€â”€ (baixa)
   â†’ SoluÃ§Ã£o: Modelo mais complexo

3. Ideal:
   Train: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta)
   Test:  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta, prÃ³xima)
   â†’ SoluÃ§Ã£o: Nenhuma, estÃ¡ Ã³timo! âœ…
```

**Conceito-chave:** Gap grande entre train/test = Overfitting â†’ Regularizar ou mais dados.
</details>

---

## ğŸ–¥ï¸ Parte 2: PrÃ¡tica (cÃ³digo)

### Desafio: Pipeline com Grid Search

Complete o cÃ³digo abaixo corrigindo os **4 erros**:

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
import pandas as pd

# Carregar dados
df = pd.read_csv('titanic.csv')
X = df.drop('survived', axis=1)
y = df['survived']

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Pipeline para SVM
pipeline_svm = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', SVC())
])

# Grid Search para SVM
param_grid_svm = {
    'C': [0.1, 1, 10],              # âŒ ERRO 1: Falta prefixo
    'kernel': ['linear', 'rbf']
}

grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=5)
grid_svm.fit(X_train, y_train)

# Pipeline para Random Forest
pipeline_rf = Pipeline([
    ('scaler', StandardScaler()),   # âŒ ERRO 2: RF nÃ£o precisa
    ('classifier', RandomForestClassifier())
])

# Treinar Random Forest
pipeline_rf.fit(X_train, y_train)

# Avaliar SVM
y_pred_svm = grid_svm.predict(X_test)
print(f"SVM Accuracy: {grid_svm.score(X_test, y_test)}")

# Avaliar Random Forest
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test)  # âŒ ERRO 3: Data leakage
y_pred_rf = pipeline_rf.predict(X_test_scaled)  # âŒ ERRO 4: Dados errados

print(f"RF Accuracy: {pipeline_rf.score(X_test, y_test)}")
```

<details>
<summary>ğŸ’¡ Ver resposta completa</summary>

### âŒ Erros e CorreÃ§Ãµes:

#### ERRO 1: ParÃ¢metros do Grid sem prefixo do pipeline
```python
# ERRADO
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# CERTO
param_grid_svm = {
    'classifier__C': [0.1, 1, 10],           # Prefixo: nome_do_passo__
    'classifier__kernel': ['linear', 'rbf']
}
```
**Por quÃª:** No Pipeline, parÃ¢metros sÃ£o acessados com `nome_passo__parametro`.

---

#### ERRO 2: Random Forest nÃ£o precisa de normalizaÃ§Ã£o
```python
# ERRADO
pipeline_rf = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier())
])

# CERTO
pipeline_rf = Pipeline([
    ('classifier', RandomForestClassifier())  # Sem scaler
])

# OU SIMPLESMENTE
rf_model = RandomForestClassifier()
```
**Por quÃª:** RF Ã© baseado em Ã¡rvores (thresholds), nÃ£o Ã© afetado por escala.

---

#### ERRO 3: `fit_transform` no test set
```python
# ERRADO
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test)  # âŒ Data leakage!

# CERTO
# NÃ£o precisa escalar separadamente, o pipeline jÃ¡ faz isso!
# Mas se fosse necessÃ¡rio:
scaler = StandardScaler()
scaler.fit(X_train)  # Fit apenas no train
X_test_scaled = scaler.transform(X_test)  # Transform no test
```

---

#### ERRO 4: Passar dados errados para o pipeline
```python
# ERRADO
y_pred_rf = pipeline_rf.predict(X_test_scaled)

# CERTO
y_pred_rf = pipeline_rf.predict(X_test)  # Dados originais
```
**Por quÃª:** O pipeline jÃ¡ faz o preprocessamento internamente (se tivesse scaler).

---

### âœ… CÃ³digo Correto Completo:

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
import pandas as pd

# Carregar dados
df = pd.read_csv('titanic.csv')
X = df.drop('survived', axis=1)
y = df['survived']

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y  # âœ… Bonus: stratify
)

# ====== SVM (precisa de normalizaÃ§Ã£o) ======
pipeline_svm = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', SVC(random_state=42))
])

# Grid Search para SVM
param_grid_svm = {
    'classifier__C': [0.1, 1, 10],              # âœ… Com prefixo
    'classifier__kernel': ['linear', 'rbf'],
    'classifier__gamma': ['scale', 'auto']      # âœ… Bonus: testar gamma
}

grid_svm = GridSearchCV(
    pipeline_svm, 
    param_grid_svm, 
    cv=5,
    scoring='accuracy',
    n_jobs=-1  # âœ… Paralelizar
)
grid_svm.fit(X_train, y_train)

print(f"Melhores hiperparÃ¢metros SVM: {grid_svm.best_params_}")
print(f"SVM CV Score: {grid_svm.best_score_:.3f}")

# ====== Random Forest (NÃƒO precisa normalizaÃ§Ã£o) ======
pipeline_rf = Pipeline([
    ('classifier', RandomForestClassifier(random_state=42))  # âœ… Sem scaler
])

# Grid Search para Random Forest
param_grid_rf = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [5, 10, None],
    'classifier__min_samples_split': [2, 5, 10]
}

grid_rf = GridSearchCV(
    pipeline_rf,
    param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_rf.fit(X_train, y_train)

print(f"Melhores hiperparÃ¢metros RF: {grid_rf.best_params_}")
print(f"RF CV Score: {grid_rf.best_score_:.3f}")

# ====== AvaliaÃ§Ã£o Final ======
svm_test_score = grid_svm.score(X_test, y_test)  # âœ… Pipeline faz tudo
rf_test_score = grid_rf.score(X_test, y_test)    # âœ… Pipeline faz tudo

print(f"\n=== Resultados no Test Set ===")
print(f"SVM Test Accuracy: {svm_test_score:.3f}")
print(f"RF Test Accuracy: {rf_test_score:.3f}")

# ComparaÃ§Ã£o
if svm_test_score > rf_test_score:
    print("\nğŸ† Melhor modelo: SVM")
else:
    print("\nğŸ† Melhor modelo: Random Forest")
```

### ğŸ“Š Checklist de ValidaÃ§Ã£o:

| Modelo | Precisa NormalizaÃ§Ã£o? | Pipeline Correto |
|--------|-----------------------|------------------|
| Random Forest | âŒ NÃ£o | `Pipeline([('classifier', RF())])` |
| XGBoost | âŒ NÃ£o | `Pipeline([('classifier', XGB())])` |
| SVM | âœ… Sim | `Pipeline([('scaler', SS()), ('classifier', SVC())])` |
| MLP | âœ… Sim | `Pipeline([('scaler', SS()), ('classifier', MLP())])` |

### ğŸ¯ Conceitos Aplicados:

1. âœ… Pipeline previne data leakage
2. âœ… `classifier__param` para acessar hiperparÃ¢metros
3. âœ… Grid Search otimiza automaticamente
4. âœ… CV valida durante otimizaÃ§Ã£o
5. âœ… Test set apenas para avaliaÃ§Ã£o final

</details>

---

## ğŸ“Š Parte 3: DiagnÃ³stico de Learning Curves

Analise as 3 curvas abaixo e identifique o problema:

### Modelo A:
```
Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 99%
Test:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’ 65%
```

### Modelo B:
```
Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’ 68%
Test:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’ 65%
```

### Modelo C:
```
Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’ 87%
Test:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’ 84%
```

**Qual modelo escolher e por quÃª?**

<details>
<summary>ğŸ’¡ Ver anÃ¡lise completa</summary>

### ğŸ“Š DiagnÃ³stico:

#### Modelo A: OVERFITTING SEVERO âŒ
```
Train: 99% | Test: 65% | Gap: 34%
```

**Problema:** Modelo decorou os dados de treino  
**Sintomas:** 
- Train muito alta
- Test muito baixa
- Gap enorme (>30%)

**SoluÃ§Ãµes:**
```python
# 1. Reduzir complexidade
RandomForestClassifier(
    max_depth=5,           # Limitar profundidade
    min_samples_split=50,  # Mais restritivo
    n_estimators=50        # Menos Ã¡rvores
)

# 2. RegularizaÃ§Ã£o
XGBClassifier(
    learning_rate=0.01,    # Menor learning rate
    reg_alpha=1.0,         # L1 regularization
    reg_lambda=1.0         # L2 regularization
)

# 3. Feature Selection
from sklearn.feature_selection import RFE
selector = RFE(model, n_features_to_select=10)
```

---

#### Modelo B: UNDERFITTING âš ï¸
```
Train: 68% | Test: 65% | Gap: 3%
```

**Problema:** Modelo muito simples, nÃ£o aprende padrÃµes  
**Sintomas:**
- Train e Test ambas baixas
- Gap pequeno (bom sinal)
- Performance geral ruim (<70%)

**SoluÃ§Ãµes:**
```python
# 1. Aumentar complexidade
RandomForestClassifier(
    max_depth=None,        # Sem limite
    n_estimators=200,      # Mais Ã¡rvores
    min_samples_split=2    # Menos restritivo
)

# 2. Mais features
# - Feature engineering
# - Interaction terms
# - Polynomial features

# 3. Modelo mais poderoso
from xgboost import XGBClassifier
model = XGBClassifier(n_estimators=200, max_depth=6)
```

---

#### Modelo C: IDEAL âœ… ğŸ†
```
Train: 87% | Test: 84% | Gap: 3%
```

**AnÃ¡lise:**
- âœ… Test accuracy boa (>80%)
- âœ… Gap pequeno (<5%)
- âœ… Generaliza bem
- âœ… NÃ£o overfitting nem underfitting

**Por que escolher:**
1. **GeneralizaÃ§Ã£o:** Test prÃ³ximo de Train
2. **Performance:** 84% Ã© boa para maioria dos problemas
3. **Estabilidade:** Gap pequeno indica modelo robusto
4. **Confiabilidade:** FuncionarÃ¡ bem em produÃ§Ã£o

**Quando NÃƒO escolher:**
- Se o problema exige >90% accuracy
- Se hÃ¡ classes desbalanceadas (olhar outras mÃ©tricas)

---

### ğŸ¯ Regras de DecisÃ£o:

| Train | Test | Gap | DiagnÃ³stico | AÃ§Ã£o |
|-------|------|-----|-------------|------|
| >95% | <70% | >25% | **Overfitting severo** | âŒ Regularizar forte |
| >90% | <80% | >10% | **Overfitting** | âš ï¸ Regularizar |
| >85% | >80% | <5% | **Ideal** | âœ… Usar! |
| <75% | <75% | <5% | **Underfitting** | âš ï¸ Aumentar complexidade |

### ğŸ’¡ Dica Profissional:

```python
# Visualizar Learning Curves no cÃ³digo
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, 
    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring='accuracy'
)

plt.plot(train_sizes, train_scores.mean(axis=1), label='Train')
plt.plot(train_sizes, test_scores.mean(axis=1), label='Test')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Learning Curves')
plt.show()
```

**O que procurar:**
- Curvas convergindo â†’ Bom sinal âœ…
- Curvas divergindo â†’ Overfitting âŒ
- Ambas baixas â†’ Underfitting âš ï¸

</details>

---

## ğŸ“ Gabarito de Auto-AvaliaÃ§Ã£o - Dia 2

### PontuaÃ§Ã£o:

- **Parte 1 (Conceitos):** 4 questÃµes Ã— 2.5 pontos = **10 pontos**
- **Parte 2 (CÃ³digo):** 4 erros Ã— 2.5 pontos = **10 pontos**  
- **Parte 3 (Learning Curves):** 1 questÃ£o Ã— 10 pontos = **10 pontos**

**TOTAL:** 30 pontos

---

### ğŸ“Š InterpretaÃ§Ã£o da sua nota (Dia 2):

#### ğŸ† 27-30 pontos: EXCELENTE!
**VocÃª domina Hyperparameter Tuning!**

âœ… Entende Grid vs Random Search  
âœ… Aplica Cross-Validation corretamente  
âœ… Diagnostica overfitting/underfitting  
âœ… Usa Pipelines sem data leakage  

**PrÃ³ximos passos:**
- AvanÃ§ar para Dia 3 (Dashboard React)
- Aplicar tuning em projeto pessoal

---

#### ğŸ’ª 20-26 pontos: BOM!
**VocÃª entende os conceitos principais.**

âœ… Conceitos gerais claros  
âš ï¸ Pode ter dÃºvidas em implementaÃ§Ã£o  

**PrÃ³ximos passos:**
- Revisar Pipeline e prefixos (`classifier__param`)
- Praticar mais Grid/Random Search
- Pode avanÃ§ar, mas mantenha material Ã  mÃ£o

---

#### ğŸ”„ 15-19 pontos: PARCIAL
**Recomendado praticar mais antes de avanÃ§ar.**

âš ï¸ Alguns conceitos nÃ£o estÃ£o sÃ³lidos  
âš ï¸ Pode ter dificuldade no Dia 3  

**PrÃ³ximos passos:**
- Refazer notebook `02-hyperparameter-tuning.ipynb`
- Focar em Pipeline e Cross-Validation
- Ler material lÃºdico (`14-arvores-decisao-explicacao-ludica.md`)
- Repetir teste em 2-3 dias

---

#### ğŸ“š 0-14 pontos: REVISAR
**Ã‰ importante refazer o Dia 2.**

âŒ Conceitos fundamentais precisam de atenÃ§Ã£o  

**PrÃ³ximos passos:**
1. **NÃ£o desanime!** Hyperparameters sÃ£o complexos
2. Releia `docs/15-dia2-semana3-hyperparameter-tuning.md`
3. Execute notebook cÃ©lula por cÃ©lula
4. Foque em entender O PORQUÃŠ de cada decisÃ£o
5. Retome este teste em 1 semana

---

## ğŸ§© Teste BÃ´nus: CenÃ¡rio Real - OtimizaÃ§Ã£o Completa

### ğŸ’¼ Desafio Profissional

VocÃª estÃ¡ otimizando um modelo de detecÃ§Ã£o de fraude:

**Dataset:**
- 50.000 transaÃ§Ãµes
- 20 features numÃ©ricas
- Target: `is_fraud` (desbalanceado: 98% legÃ­timo, 2% fraude)

**Requisitos:**
- Recall mÃ­nimo: 85% (nÃ£o perder fraudes)
- Tempo de inferÃªncia: <100ms por prediÃ§Ã£o
- Explicabilidade: Importante (stakeholders nÃ£o-tÃ©cnicos)

**Seu plano:**

```markdown
## Escolha do Modelo
Modelo principal: __________
Por quÃª: __________

Modelo alternativo (backup): __________
Por quÃª: __________

## EstratÃ©gia de OtimizaÃ§Ã£o
[ ] Grid Search ou Random Search? __________
[ ] HiperparÃ¢metros prioritÃ¡rios: __________, __________, __________
[ ] Cross-Validation: K-Fold ou Stratified K-Fold? __________

## Preprocessing
[ ] NormalizaÃ§Ã£o necessÃ¡ria? __________
[ ] Feature Selection? __________
[ ] Balanceamento de classes? __________

## ValidaÃ§Ã£o
MÃ©trica principal: __________
Por quÃª: __________
```

<details>
<summary>ğŸ’¡ Ver exemplo de plano profissional</summary>

## âœ… Exemplo de Plano:

### Escolha do Modelo

**Modelo principal: Random Forest**

**Por quÃª:**
- âœ… Explicabilidade: Feature importance clara
- âœ… Performance: Boa em dados tabulares
- âœ… Velocidade: <100ms factÃ­vel
- âœ… Lida bem com desbalanceamento (ajustando `class_weight`)

**Modelo alternativo: XGBoost**

**Por quÃª:**
- âœ… Melhor performance (backup se RF nÃ£o atingir 85% recall)
- âœ… Feature importance disponÃ­vel
- âš ï¸ Menos interpretÃ¡vel que RF
- âš ï¸ Pode ser mais lento

---

### EstratÃ©gia de OtimizaÃ§Ã£o

**Random Search (n_iter=50)**

**Por quÃª:**
- 5+ hiperparÃ¢metros para otimizar
- Tempo limitado
- EspaÃ§o de busca grande

**HiperparÃ¢metros prioritÃ¡rios:**

```python
param_distributions = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [10, 20, 50],
    'min_samples_leaf': [5, 10, 20],
    'class_weight': ['balanced', 'balanced_subsample'],  # CRUCIAL para desbalanceamento
    'max_features': ['sqrt', 'log2', None]
}
```

**Cross-Validation: Stratified K-Fold (k=5)**

**Por quÃª:**
- Dataset desbalanceado (98/2)
- Garante 2% fraude em cada fold
- ValidaÃ§Ã£o mais confiÃ¡vel

---

### Preprocessing

**NormalizaÃ§Ã£o: NÃƒO**
- Random Forest nÃ£o precisa
- Se usar XGBoost: tambÃ©m nÃ£o precisa

**Feature Selection: SIM (se performance permitir)**
```python
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(f_classif, k=15)  # Reduzir de 20 para 15
```
**Por quÃª:**
- Melhora velocidade de inferÃªncia
- Reduz ruÃ­do
- Aumenta explicabilidade

**Balanceamento: SIM**
```python
RandomForestClassifier(
    class_weight='balanced',  # Penaliza erro em classe minoritÃ¡ria
    ...
)

# OU SMOTE (se necessÃ¡rio)
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

---

### ValidaÃ§Ã£o

**MÃ©trica principal: RECALL (classe fraude)**

**Por quÃª:**
- Requisito: 85% recall mÃ­nimo
- DetecÃ§Ã£o de fraude: **Falso Negativo Ã© MUITO pior** que Falso Positivo
- Melhor perder 10 transaÃ§Ãµes legÃ­timas (FP) que deixar passar 1 fraude (FN)

**MÃ©tricas secundÃ¡rias:**
- **Precision:** Evitar muitos falsos alarmes
- **F1-Score:** Balancear recall e precision
- **ROC-AUC:** SeparaÃ§Ã£o geral das classes

**Pipeline completo:**

```python
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold

# Pipeline
pipeline = Pipeline([
    ('selector', SelectKBest(f_classif, k=15)),
    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))
])

# ParÃ¢metros
param_distributions = {
    'selector__k': [10, 15, 20],
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [10, 20, None],
    'classifier__min_samples_split': [10, 20, 50],
    'classifier__class_weight': ['balanced', 'balanced_subsample']
}

# Stratified K-Fold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Random Search
random_search = RandomizedSearchCV(
    pipeline,
    param_distributions,
    n_iter=50,
    cv=cv,
    scoring='recall',  # MÃ‰TRICA PRINCIPAL
    n_jobs=-1,
    random_state=42,
    verbose=2
)

# Treinar
random_search.fit(X_train, y_train)

# Melhor modelo
best_model = random_search.best_estimator_

# Avaliar no test
from sklearn.metrics import classification_report, recall_score

y_pred = best_model.predict(X_test)
recall = recall_score(y_test, y_pred, pos_label=1)  # Classe fraude

print(f"Recall (fraude): {recall:.3f}")
print(classification_report(y_test, y_pred))

# Validar requisito
if recall >= 0.85:
    print("âœ… Requisito de recall atingido!")
else:
    print(f"âŒ Recall abaixo do mÃ­nimo. Falta: {0.85 - recall:.3f}")
```

---

### ğŸ¯ DecisÃ£o Final

**SE recall < 85%:**
1. Ajustar threshold de decisÃ£o
```python
# Reduzir threshold para aumentar recall
y_proba = best_model.predict_proba(X_test)[:, 1]
y_pred_adjusted = (y_proba > 0.3).astype(int)  # Em vez de 0.5
```

2. Tentar XGBoost (modelo alternativo)

3. Aplicar SMOTE (oversampling)

**SE recall >= 85%:**
âœ… Modelo pronto para produÃ§Ã£o!

---

### ğŸ“Š EntregÃ¡veis

1. Modelo salvo (`fraud_detector.pkl`)
2. Pipeline de preprocessamento
3. RelatÃ³rio de performance
4. Feature importance (top 10)
5. DocumentaÃ§Ã£o para stakeholders

</details>

---

## ğŸ’¬ Mensagem Final - Dia 2

> **"Eu entendi, mas nÃ£o faÃ§o sozinho sem consultar"**  
> **Isso Ã© NORMAL e ESPERADO!** ğŸ‘

Profissionais consultam:
- DocumentaÃ§Ã£o do Scikit-learn
- Stack Overflow
- Notebooks anteriores
- Papers acadÃªmicos

**A diferenÃ§a Ã©:** eles sabem **o que procurar** e **como aplicar**.

Se vocÃª chegou atÃ© aqui, executou tudo, e entende os conceitos quando lÃª as explicaÃ§Ãµes, **vocÃª estÃ¡ no caminho certo!** ğŸ¯

---

**Sucesso na sua jornada! ğŸš€**

_Este teste pode ser refeito quantas vezes quiser. Use-o como ferramenta de aprendizado, nÃ£o como prova!_

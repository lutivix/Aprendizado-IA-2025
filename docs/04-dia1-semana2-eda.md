# ğŸ“Š Semana 2 - Dia 1: AnÃ¡lise ExploratÃ³ria de Dados (EDA)

**Data:** 28 Outubro 2025  
**Tempo:** _A registrar_  
**Status:** ğŸŸ¡ Em progresso

---

## ğŸ¯ Objetivos do Dia

- [ ] Dominar tÃ©cnicas de EDA (Exploratory Data Analysis)
- [ ] Trabalhar com dataset real (Titanic)
- [ ] Limpeza e preparaÃ§Ã£o de dados profissional
- [ ] Feature engineering bÃ¡sico
- [ ] Comparar mÃºltiplos modelos ML
- [ ] MÃ©tricas avanÃ§adas (accuracy, precision, recall, F1-score)

---

## ğŸ“š Conceitos a Dominar

### 1. AnÃ¡lise ExploratÃ³ria de Dados (EDA)
**O que Ã©:** Processo de investigaÃ§Ã£o dos dados para descobrir padrÃµes, detectar anomalias, testar hipÃ³teses e verificar suposiÃ§Ãµes atravÃ©s de estatÃ­sticas e visualizaÃ§Ãµes.

**Por que importa:**
- Entender a qualidade dos dados antes de treinar modelos
- Identificar features relevantes
- Detectar valores nulos, outliers, desbalanceamento
- Orientar decisÃµes de feature engineering

### 2. Feature Engineering
**O que Ã©:** Processo de criar novas variÃ¡veis (features) a partir das existentes para melhorar a performance do modelo.

**Exemplos no Titanic:**
- `family_size = sibsp + parch + 1` (tamanho da famÃ­lia)
- `is_alone = (family_size == 1)` (viajou sozinho?)
- `age_group` (crianÃ§a, adulto, idoso)

### 3. MÃ©tricas de ClassificaÃ§Ã£o

| MÃ©trica | O que mede | Quando usar |
|---------|-----------|-------------|
| **Accuracy** | % de prediÃ§Ãµes corretas | Dados balanceados |
| **Precision** | % de positivos preditos que sÃ£o realmente positivos | Custo de falso positivo Ã© alto |
| **Recall** | % de positivos reais que foram identificados | Custo de falso negativo Ã© alto |
| **F1-Score** | MÃ©dia harmÃ´nica entre Precision e Recall | Balancear precision e recall |

**FÃ³rmulas:**
```
Accuracy  = (TP + TN) / (TP + TN + FP + FN)
Precision = TP / (TP + FP)
Recall    = TP / (TP + FN)
F1-Score  = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

### 4. Modelos de ClassificaÃ§Ã£o

#### Logistic Regression
- **Como funciona:** Usa funÃ§Ã£o sigmoide para calcular probabilidade (0-1)
- **Vantagens:** Simples, interpretÃ¡vel, rÃ¡pido
- **LimitaÃ§Ãµes:** Assume relaÃ§Ã£o linear, nÃ£o captura interaÃ§Ãµes complexas

#### Decision Tree
- **Como funciona:** Cria Ã¡rvore de decisÃµes baseada em regras if/else
- **Vantagens:** InterpretÃ¡vel, captura nÃ£o-linearidades, nÃ£o precisa normalizar
- **LimitaÃ§Ãµes:** Tende a overfitting, instÃ¡vel (pequenas mudanÃ§as nos dados alteram muito)

---

## ğŸ“ Roteiro do Notebook

### Parte 1: Setup e Carregamento
```python
- Importar bibliotecas (pandas, numpy, matplotlib, seaborn, sklearn)
- Carregar dataset Titanic
- Verificar shape e primeiras linhas
```

### Parte 2: AnÃ¡lise Inicial
```python
- df.info() - tipos de dados e nulos
- df.describe() - estatÃ­sticas descritivas
- Identificar valores nulos por coluna
```

### Parte 3: VisualizaÃ§Ãµes ExploratÃ³rias
```python
- Taxa de sobrevivÃªncia geral
- SobrevivÃªncia por gÃªnero
- SobrevivÃªncia por classe
- DistribuiÃ§Ã£o de idade
- Mapa de correlaÃ§Ã£o
```

### Parte 4: Limpeza de Dados
```python
- Tratar valores nulos (age â†’ mediana, embarked â†’ moda)
- Criar features: family_size, is_alone
- Transformar categÃ³ricas em numÃ©ricas (sex â†’ sex_numeric)
```

### Parte 5: Machine Learning
```python
- Selecionar features
- Train/Test split (80/20)
- Treinar Logistic Regression
- Treinar Decision Tree
- Comparar mÃ©tricas
- Matriz de confusÃ£o
- Feature importance
```

---

## ğŸ”§ Comandos e FunÃ§Ãµes Importantes

### Pandas
```python
df.head()                    # Primeiras 5 linhas
df.info()                    # InformaÃ§Ãµes do dataset
df.describe()                # EstatÃ­sticas descritivas
df.isnull().sum()           # Contar valores nulos
df['col'].fillna(value)     # Preencher nulos
df['col'].median()          # Mediana
df['col'].mode()[0]         # Moda
df.corr()                   # Matriz de correlaÃ§Ã£o
```

### Scikit-learn
```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
accuracy = accuracy_score(y_test, predictions)
```

### VisualizaÃ§Ãµes
```python
import seaborn as sns
sns.countplot(data=df, x='survived')     # Contagem
sns.heatmap(correlation, annot=True)     # Mapa de calor
df['age'].hist(bins=30)                  # Histograma
df.boxplot(column='age', by='survived')  # Boxplot
```

---

## ğŸ¯ Checklist de ConclusÃ£o

- [ ] Dataset carregado e explorado
- [ ] Valores nulos tratados
- [ ] Features criadas (family_size, is_alone)
- [ ] VisualizaÃ§Ãµes geradas (mÃ­nimo 5)
- [ ] 2 modelos treinados (Logistic Regression + Decision Tree)
- [ ] MÃ©tricas calculadas (accuracy, precision, recall, F1)
- [ ] Matriz de confusÃ£o visualizada
- [ ] Feature importance analisada
- [ ] Insights documentados no notebook

---

## ğŸ“Š Resultados Esperados

**Meta mÃ­nima:**
- Accuracy > 75%
- Entender diferenÃ§a entre mÃ©tricas
- Identificar features mais importantes

**Meta ideal:**
- Accuracy > 80%
- Comparar 2+ modelos
- Feature engineering criativo
- Insights acionÃ¡veis sobre o dataset

---

## ğŸ”® PrÃ³ximos Passos

**Dia 2:** Criar API REST em Python (Flask/FastAPI)
- Endpoint de prediÃ§Ã£o usando modelo treinado
- Salvar modelo com pickle/joblib
- ValidaÃ§Ã£o de inputs
- Retornar JSON com probabilidades

---

## ğŸ“š Recursos Complementares

### Datasets para praticar:
- **Titanic:** ClassificaÃ§Ã£o (sobreviventes) - dataset clÃ¡ssico
- **Iris:** ClassificaÃ§Ã£o multi-classe (3 espÃ©cies)
- **Wine Quality:** ClassificaÃ§Ã£o (qualidade 0-10)
- **House Prices:** RegressÃ£o (Kaggle)

### DocumentaÃ§Ã£o:
- [Pandas Docs](https://pandas.pydata.org/docs/)
- [Seaborn Gallery](https://seaborn.pydata.org/examples/index.html)
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)

---

**ğŸš€ Bora comeÃ§ar! Abra o notebook `01-eda-analise-exploratoria.ipynb`**

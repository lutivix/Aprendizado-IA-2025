# ğŸ“š Dia 1 - Semana 4: RevisÃ£o e ConsolidaÃ§Ã£o de ML

**Data:** 3 Dezembro 2025  
**DuraÃ§Ã£o:** TBD  
**Foco:** ğŸ§˜â€â™€ï¸ Consolidar conhecimento, nÃ£o correr

---

## ğŸ¯ Objetivos do Dia

1. **Revisar quando usar cada algoritmo ML**
2. **Criar comparaÃ§Ã£o visual e prÃ¡tica de modelos**
3. **Desenvolver Ã¡rvore de decisÃ£o para escolha de algoritmos**
4. **Praticar cenÃ¡rios reais de decisÃ£o**

## ğŸ§  Por que Esta RevisÃ£o Ã© Importante?

ApÃ³s 3 semanas intensas, vocÃª implementou diversos algoritmos ML:
- **RegressÃ£o:** Linear Regression, Ridge, Lasso
- **ClassificaÃ§Ã£o:** Logistic Regression, Random Forest, XGBoost, SVM, Neural Networks
- **TÃ©cnicas:** Cross-Validation, Hyperparameter Tuning, Feature Engineering

**Mas vocÃª sabe QUANDO usar cada um?** Esta Ã© a diferenÃ§a entre alguÃ©m que implementa cÃ³digo e alguÃ©m que **resolve problemas com ML**. ğŸ¯

---

## ğŸ“Š Guia Comparativo de Algoritmos ML

### 1ï¸âƒ£ RegressÃ£o Linear (Linear Regression)

#### âœ… Quando Usar
- Relacionamento **linear** entre features e target
- Dados com **poucas features** (< 10-20)
- VocÃª precisa de **interpretabilidade** (coeficientes claros)
- Baseline rÃ¡pido para comparaÃ§Ã£o

#### âŒ Quando NÃƒO Usar
- RelaÃ§Ãµes nÃ£o-lineares complexas
- Muitas features correlacionadas (multicolinearidade)
- Outliers severos nos dados
- Target nÃ£o tem distribuiÃ§Ã£o normal

#### ğŸ’¡ Exemplo PrÃ¡tico
```python
# BOM USO: Prever preÃ§o de casa baseado em Ã¡rea
# Features: Ã¡rea (mÂ²), quartos, banheiros
# Target: preÃ§o (relaÃ§Ã£o aproximadamente linear)

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
```

**Seu Projeto:** Semana 1 - PrevisÃ£o simples (RÂ² 96.5%)

---

### 2ï¸âƒ£ RegressÃ£o LogÃ­stica (Logistic Regression)

#### âœ… Quando Usar
- **ClassificaÃ§Ã£o binÃ¡ria** (sim/nÃ£o, 0/1)
- Precisa de **probabilidades** como output
- Dados **linearmente separÃ¡veis** (ou quase)
- Baseline rÃ¡pido para classificaÃ§Ã£o
- Precisa entender **importÃ¢ncia das features**

#### âŒ Quando NÃƒO Usar
- RelaÃ§Ãµes nÃ£o-lineares complexas
- MÃºltiplas classes com padrÃµes complexos
- Features altamente correlacionadas sem regularizaÃ§Ã£o

#### ğŸ’¡ Exemplo PrÃ¡tico
```python
# BOM USO: Prever sobrevivÃªncia no Titanic
# Features: idade, sexo, classe, tarifa
# Target: survived (0/1)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
```

**Seu Projeto:** Semana 2 - Titanic (79% accuracy)

---

### 3ï¸âƒ£ Random Forest

#### âœ… Quando Usar
- **Primeira escolha** para muitos problemas
- Dados com **features nÃ£o-lineares**
- Precisa de **feature importance**
- Dados com outliers (mais robusto)
- ClassificaÃ§Ã£o ou regressÃ£o
- NÃ£o quer gastar muito tempo com feature engineering

#### âŒ Quando NÃƒO Usar
- Dados com muitas dimensÃµes e poucas amostras
- Precisa de modelo muito rÃ¡pido em produÃ§Ã£o
- MemÃ³ria limitada (armazena mÃºltiplas Ã¡rvores)

#### ğŸ’¡ Exemplo PrÃ¡tico
```python
# BOM USO: Qualquer problema tabular balanceado
# Features: mix de categÃ³ricas e numÃ©ricas
# Target: classificaÃ§Ã£o ou regressÃ£o

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Feature importance
importances = model.feature_importances_
```

**Seu Projeto:** Semana 3 - Dia 1 (81% accuracy), Dia 3 (81.46% API)

---

### 4ï¸âƒ£ XGBoost (Gradient Boosting)

#### âœ… Quando Usar
- Quer a **melhor accuracy** possÃ­vel
- Dados tabulares estruturados
- CompetiÃ§Ãµes de Kaggle ğŸ†
- Tem tempo para hyperparameter tuning
- Dados desbalanceados (com scale_pos_weight)

#### âŒ Quando NÃƒO Usar
- Poucos dados de treino (< 1000 amostras)
- Interpretabilidade Ã© prioridade mÃ¡xima
- NÃ£o tem tempo/recursos para tuning
- Dados muito ruidosos (pode overfit)

#### ğŸ’¡ Exemplo PrÃ¡tico
```python
# BOM USO: Maximizar performance em competiÃ§Ã£o
# Features: bem preparadas, sem missing values tratado
# Target: classificaÃ§Ã£o ou regressÃ£o

import xgboost as xgb
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)
model.fit(X_train, y_train)
```

**Seu Projeto:** Semana 3 - Dia 1 (**85.1% accuracy** - melhor resultado!)

---

### 5ï¸âƒ£ Support Vector Machine (SVM)

#### âœ… Quando Usar
- Dados **pequenos** mas com **alta dimensionalidade**
- ClassificaÃ§Ã£o binÃ¡ria com **margem clara**
- Kernel trick para relaÃ§Ãµes nÃ£o-lineares (RBF kernel)
- Dados bem escalados e normalizados

#### âŒ Quando NÃƒO Usar
- Datasets muito grandes (> 10k amostras - lento!)
- MÃºltiplas classes (pode ser lento)
- Features nÃ£o escaladas
- Precisa de probabilidades (nÃ£o nativo)

#### ğŸ’¡ Exemplo PrÃ¡tico
```python
# BOM USO: ClassificaÃ§Ã£o de textos, imagens pequenas
# Features: escaladas (StandardScaler)
# Target: classificaÃ§Ã£o binÃ¡ria ou multi-classe pequena

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

model = SVC(kernel='rbf', C=1.0, random_state=42)
model.fit(X_train_scaled, y_train)
```

**Seu Projeto:** Semana 3 - Dia 1 (implementado com kernel RBF)

---

### 6ï¸âƒ£ Neural Networks (MLP)

#### âœ… Quando Usar
- RelaÃ§Ãµes **muito complexas** e nÃ£o-lineares
- Dados com **padrÃµes ocultos**
- Tem muitos dados de treino (> 10k amostras)
- Features jÃ¡ bem preprocessadas
- Pode usar GPU para treino

#### âŒ Quando NÃƒO Usar
- Poucos dados (vai overfit)
- Precisa de interpretabilidade
- Dados nÃ£o estÃ£o escalados
- Sem tempo para tuning (muitos hiperparÃ¢metros)
- Baseline ou prototipagem rÃ¡pida

#### ğŸ’¡ Exemplo PrÃ¡tico
```python
# BOM USO: PadrÃµes complexos com muitos dados
# Features: escaladas, muitas amostras
# Target: classificaÃ§Ã£o ou regressÃ£o complexa

from sklearn.neural_network import MLPClassifier

model = MLPClassifier(
    hidden_layer_sizes=(100, 50),
    max_iter=1000,
    random_state=42,
    early_stopping=True
)
model.fit(X_train, y_train)
```

**Seu Projeto:** Semana 3 - Dia 1 (implementado com early stopping)

---

## ğŸŒ³ Ãrvore de DecisÃ£o: Como Escolher seu Algoritmo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Seu problema Ã© classificaÃ§Ã£o    â”‚
â”‚         ou regressÃ£o?               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
      â”‚             â”‚
 REGRESSÃƒO    CLASSIFICAÃ‡ÃƒO
      â”‚             â”‚
      â”‚             â”œâ”€â”€ BinÃ¡ria (0/1)?
      â”‚             â”‚   â”œâ”€â”€ Sim â†’ LogisticRegression (baseline)
      â”‚             â”‚   â”‚         RandomForest (melhor)
      â”‚             â”‚   â”‚         XGBoost (competiÃ§Ã£o)
      â”‚             â”‚   â”‚
      â”‚             â”‚   â””â”€â”€ Multi-classe?
      â”‚             â”‚       â””â”€â”€ RandomForest ou XGBoost
      â”‚             â”‚
      â”‚             â””â”€â”€ Tem muitos dados (>10k)?
      â”‚                 â”œâ”€â”€ Sim â†’ XGBoost ou Neural Network
      â”‚                 â””â”€â”€ NÃ£o â†’ RandomForest ou SVM
      â”‚
      â””â”€â”€ LinearRegression (baseline linear)
          RandomForest (nÃ£o-linear)
          XGBoost (melhor accuracy)
          Neural Network (muito complexo)
```

---

## ğŸ¯ Guia PrÃ¡tico: Seu Fluxo de Trabalho ML

### Etapa 1: Sempre comece com baseline simples âš¡
```python
# CLASSIFICAÃ‡ÃƒO
baseline = LogisticRegression()

# REGRESSÃƒO  
baseline = LinearRegression()
```
**Por quÃª?** DÃ¡ uma referÃªncia rÃ¡pida. Se modelos complexos nÃ£o batem baseline, tem problema nos dados!

### Etapa 2: Random Forest como segundo teste ğŸŒ²
```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
```
**Por quÃª?** Funciona bem "out of the box", robusto, dÃ¡ feature importance.

### Etapa 3: Se precisa mais accuracy, XGBoost ğŸš€
```python
import xgboost as xgb
model = xgb.XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1)
```
**Por quÃª?** Melhor performance, mas precisa tuning.

### Etapa 4: Hyperparameter Tuning ğŸ›ï¸
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3]
}

grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)
```

---

## ğŸ“‹ Checklist de DecisÃ£o RÃ¡pida

**Antes de escolher um algoritmo, pergunte:**

1. **Quantos dados tenho?**
   - < 1000 â†’ Logistic Regression, SVM
   - 1k-10k â†’ Random Forest
   - > 10k â†’ XGBoost, Neural Network

2. **Preciso interpretar o modelo?**
   - Sim â†’ Linear/Logistic Regression
   - Mais ou menos â†’ Random Forest (feature importance)
   - NÃ£o â†’ XGBoost, Neural Network

3. **Quanto tempo tenho para treinar?**
   - Pouco â†’ Logistic Regression, Random Forest
   - MÃ©dio â†’ Random Forest, XGBoost
   - Muito â†’ XGBoost tuning, Neural Networks

4. **Os dados sÃ£o lineares?**
   - Sim â†’ Linear/Logistic Regression
   - NÃ£o â†’ Random Forest, XGBoost, Neural Network
   - NÃ£o sei â†’ Teste baseline linear primeiro!

5. **Tenho outliers?**
   - Sim â†’ Random Forest (robusto)
   - NÃ£o â†’ Qualquer modelo

---

## ğŸ’¡ LiÃ§Ãµes Aprendidas nas Semanas 1-3

### âœ… O que Funcionou Bem

1. **ComeÃ§ar simples:** LinearRegression na Semana 1 foi excelente para aprender
2. **Feature Engineering:** TransformaÃ§Ãµes aumentaram accuracy de 76% â†’ 79% (Semana 2)
3. **Cross-Validation:** Evitou overfitting e deu confianÃ§a nos resultados
4. **Random Forest primeiro:** Quase sempre deu bons resultados iniciais
5. **XGBoost quando importa:** Melhor accuracy (85.1%) quando otimizado

### âš ï¸ Armadilhas Comuns

1. **Esquecer de escalar dados** â†’ SVM e Neural Networks precisam!
2. **Overfit em dados pequenos** â†’ Use cross-validation sempre
3. **NÃ£o fazer baseline** â†’ Como saber se modelo complexo vale a pena?
4. **Tuning excessivo antes de entender** â†’ Entenda o modelo primeiro
5. **Ignorar feature importance** â†’ Pode remover features desnecessÃ¡rias

---

## ğŸ§ª ExercÃ­cios PrÃ¡ticos

### ExercÃ­cio 1: Escolha o Algoritmo
Para cada cenÃ¡rio, escolha o melhor algoritmo inicial:

**A) Dataset:** 500 amostras, 5 features, prever se cliente compra (0/1)  
**Resposta:** `_______________`

**B) Dataset:** 50.000 amostras, 20 features, prever preÃ§o de casa  
**Resposta:** `_______________`

**C) Dataset:** 100 amostras, 1000 features (texto), classificaÃ§Ã£o de emails  
**Resposta:** `_______________`

**D) Dataset:** 10.000 amostras, 15 features, competiÃ§Ã£o Kaggle de classificaÃ§Ã£o  
**Resposta:** `_______________`

### ExercÃ­cio 2: Debug de Modelo
VocÃª treinou um XGBoost e o resultado Ã©:
- Train accuracy: 99%
- Test accuracy: 65%

**O que estÃ¡ acontecendo?** `_______________`  
**Como resolver?** `_______________`

---

## ğŸ“š Notebook PrÃ¡tico

Veja o notebook `01-revisao-algoritmos-ml.ipynb` para:
- âœ… ImplementaÃ§Ã£o lado-a-lado dos 6 algoritmos
- âœ… ComparaÃ§Ã£o visual de desempenho
- âœ… AnÃ¡lise de quando cada um funciona melhor
- âœ… Exemplos prÃ¡ticos com dados reais

---

## ğŸ¯ PrÃ³ximos Passos

- [ ] Completar notebook de revisÃ£o
- [ ] Fazer exercÃ­cios de escolha de algoritmos
- [ ] Revisar seus projetos anteriores com novo olhar
- [ ] Documentar suas prÃ³prias "regras de bolso"

---

## ğŸ“ ReflexÃµes Finais

*"O melhor algoritmo nÃ£o Ã© o mais complexo, Ã© aquele que resolve seu problema de forma confiÃ¡vel e mantÃ­vel."*

**Perguntas para reflexÃ£o:**
1. Consigo explicar quando usar Random Forest vs XGBoost?
2. Sei por que preciso escalar dados para SVM?
3. Entendo o tradeoff entre interpretabilidade e accuracy?

---

**Tempo Total:** TBD  
**PrÃ³ximo:** Dia 2 - Feature Engineering na PrÃ¡tica

*Lembre-se: CompreensÃ£o profunda > MemorizaÃ§Ã£o de cÃ³digo* ğŸ¯

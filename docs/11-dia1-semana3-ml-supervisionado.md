# üìä Semana 3, Dia 1: ML Supervisionado Avan√ßado

**Data:** 04 de Novembro de 2025  
**Dura√ß√£o:** TBD  
**Objetivo:** Implementar modelos ML avan√ßados e comparar performance

---

## üéØ **Objetivos do Dia**

### Aprendizado
- [x] Entender ensemble methods (Random Forest, Gradient Boosting)
- [ ] Compreender Support Vector Machines (SVM)
- [ ] Feature engineering avan√ßado
- [ ] M√©tricas de avalia√ß√£o detalhadas
- [ ] Visualiza√ß√µes profissionais

### Implementa√ß√£o
- [ ] Random Forest Classifier
- [ ] XGBoost/LightGBM
- [ ] Support Vector Machine (SVM)
- [ ] Perceptron Multicamadas (MLP)
- [ ] Compara√ß√£o estat√≠stica entre modelos

### Entreg√°veis
- [ ] Notebook completo com 4-5 modelos
- [ ] Visualiza√ß√µes comparativas
- [ ] Feature importance analysis
- [ ] Documenta√ß√£o t√©cnica

---

## üìö **Conceitos Te√≥ricos**

### 1. Ensemble Methods

#### Random Forest
**Conceito:** Combina m√∫ltiplas √°rvores de decis√£o para reduzir overfitting.

**Como funciona:**
1. Cria N √°rvores de decis√£o independentes
2. Cada √°rvore treina em uma amostra diferente dos dados (bootstrap)
3. Cada split considera apenas um subconjunto aleat√≥rio de features
4. Predi√ß√£o final = vota√ß√£o majorit√°ria (classifica√ß√£o) ou m√©dia (regress√£o)

**Vantagens:**
- ‚úÖ Robusto contra overfitting
- ‚úÖ Lida bem com features irrelevantes
- ‚úÖ Feature importance autom√°tico
- ‚úÖ Funciona bem "out of the box"

**Hiperpar√¢metros principais:**
- `n_estimators`: N√∫mero de √°rvores (padr√£o: 100)
- `max_depth`: Profundidade m√°xima das √°rvores
- `min_samples_split`: M√≠nimo de amostras para split
- `max_features`: Features consideradas em cada split

#### Gradient Boosting (XGBoost/LightGBM)
**Conceito:** Constr√≥i √°rvores sequencialmente, cada uma corrigindo erros da anterior.

**Como funciona:**
1. Treina primeira √°rvore nos dados
2. Calcula erros (res√≠duos)
3. Treina pr√≥xima √°rvore para prever esses erros
4. Repete N vezes, combinando todas as √°rvores
5. Usa gradient descent para otimizar

**Vantagens:**
- ‚úÖ Alta performance (frequentemente vence competi√ß√µes Kaggle)
- ‚úÖ Lida bem com dados desbalanceados
- ‚úÖ Regulariza√ß√£o embutida
- ‚úÖ Extremamente flex√≠vel

**XGBoost vs LightGBM:**
- **XGBoost:** Mais maduro, maior comunidade
- **LightGBM:** Mais r√°pido, menor uso de mem√≥ria

**Hiperpar√¢metros principais:**
- `n_estimators`: N√∫mero de √°rvores
- `learning_rate`: Taxa de aprendizado (0.01-0.3)
- `max_depth`: Profundidade das √°rvores
- `subsample`: Fra√ß√£o de amostras por √°rvore
- `colsample_bytree`: Fra√ß√£o de features por √°rvore

### 2. Support Vector Machines (SVM)

**Conceito:** Encontra o hiperplano que melhor separa as classes.

**Como funciona:**
1. Projeta dados em espa√ßo de alta dimens√£o (kernel trick)
2. Encontra hiperplano com m√°xima margem entre classes
3. Usa apenas "support vectors" (pontos mais pr√≥ximos da fronteira)

**Kernels:**
- **Linear:** Separa√ß√£o linear simples
- **RBF (Radial Basis Function):** N√£o-linear, mais comum
- **Polynomial:** N√£o-linear, grau configur√°vel
- **Sigmoid:** Similar a redes neurais

**Vantagens:**
- ‚úÖ Eficaz em alta dimensionalidade
- ‚úÖ Mem√≥ria eficiente (usa apenas support vectors)
- ‚úÖ Vers√°til (diferentes kernels)

**Desvantagens:**
- ‚ùå Lento com grandes datasets
- ‚ùå Sens√≠vel a escala dos dados (requer normaliza√ß√£o)
- ‚ùå Dif√≠cil interpretar

**Hiperpar√¢metros principais:**
- `C`: Regulariza√ß√£o (menor = mais regulariza√ß√£o)
- `kernel`: Tipo de kernel ('rbf', 'linear', 'poly')
- `gamma`: Coeficiente do kernel ('scale', 'auto', ou float)

### 3. Neural Networks (MLP)

**Conceito:** Rede de neur√¥nios artificiais em camadas.

**Arquitetura:**
```
Input Layer ‚Üí Hidden Layers ‚Üí Output Layer
```

**Como funciona:**
1. Cada neur√¥nio aplica: `output = activation(sum(weights * inputs) + bias)`
2. Backpropagation ajusta pesos para minimizar erro
3. M√∫ltiplas camadas permitem aprender rela√ß√µes complexas

**Vantagens:**
- ‚úÖ Aprende rela√ß√µes n√£o-lineares complexas
- ‚úÖ Flex√≠vel (arquitetura customiz√°vel)
- ‚úÖ Pode ser muito poderoso

**Desvantagens:**
- ‚ùå Requer mais dados
- ‚ùå Risco de overfitting
- ‚ùå Dif√≠cil interpretar
- ‚ùå Muitos hiperpar√¢metros

---

## üîß **Implementa√ß√£o**

### Setup Inicial

```python
# Imports principais
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Modelos
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier

# M√©tricas
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score
)

# Valida√ß√£o
from sklearn.model_selection import train_test_split, cross_val_score

# Preprocessing
from sklearn.preprocessing import StandardScaler

# Visualiza√ß√µes avan√ßadas
import plotly.express as px
import plotly.graph_objects as go

# XGBoost/LightGBM (se instalados)
try:
    import xgboost as xgb
    import lightgbm as lgb
    BOOSTING_AVAILABLE = True
except ImportError:
    BOOSTING_AVAILABLE = False
    print("XGBoost/LightGBM n√£o dispon√≠veis. Instale com:")
    print("pip install xgboost lightgbm")
```

---

## üìä **Estrutura do Notebook**

### 1. Carregamento e Prepara√ß√£o dos Dados
- Carregar dataset Titanic (ou outro)
- Feature engineering
- Tratamento de valores faltantes
- Encoding de vari√°veis categ√≥ricas
- Split train/test
- **Normaliza√ß√£o (importante para SVM e MLP!)**

### 2. Baseline Model
- Treinar Decision Tree simples como baseline
- Avaliar m√©tricas b√°sicas

### 3. Random Forest
- Treinar com hiperpar√¢metros padr√£o
- Analisar feature importance
- Avaliar performance

### 4. Gradient Boosting
- Treinar XGBoost (se dispon√≠vel)
- Treinar LightGBM (se dispon√≠vel)
- Comparar com Random Forest

### 5. Support Vector Machine
- **Normalizar dados (StandardScaler)**
- Testar kernel RBF
- Avaliar performance

### 6. Neural Network (MLP)
- **Normalizar dados**
- Definir arquitetura (ex: 100-50-25)
- Treinar com early stopping
- Avaliar performance

### 7. Compara√ß√£o Final
- Tabela comparativa de m√©tricas
- Gr√°ficos de barras
- Confusion matrices lado a lado
- ROC curves sobrepostas
- An√°lise de trade-offs

---

## üìà **Visualiza√ß√µes Esperadas**

### 1. Feature Importance (Random Forest)
```python
# Extrair import√¢ncias
importances = rf_model.feature_importances_
feature_names = X_train.columns

# Criar DataFrame
feat_imp_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

# Plotar
plt.figure(figsize=(10, 6))
sns.barplot(data=feat_imp_df, x='importance', y='feature')
plt.title('Feature Importance - Random Forest')
plt.tight_layout()
plt.show()
```

### 2. Confusion Matrix Comparativa
```python
# Para cada modelo
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
models = [dt, rf, xgb, svm, mlp]
names = ['Decision Tree', 'Random Forest', 'XGBoost', 'SVM', 'MLP']

for ax, model, name in zip(axes.flat, models, names):
    cm = confusion_matrix(y_test, model.predict(X_test))
    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')
    ax.set_title(f'{name}\nAccuracy: {accuracy_score(y_test, model.predict(X_test)):.3f}')
```

### 3. ROC Curves
```python
plt.figure(figsize=(10, 8))

for model, name in zip(models, names):
    y_proba = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    auc = roc_auc_score(y_test, y_proba)
    plt.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves Comparison')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 4. Compara√ß√£o de M√©tricas
```python
# Criar DataFrame de resultados
results = []
for model, name in zip(models, names):
    y_pred = model.predict(X_test)
    results.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred)
    })

results_df = pd.DataFrame(results)

# Plotar
results_df.set_index('Model').plot(kind='bar', figsize=(12, 6))
plt.title('Model Performance Comparison')
plt.ylabel('Score')
plt.legend(loc='lower right')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## üéØ **Crit√©rios de Sucesso**

### Performance
- [ ] Accuracy > 85% no test set
- [ ] F1-Score > 0.83
- [ ] AUC > 0.90
- [ ] Pelo menos 1 modelo supera baseline em 5%+

### C√≥digo
- [ ] C√≥digo limpo e comentado
- [ ] Fun√ß√µes reutiliz√°veis
- [ ] Sem warnings
- [ ] Reproduz√≠vel (random_state fixo)

### Visualiza√ß√µes
- [ ] 5+ gr√°ficos profissionais
- [ ] Todos com t√≠tulos e labels
- [ ] Paleta de cores consistente
- [ ] Compara√ß√µes claras

### Documenta√ß√£o
- [ ] Explica√ß√£o de cada modelo
- [ ] Interpreta√ß√£o dos resultados
- [ ] An√°lise de trade-offs
- [ ] Recomenda√ß√µes finais

---

## üìù **Cronograma da Sess√£o**

| Tempo | Atividade |
|-------|-----------|
| 0-15min | Setup + revis√£o Semana 2 |
| 15-30min | Feature engineering avan√ßado |
| 30-60min | Random Forest + an√°lise |
| 60-90min | Gradient Boosting (XGB/LGB) |
| 90-120min | SVM + MLP |
| 120-150min | Compara√ß√£o e visualiza√ß√µes |
| 150-180min | Documenta√ß√£o e conclus√µes |

---

## üéì **Aprendizados do Dia**

_A ser preenchido ao final da sess√£o..._

### Insights T√©cnicos
- TBD

### Desafios Encontrados
- TBD

### Pr√≥ximos Passos
- TBD

---

## üîó **Recursos Adicionais**

### Documenta√ß√£o Oficial
- [Scikit-learn Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [SVM Guide](https://scikit-learn.org/stable/modules/svm.html)

### Tutoriais Recomendados
- Random Forest: [StatQuest Video](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)
- Gradient Boosting: [XGBoost Tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)
- SVM: [Understanding SVM](https://www.youtube.com/watch?v=efR1C6CvhmE)

---

**Status:** üîÑ Em andamento  
**Pr√≥xima atualiza√ß√£o:** Fim da sess√£o Dia 1

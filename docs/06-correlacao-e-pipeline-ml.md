# üîó Correla√ß√£o e Pipeline de Machine Learning

**Data:** 28 Outubro 2025  
**Contexto:** Semana 2 - Dia 1 (Insights do Titanic)

---

## üéØ **O que √© Correla√ß√£o?**

**Correla√ß√£o** mede **como duas vari√°veis se relacionam**.

### **Escala de Valores**
- **-1 a +1**
- **Quanto mais pr√≥ximo de 1 ou -1** ‚Üí Rela√ß√£o mais forte
- **Pr√≥ximo de 0** ‚Üí Pouca ou nenhuma rela√ß√£o

---

## üìä **Interpretando Correla√ß√£o**

### **Escala de Cores (Heatmap)**

| Cor | Valor | Significado | Exemplo |
|-----|-------|-------------|---------|
| üî¥ Vermelho escuro | **+1.00** | Correla√ß√£o PERFEITA positiva | Temperatura ¬∞C vs ¬∞F |
| üü† Laranja | **+0.5 a +0.8** | Correla√ß√£o FORTE positiva | Altura vs Peso |
| üü° Bege claro | **+0.2 a +0.4** | Correla√ß√£o FRACA positiva | Pre√ßo bilhete vs Sobreviv√™ncia |
| ‚ö™ Cinza | **0.0** | SEM correla√ß√£o | Idade vs Cor de cabelo |
| üîµ Azul claro | **-0.2 a -0.4** | Correla√ß√£o FRACA negativa | Classe vs Sobreviv√™ncia |
| üîµ Azul escuro | **-0.5 a -0.8** | Correla√ß√£o FORTE negativa | Classe vs Pre√ßo bilhete |
| üü£ Roxo | **-1.00** | Correla√ß√£o PERFEITA negativa | Altitude vs Temperatura |

---

## üí° **Tipos de Correla√ß√£o (Exemplos Visuais)**

### **Correla√ß√£o Positiva (+0.8)**
```
Vari√°vel A ‚Üë ‚Üí Vari√°vel B ‚Üë
Exemplo: Pre√ßo do bilhete ‚Üë ‚Üí Sobreviv√™ncia ‚Üë
```
**Interpreta√ß√£o:** Quando uma aumenta, a outra tamb√©m aumenta

### **Correla√ß√£o Negativa (-0.8)**
```
Vari√°vel A ‚Üë ‚Üí Vari√°vel B ‚Üì
Exemplo: Classe ‚Üë (pior) ‚Üí Sobreviv√™ncia ‚Üì
```
**Interpreta√ß√£o:** Quando uma aumenta, a outra diminui

### **Sem Correla√ß√£o (0.0)**
```
Vari√°vel A ‚Üë‚Üì ‚Üí Vari√°vel B ‚Üë‚Üì (aleat√≥rio)
Exemplo: Idade ‚Üë‚Üì ‚Üí Sobreviv√™ncia ‚Üë‚Üì
```
**Interpreta√ß√£o:** N√£o existe rela√ß√£o aparente

---

## üö¢ **Exemplo Pr√°tico: Titanic**

### **Mapa de Correla√ß√£o Analisado**

| Vari√°veis | Correla√ß√£o | Interpreta√ß√£o |
|-----------|-----------|---------------|
| **survived √ó pclass** | **-0.34** | üî• Classe social foi decisiva! Quanto pior a classe, menor sobreviv√™ncia |
| **survived √ó fare** | **+0.26** | üí∞ Pre√ßo indica classe. Bilhetes caros = mais sobreviv√™ncia |
| **survived √ó age** | **-0.06** | üòê Idade quase n√£o influenciou. "Mulheres e crian√ßas primeiro" n√£o foi dominante |
| **survived √ó sibsp** | **-0.04** | üòê Ter irm√£os/c√¥njuges n√£o ajudou muito |
| **survived √ó parch** | **+0.08** | üòê Ter filhos ajudou um pouquinho |
| **pclass √ó fare** | **-0.55** | üîÑ Redund√¢ncia! Classe e pre√ßo s√£o quase a mesma informa√ß√£o |
| **sibsp √ó parch** | **+0.41** | üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Fam√≠lias grandes viajavam juntas |

### **Insights Principais**

‚úÖ **Dinheiro/Classe social salvou mais vidas que idade!**  
‚úÖ **Features mais importantes:** `pclass` e `fare`  
‚úÖ **Features fracas:** `age`, `sibsp`, `parch`  
‚úÖ **Redund√¢ncia detectada:** `pclass` ‚âà `fare` (usar apenas uma)

---

## ‚ö†Ô∏è **CUIDADOS com Correla√ß√£o**

### **1. Correla√ß√£o ‚â† Causa√ß√£o**

```
‚ùå ERRADO:
"Vendas de sorvete ‚Üë ‚Üí Afogamentos ‚Üë"
Correla√ß√£o: SIM (+0.8)
Causa√ß√£o: N√ÉO! (ambos aumentam no ver√£o)

‚úÖ CORRETO:
"Temperatura ‚Üë ‚Üí Sorvete ‚Üë"
"Temperatura ‚Üë ‚Üí Mais praia ‚Üí Afogamentos ‚Üë"
```

**Regra de ouro:** Correla√ß√£o mostra rela√ß√£o, mas n√£o prova que uma **causa** a outra.

### **2. Correla√ß√£o mede apenas rela√ß√£o LINEAR**

```python
# Rela√ß√£o n√£o-linear pode ter correla√ß√£o baixa
x = [1, 2, 3, 4, 5]
y = [1, 4, 9, 16, 25]  # y = x¬≤

# Correla√ß√£o linear pode ser 0.8
# Mas rela√ß√£o real √© x¬≤!
```

**Solu√ß√£o:** Use scatter plots para ver rela√ß√µes n√£o-lineares.

### **3. Outliers distorcem correla√ß√£o**

```python
# Dados normais: correla√ß√£o = 0.3
# Adiciona 1 outlier extremo
# Nova correla√ß√£o: 0.9 (falso positivo!)
```

**Solu√ß√£o:** Remover outliers antes de calcular correla√ß√£o.

---

## üîÑ **Pipeline Completo de Machine Learning**

### **1Ô∏è‚É£ ENTENDER O PROBLEMA**
```
‚ùì Qual pergunta queremos responder?
   ‚Üí "Quem sobreviveu no Titanic?"
   
üéØ Tipo de problema?
   ‚Üí Classifica√ß√£o (sim/n√£o)
   ‚Üí Regress√£o (valor num√©rico)
   ‚Üí Clustering (agrupar)
```

### **2Ô∏è‚É£ COLETAR DADOS**
```
üìä Obter o dataset
   ‚Üí CSV, banco de dados, API, web scraping
   
‚úÖ Verificar:
   ‚Üí Tamanho suficiente? (m√≠nimo 100-1000 linhas)
   ‚Üí Qualidade boa? (poucos nulos/erros)
```

### **3Ô∏è‚É£ EDA - AN√ÅLISE EXPLORAT√ìRIA** ‚≠ê

```python
# a) Carregar dados
df = pd.read_csv('dados.csv')

# b) An√°lise inicial
df.info()        # Tipos, nulos, mem√≥ria
df.describe()    # Estat√≠sticas (m√©dia, min, max)
df.head()        # Ver primeiras linhas

# c) Visualizar distribui√ß√µes
df['idade'].hist()                    # Histograma
sns.countplot(data=df, x='classe')    # Contagem

# d) Verificar valores nulos
df.isnull().sum()
df.isnull().sum() / len(df) * 100    # Porcentagem

# e) Analisar correla√ß√µes ‚Üê AQUI!
correlation = df.corr()
sns.heatmap(correlation, annot=True)

# f) Gerar insights
# "Classe social > Idade"
# "Pre√ßo e classe s√£o redundantes"
```

**Tempo estimado:** 30-50% do projeto

### **4Ô∏è‚É£ LIMPEZA E PREPARA√á√ÉO**

```python
# a) Tratar valores nulos
df['age'].fillna(df['age'].median(), inplace=True)  # Mediana
df['embarked'].fillna(df['embarked'].mode()[0])     # Moda
df.dropna(subset=['critical_column'])               # Remover

# b) Remover outliers
Q1 = df['fare'].quantile(0.25)
Q3 = df['fare'].quantile(0.75)
IQR = Q3 - Q1
df = df[(df['fare'] >= Q1 - 1.5*IQR) & (df['fare'] <= Q3 + 1.5*IQR)]

# c) Encoding (texto ‚Üí n√∫mero)
df['sex_numeric'] = (df['sex'] == 'male').astype(int)
df['embarked_encoded'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})

# d) Normaliza√ß√£o (se necess√°rio)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['age', 'fare']] = scaler.fit_transform(df[['age', 'fare']])
```

### **5Ô∏è‚É£ FEATURE ENGINEERING**

```python
# Criar novas vari√°veis √∫teis baseadas nas existentes

# Titanic - Exemplos:
df['family_size'] = df['sibsp'] + df['parch'] + 1
df['is_alone'] = (df['family_size'] == 1).astype(int)
df['age_group'] = pd.cut(df['age'], bins=[0, 12, 18, 60, 100], 
                         labels=['crian√ßa', 'adolescente', 'adulto', 'idoso'])

# Dicas:
# - Combine vari√°veis relacionadas
# - Crie categorias de faixas
# - Extraia informa√ß√µes (ex: nome ‚Üí t√≠tulo)
```

### **6Ô∏è‚É£ SELECIONAR FEATURES**

```python
# Baseado na correla√ß√£o + conhecimento de dom√≠nio

# ‚úÖ INCLUIR:
features = ['pclass', 'sex_numeric', 'fare', 'family_size', 'is_alone']
# - Alta correla√ß√£o com target (> 0.2 ou < -0.2)
# - Faz sentido para o problema

# ‚ùå REMOVER:
# - Identificadores √∫nicos (name, ticket)
# - Correla√ß√£o muito fraca (< 0.1)
# - Redundantes (pclass e fare s√£o similares, escolha 1)

X = df[features]
y = df['survived']
```

### **7Ô∏è‚É£ DIVIDIR DADOS (Train/Test Split)**

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 20% para teste
    random_state=42     # Reproduzibilidade
)

# 80% ‚Üí Treinar modelo (ensinar)
# 20% ‚Üí Testar modelo (validar)
```

### **8Ô∏è‚É£ TREINAR MODELOS** ü§ñ

```python
# Testar m√∫ltiplos modelos

# Modelo 1: Logistic Regression
from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

# Modelo 2: Decision Tree
from sklearn.tree import DecisionTreeClassifier
dt_model = DecisionTreeClassifier(max_depth=5)
dt_model.fit(X_train, y_train)

# Modelo 3: Random Forest
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)
```

### **9Ô∏è‚É£ AVALIAR MODELOS**

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Fazer predi√ß√µes
predictions = lr_model.predict(X_test)

# Calcular m√©tricas
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
f1 = f1_score(y_test, predictions)

print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-Score:  {f1:.4f}")

# Comparar modelos e escolher o melhor
```

**M√©tricas - Quando usar:**
- **Accuracy:** Dados balanceados (50% sim, 50% n√£o)
- **Precision:** Custo de falso positivo √© alto (ex: spam)
- **Recall:** Custo de falso negativo √© alto (ex: doen√ßa)
- **F1-Score:** Balancear precision e recall

### **üîü MELHORAR (Itera√ß√£o)**

```python
# a) Ajustar hiperpar√¢metros
from sklearn.model_selection import GridSearchCV

params = {
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(DecisionTreeClassifier(), params, cv=5)
grid.fit(X_train, y_train)
best_model = grid.best_estimator_

# b) Criar novas features
# c) Remover features ruins
# d) Testar outros algoritmos
# e) Coletar mais dados
```

### **1Ô∏è‚É£1Ô∏è‚É£ DEPLOY (Produ√ß√£o)**

```python
# Salvar modelo treinado
import pickle

with open('modelo_titanic.pkl', 'wb') as file:
    pickle.dump(best_model, file)

# Carregar depois
with open('modelo_titanic.pkl', 'rb') as file:
    modelo = pickle.load(file)

# Usar para predi√ß√µes novas
novo_passageiro = [[3, 1, 7.25, 1, 1]]  # pclass, sex, fare, family_size, is_alone
predicao = modelo.predict(novo_passageiro)
print(f"Sobreviveu? {predicao[0]}")
```

---

## üéØ **Papel da Correla√ß√£o no Pipeline**

### **Onde a correla√ß√£o entra?**

```
1. Entender problema
2. Coletar dados
3. An√°lise inicial (info, describe, visualizar)
4. Verificar nulos
5. CORRELA√á√ÉO ‚Üê Etapa 3e (EDA)
6. Limpeza
7. Feature Engineering
8. Selecionar features (usa correla√ß√£o!)
9. Treinar ML
```

### **Para que serve?**

‚úÖ **Identificar features relevantes** (correla√ß√£o forte com target)  
‚úÖ **Detectar redund√¢ncia** (features correlacionadas entre si)  
‚úÖ **Gerar insights** (entender rela√ß√µes nos dados)  
‚úÖ **Guiar feature engineering** (combinar vari√°veis relacionadas)

### **Limita√ß√µes**

‚ùå N√£o detecta rela√ß√µes n√£o-lineares  
‚ùå N√£o prova causalidade  
‚ùå Sens√≠vel a outliers  
‚ùå S√≥ funciona para vari√°veis num√©ricas

---

## üí™ **Como Usar Correla√ß√£o Efetivamente**

### **1. Selecionar Features Importantes**

```python
# Pegar correla√ß√£o com target
correlation_with_target = df.corr()['survived'].abs().sort_values(ascending=False)

print(correlation_with_target)
# survived    1.00  ‚Üê √ìbvio
# pclass     0.34   ‚Üê USAR! üî•
# fare       0.26   ‚Üê USAR! üî•
# parch      0.08   ‚Üê Fraco, talvez remover
# age        0.06   ‚Üê Fraco, talvez remover
# sibsp      0.04   ‚Üê Fraco, talvez remover

# Filtrar features com correla√ß√£o > 0.2
strong_features = correlation_with_target[correlation_with_target > 0.2].index.tolist()
```

### **2. Remover Redund√¢ncia**

```python
# Encontrar features muito correlacionadas entre si
correlation_matrix = df.corr().abs()

# Pares com correla√ß√£o > 0.7
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if correlation_matrix.iloc[i, j] > 0.7:
            high_corr_pairs.append((correlation_matrix.columns[i], 
                                   correlation_matrix.columns[j], 
                                   correlation_matrix.iloc[i, j]))

print(high_corr_pairs)
# [('pclass', 'fare', 0.55)]  ‚Üê Usar apenas 1 delas!
```

### **3. Validar Hip√≥teses**

```python
# Hip√≥tese: "Classe social influenciou mais que idade"
print(f"Correla√ß√£o pclass: {df['survived'].corr(df['pclass']):.2f}")  # -0.34
print(f"Correla√ß√£o age: {df['survived'].corr(df['age']):.2f}")        # -0.06

# Conclus√£o: Hip√≥tese confirmada! ‚úÖ
```

---

## üìö **Resumo Executivo**

### **Correla√ß√£o √© importante?**
‚úÖ **SIM!** √â ferramenta essencial para EDA e sele√ß√£o de features.

### **√â o primeiro passo?**
‚ùå **N√ÉO!** Vem depois de entender dados e visualizar.

### **Quando usar?**
- Durante EDA (etapa 3e)
- Para selecionar features (etapa 6)
- Para detectar redund√¢ncia
- Para gerar insights

### **Limita√ß√µes:**
- N√£o detecta n√£o-linearidade
- N√£o prova causa√ß√£o
- Sens√≠vel a outliers

---

## üéì **Analogia Final: Receita de Bolo**

| Etapa ML | Analogia |
|----------|----------|
| **Entender problema** | Decidir que bolo fazer |
| **Coletar dados** | Comprar ingredientes |
| **EDA inicial** | Ler receita b√°sica |
| **Correla√ß√£o** | Descobrir o que combina (a√ß√∫car deixa doce, sal estraga) |
| **Limpeza** | Peneirar farinha, quebrar ovos |
| **Feature Engineering** | Criar cobertura especial |
| **Selecionar features** | Escolher melhores ingredientes |
| **Train/Test split** | Separar massa para testar |
| **Treinar ML** | Assar o bolo |
| **Avaliar** | Provar e ajustar receita |
| **Deploy** | Servir o bolo! |

---

## ‚úÖ **Checklist: Pipeline Completo**

- [ ] 1. Definir problema (classifica√ß√£o/regress√£o/clustering)
- [ ] 2. Coletar dados (CSV, API, banco)
- [ ] 3. EDA - An√°lise explorat√≥ria
  - [ ] df.info(), df.describe(), df.head()
  - [ ] Visualiza√ß√µes (histogramas, countplots)
  - [ ] Verificar nulos
  - [ ] **Mapa de correla√ß√£o** ‚≠ê
  - [ ] Gerar insights
- [ ] 4. Limpeza (nulos, outliers, encoding)
- [ ] 5. Feature Engineering (criar vari√°veis)
- [ ] 6. Selecionar features (baseado em correla√ß√£o)
- [ ] 7. Train/Test split (80/20)
- [ ] 8. Treinar m√∫ltiplos modelos
- [ ] 9. Avaliar m√©tricas (accuracy, precision, recall, F1)
- [ ] 10. Melhorar (hiperpar√¢metros, features)
- [ ] 11. Deploy (salvar modelo, criar API)

---

**üöÄ Pipeline completo documentado e pronto para refer√™ncia futura!**

# ğŸ§ª Teste de ValidaÃ§Ã£o - Semana 3: ML Supervisionado AvanÃ§ado

## â±ï¸ Tempo estimado total: 40 minutos
- Dia 1 (Modelos AvanÃ§ados): 20 minutos
- Dia 2 (Hyperparameter Tuning): 20 minutos

---

# ğŸ“… DIA 1: Modelos AvanÃ§ados

## ğŸ“‹ Parte 1: Conceitos (mÃºltipla escolha)

### QuestÃ£o 1: Ensemble Methods
VocÃª tem um dataset pequeno (500 amostras) e precisa de alta acurÃ¡cia. Qual modelo seria mais adequado?

**A)** Decision Tree (Ã¡rvore Ãºnica)  
**B)** Random Forest  
**C)** XGBoost  
**D)** SVM  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Random Forest**

**Por quÃª:**
- Decision Tree: Tende a overfitting com poucos dados
- **Random Forest: Reduz overfitting usando vÃ¡rias Ã¡rvores, ideal para dados pequenos**
- XGBoost: Excelente, mas pode ser overkill e mais complexo de configurar
- SVM: Bom tambÃ©m, mas precisa de normalizaÃ§Ã£o e ajuste de hiperparÃ¢metros

**Conceito-chave:** Random Forest Ã© o "safe choice" para maioria dos problemas tabulares.
</details>

---

### QuestÃ£o 2: NormalizaÃ§Ã£o
VocÃª vai treinar 3 modelos: Random Forest, SVM e MLP. Para quais vocÃª DEVE aplicar StandardScaler?

**A)** Apenas Random Forest  
**B)** Apenas SVM  
**C)** SVM e MLP  
**D)** Todos os trÃªs  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: C) SVM e MLP**

**Por quÃª:**
- **Random Forest:** Baseado em Ã¡rvores, **nÃ£o Ã© afetado** por escala
- **SVM:** Usa distÃ¢ncias euclidianas, **sensÃ­vel Ã  escala**
- **MLP:** Usa gradientes, **sensÃ­vel Ã  escala**

**CÃ³digo correto:**
```python
# âŒ ERRADO
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
# Treinar todos os modelos com X_train_scaled

# âœ… CERTO
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Random Forest: usa X_train original
# SVM e MLP: usa X_train_scaled
```

**Conceito-chave:** Modelos baseados em distÃ¢ncia precisam de normalizaÃ§Ã£o.
</details>

---

### QuestÃ£o 3: Feature Importance
VocÃª treinou um Random Forest e a feature `age` tem importÃ¢ncia 0.35 e `fare` tem 0.05. O que isso significa?

**A)** `age` Ã© 7x mais importante que `fare`  
**B)** Devo remover `fare` do modelo  
**C)** `age` contribui 35% para as decisÃµes do modelo  
**D)** Devo normalizar `age` porque o valor Ã© muito alto  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: C) `age` contribui 35% para as decisÃµes do modelo**

**Por quÃª:**
- A) Incorreto: NÃ£o Ã© uma relaÃ§Ã£o linear direta de "vezes"
- B) Incorreto: Features com baixa importÃ¢ncia ainda podem ser Ãºteis
- **C) Correto: Feature importance soma 1.0 (100%), entÃ£o 0.35 = 35% de contribuiÃ§Ã£o**
- D) Incorreto: Valor alto de importance Ã© DESEJÃVEL, nÃ£o um problema

**Quando remover features:**
- ImportÃ¢ncia prÃ³xima de **zero** (<0.001)
- **Causa overfitting** (modelo muito complexo)
- **Custo de coleta** Ã© alto demais para o benefÃ­cio

**Conceito-chave:** Feature importance mostra contribuiÃ§Ã£o relativa, nÃ£o valor absoluto.
</details>

---

### QuestÃ£o 4: Overfitting
Seu modelo tem:
- **Train accuracy:** 98%
- **Test accuracy:** 75%

O que estÃ¡ acontecendo e como corrigir?

**A)** Underfitting - aumentar complexidade do modelo  
**B)** Overfitting - reduzir complexidade ou adicionar regularizaÃ§Ã£o  
**C)** Modelo perfeito - nenhuma aÃ§Ã£o necessÃ¡ria  
**D)** Dataset ruim - coletar mais dados  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Overfitting - reduzir complexidade ou adicionar regularizaÃ§Ã£o**

**Por quÃª:**
Grande diferenÃ§a entre train e test (23%) indica **overfitting** = modelo decorou os dados de treino.

**SoluÃ§Ãµes prÃ¡ticas:**

1. **Random Forest:**
```python
# âŒ Overfitting
RandomForestClassifier(n_estimators=200, max_depth=None)

# âœ… Melhor
RandomForestClassifier(
    n_estimators=100,
    max_depth=10,        # Limita profundidade
    min_samples_split=20 # Exige mais amostras para dividir
)
```

2. **Neural Network:**
```python
# âŒ Overfitting
MLPClassifier(hidden_layer_sizes=(200, 100, 50))

# âœ… Melhor
MLPClassifier(
    hidden_layer_sizes=(50, 25),  # Menos neurÃ´nios
    early_stopping=True,           # Para quando nÃ£o melhora
    validation_fraction=0.2        # Valida durante treino
)
```

**Conceito-chave:** Train >> Test = Overfitting. Simplificar ou regularizar.
</details>

---

## ğŸ–¥ï¸ Parte 2: PrÃ¡tica (cÃ³digo)

### Desafio: Completar o Pipeline

VocÃª tem este cÃ³digo com **3 erros**. Encontre e corrija:

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Carregar dados
df = pd.read_csv('dataset.csv')
X = df.drop('target', axis=1)
y = df['target']

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Normalizar
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)  # âŒ ERRO 1

# Treinar Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train_scaled, y_train)  # âŒ ERRO 2
y_pred_rf = rf_model.predict(X_test_scaled)

# Treinar SVM
svm_model = SVC()
svm_model.fit(X_train, y_train)  # âŒ ERRO 3
y_pred_svm = svm_model.predict(X_test_scaled)

# Avaliar
print(f"RF Accuracy: {accuracy_score(y_test, y_pred_rf)}")
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm)}")
```

<details>
<summary>ğŸ’¡ Ver resposta e explicaÃ§Ã£o</summary>

### Erros e CorreÃ§Ãµes:

#### âŒ ERRO 1: `scaler.fit_transform(X_test)`
```python
# ERRADO
X_test_scaled = scaler.fit_transform(X_test)

# CERTO
X_test_scaled = scaler.transform(X_test)
```
**Por quÃª:** `fit_transform` recalcula mÃ©dia/desvio no test set = **data leakage**!  
**Regra:** `fit` apenas no train, `transform` no test.

---

#### âŒ ERRO 2: Random Forest com dados normalizados
```python
# ERRADO
rf_model.fit(X_train_scaled, y_train)

# CERTO
rf_model.fit(X_train, y_train)  # Dados originais
```
**Por quÃª:** Random Forest nÃ£o precisa de normalizaÃ§Ã£o (baseado em thresholds, nÃ£o distÃ¢ncias).

---

#### âŒ ERRO 3: SVM sem normalizaÃ§Ã£o
```python
# ERRADO
svm_model.fit(X_train, y_train)

# CERTO
svm_model.fit(X_train_scaled, y_train)
```
**Por quÃª:** SVM Ã© **sensÃ­vel Ã  escala** das features (usa distÃ¢ncias).

---

### âœ… CÃ³digo Correto Completo:

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Carregar dados
df = pd.read_csv('dataset.csv')
X = df.drop('target', axis=1)
y = df['target']

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42  # âœ… Adicione random_state!
)

# Normalizar
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # âœ… transform, nÃ£o fit_transform

# Treinar Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)  # âœ… Dados originais
y_pred_rf = rf_model.predict(X_test)  # âœ… Dados originais

# Treinar SVM
svm_model = SVC(random_state=42)
svm_model.fit(X_train_scaled, y_train)  # âœ… Dados normalizados
y_pred_svm = svm_model.predict(X_test_scaled)  # âœ… Dados normalizados

# Avaliar
print(f"RF Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
```

### ğŸ“Š Checklist de ValidaÃ§Ã£o:

| Modelo | Dados de Treino | Dados de Teste | Por quÃª? |
|--------|----------------|----------------|----------|
| Random Forest | `X_train` | `X_test` | NÃ£o precisa normalizaÃ§Ã£o |
| XGBoost | `X_train` | `X_test` | NÃ£o precisa normalizaÃ§Ã£o |
| SVM | `X_train_scaled` | `X_test_scaled` | Precisa normalizaÃ§Ã£o |
| MLP | `X_train_scaled` | `X_test_scaled` | Precisa normalizaÃ§Ã£o |

</details>

---

## ğŸ“Š Parte 3: InterpretaÃ§Ã£o de Resultados

VocÃª treinou 4 modelos no Titanic e obteve:

| Modelo | Train Acc | Test Acc | CV Score | Tempo (s) |
|--------|-----------|----------|----------|-----------|
| Decision Tree | 98.5% | 76.2% | 75.1% Â± 3.2% | 0.05 |
| Random Forest | 95.3% | 83.8% | 82.9% Â± 2.1% | 1.2 |
| XGBoost | 96.8% | 85.1% | 84.3% Â± 1.8% | 2.5 |
| MLP | 89.2% | 81.5% | 80.2% Â± 4.5% | 3.8 |

### QuestÃ£o: Qual modelo vocÃª escolheria para produÃ§Ã£o? Por quÃª?

<details>
<summary>ğŸ’¡ Ver resposta e anÃ¡lise</summary>

### ğŸ† Resposta: **XGBoost**

### ğŸ“Š AnÃ¡lise Detalhada:

#### 1. **Decision Tree** âŒ
- **Train 98.5% vs Test 76.2%** = Overfitting severo (22.3% diferenÃ§a!)
- CV score baixo e alta variÃ¢ncia (Â±3.2%)
- **Veredicto:** NÃ£o confiÃ¡vel para produÃ§Ã£o

#### 2. **Random Forest** âœ… (2Âª opÃ§Ã£o)
- Train vs Test: 95.3% - 83.8% = 11.5% (razoÃ¡vel)
- Boa estabilidade (CV Â±2.1%)
- **RÃ¡pido** (1.2s)
- **Veredicto:** Excelente opÃ§Ã£o se tempo de inferÃªncia Ã© crÃ­tico

#### 3. **XGBoost** ğŸ† (MELHOR)
- **Melhor test accuracy:** 85.1%
- **Melhor CV score:** 84.3%
- **Menor variÃ¢ncia:** Â±1.8% (mais estÃ¡vel)
- Train vs Test: 96.8% - 85.1% = 11.7% (similar ao RF)
- Tempo aceitÃ¡vel (2.5s para treino)
- **Veredicto:** Melhor generalizaÃ§Ã£o + estabilidade

#### 4. **MLP** âš ï¸
- Performance OK (81.5%)
- **Problema:** Alta variÃ¢ncia (Â±4.5%) = instÃ¡vel
- Mais lento (3.8s)
- Train-Test gap pequeno (7.7%), mas CV ruim
- **Veredicto:** Necessita mais tuning de hiperparÃ¢metros

---

### ğŸ¯ CritÃ©rios de DecisÃ£o:

```
Escolha baseado em:

1. Test Accuracy (principal mÃ©trica)
   âœ… XGBoost: 85.1%

2. Estabilidade (CV variance)
   âœ… XGBoost: Â±1.8%

3. GeneralizaÃ§Ã£o (Train-Test gap)
   âœ… RF e XGBoost: ~11%

4. Tempo de treino/inferÃªncia
   âœ… RF: 1.2s (se crÃ­tico)

5. Interpretabilidade
   âœ… RF: Mais fÃ¡cil de explicar
```

### ğŸ’¼ DecisÃ£o para ProduÃ§Ã£o:

**CenÃ¡rio 1: Performance mÃ¡xima**
â†’ **XGBoost** (85.1% acc, mais estÃ¡vel)

**CenÃ¡rio 2: Tempo real crÃ­tico**
â†’ **Random Forest** (83.8% acc, 2x mais rÃ¡pido)

**CenÃ¡rio 3: Interpretabilidade crÃ­tica**
â†’ **Random Forest** (feature importance mais clara)

---

### âš ï¸ Red Flags a Observar:

1. **Train >> Test** (>15% diferenÃ§a) = Overfitting
2. **CV variance alta** (>3%) = Modelo instÃ¡vel
3. **Test < 80%** no Titanic = Modelo fraco
4. **Tempo > 10s** = Pode nÃ£o escalar

</details>

---

## ğŸ“ Gabarito de Auto-AvaliaÃ§Ã£o

### PontuaÃ§Ã£o:

- **Parte 1 (Conceitos):** 4 questÃµes Ã— 2 pontos = **8 pontos**
- **Parte 2 (CÃ³digo):** 3 erros Ã— 2 pontos = **6 pontos**  
- **Parte 3 (InterpretaÃ§Ã£o):** 1 questÃ£o Ã— 6 pontos = **6 pontos**

**TOTAL:** 20 pontos

---

### ğŸ“Š InterpretaÃ§Ã£o da sua nota:

#### ğŸ† 18-20 pontos: EXCELENTE!
**VocÃª estÃ¡ PRONTO para Semana 4!**

âœ… Domina os conceitos principais  
âœ… Identifica erros comuns  
âœ… Interpreta resultados corretamente  

**PrÃ³ximos passos:**
- AvanÃ§ar para Semana 4 com confianÃ§a
- Considere fazer um mini-projeto de consolidaÃ§Ã£o (opcional)

---

#### ğŸ’ª 14-17 pontos: BOM!
**VocÃª pode avanÃ§ar, mas revise alguns pontos.**

âœ… Entende a maioria dos conceitos  
âš ï¸ Pode ter dÃºvidas em situaÃ§Ãµes especÃ­ficas  

**PrÃ³ximos passos:**
- Revise as questÃµes que errou (leia as explicaÃ§Ãµes)
- Pratique um pouco mais os conceitos fracos
- AvanÃ§ar para Semana 4, mas mantenha o material da S3 Ã  mÃ£o

---

#### ğŸ”„ 10-13 pontos: PARCIAL
**Recomendado revisar antes de avanÃ§ar.**

âš ï¸ Alguns conceitos nÃ£o estÃ£o claros  
âš ï¸ Pode ter dificuldade na Semana 4  

**PrÃ³ximos passos:**
- RefaÃ§a o notebook da Semana 3
- Foque nos conceitos que errou aqui
- Tente o teste novamente apÃ³s 2-3 dias
- **SÃ³ avance quando se sentir mais confiante**

---

#### ğŸ“š 0-9 pontos: REVISAR
**Ã‰ importante refazer o conteÃºdo da Semana 3.**

âŒ Conceitos fundamentais precisam de atenÃ§Ã£o  

**PrÃ³ximos passos:**
1. **NÃ£o se desanime!** Isso Ã© aprendizado, nÃ£o prova
2. Releia o material da Semana 3
3. Execute o notebook cÃ©lula por cÃ©lula, entendendo cada parte
4. FaÃ§a anotaÃ§Ãµes com suas palavras
5. Retome este teste em 1 semana

**Dica:** Foque em entender O PORQUÃŠ, nÃ£o em memorizar cÃ³digo.

---

## ğŸ§© Teste BÃ´nus: CenÃ¡rio Real

### ğŸ’¼ Desafio Profissional (avanÃ§ado)

VocÃª Ã© contratado para prever churn (cancelamento) de clientes de uma empresa de streaming. O time te passa:

- **Dataset:** 10.000 clientes, 25 features
- **Target:** `churn` (0 = fica, 1 = cancela)
- **Deadline:** 1 semana
- **Requisito:** MÃ­nimo 80% de recall (nÃ£o perder clientes que vÃ£o sair)

**Seu plano de aÃ§Ã£o:**

```markdown
## Dia 1-2: EDA
- [ ] ___________
- [ ] ___________

## Dia 3-4: Feature Engineering
- [ ] ___________
- [ ] ___________

## Dia 5: Modelagem
- [ ] Modelos a testar: __________, __________, __________
- [ ] Por que esses modelos? ___________

## Dia 6: OtimizaÃ§Ã£o
- [ ] ___________

## Dia 7: Deploy
- [ ] ___________
```

<details>
<summary>ğŸ’¡ Ver exemplo de plano profissional</summary>

## âœ… Exemplo de Plano:

### Dia 1-2: EDA
- [ ] Verificar missing values e outliers
- [ ] Analisar distribuiÃ§Ã£o do target (balanceamento)
- [ ] CorrelaÃ§Ã£o entre features e churn
- [ ] Identificar features mais importantes visualmente

### Dia 3-4: Feature Engineering
- [ ] Criar features de comportamento (mÃ©dia de uso, frequÃªncia)
- [ ] Encoding de variÃ¡veis categÃ³ricas
- [ ] Tratar valores faltantes
- [ ] Normalizar features numÃ©ricas (se necessÃ¡rio)

### Dia 5: Modelagem
- **Modelos:** Random Forest, XGBoost, Logistic Regression
- **Por quÃª:**
  - RF: Baseline sÃ³lido, lida com imbalance
  - XGBoost: Melhor performance geralmente
  - LogReg: RÃ¡pido, interpretÃ¡vel para stakeholders

### Dia 6: OtimizaÃ§Ã£o
- [ ] Hyperparameter tuning (GridSearch/RandomSearch)
- [ ] Ajustar threshold para priorizar recall (>80%)
- [ ] Validar com cross-validation
- [ ] Analisar feature importance para insights de negÃ³cio

### Dia 7: Deploy
- [ ] Salvar modelo final (pickle/joblib)
- [ ] Criar script de prediÃ§Ã£o
- [ ] Documentar pipeline
- [ ] Apresentar insights para o time

### ğŸ“Š MÃ©tricas a Focar:
- **Recall:** >80% (requisito principal)
- **Precision:** Quanto maior melhor (evitar falsos positivos)
- **F1-Score:** Balancear recall e precision
- **ROC-AUC:** Avaliar separaÃ§Ã£o geral

### ğŸ’¡ Dica Profissional:
Em problemas de churn, **recall Ã© mais importante** que accuracy!
Ã‰ melhor contatar 100 clientes (10 falsos alarmes) que perder 10 clientes reais.

</details>

---

## ğŸ¯ ReflexÃ£o Final

Responda honestamente (sÃ³ para vocÃª):

1. **Consegui entender o propÃ³sito de cada modelo?**  
   [ ] Sim, totalmente  
   [ ] Mais ou menos  
   [ ] Preciso revisar  

2. **Sei quando usar normalizaÃ§Ã£o?**  
   [ ] Sim, com confianÃ§a  
   [ ] Tenho dÃºvidas  
   [ ] NÃ£o entendi bem  

3. **Interpreto matriz de confusÃ£o e feature importance?**  
   [ ] Sim, tranquilamente  
   [ ] Com ajuda do material  
   [ ] Ainda confuso  

4. **Me sinto confortÃ¡vel para tentar um problema novo similar?**  
   [ ] Sim, vou tentar!  
   [ ] Com o material ao lado, sim  
   [ ] Acho que nÃ£o ainda  

---

## âœ… DecisÃ£o Final

Com base nas suas respostas:

### â¡ï¸ AVANCE se:
- Acertou 14+ pontos no teste
- Respondeu "Sim" ou "Mais ou menos" na maioria das reflexÃµes
- Sente que, **com consulta ao material**, consegue fazer

### ğŸ”„ REVISE se:
- Acertou <10 pontos
- Respondeu "Preciso revisar" ou "NÃ£o entendi" em 3+ reflexÃµes
- Sente que **nÃ£o conseguiria nem com material ao lado**

---

---

# ğŸ“… DIA 2: Hyperparameter Tuning e Cross-Validation

## ğŸ“‹ Parte 1: Conceitos (mÃºltipla escolha)

### QuestÃ£o 1: ParÃ¢metros vs HiperparÃ¢metros
Qual das seguintes afirmaÃ§Ãµes estÃ¡ CORRETA?

**A)** ParÃ¢metros sÃ£o definidos antes do treino, hiperparÃ¢metros sÃ£o aprendidos durante  
**B)** HiperparÃ¢metros sÃ£o definidos antes do treino, parÃ¢metros sÃ£o aprendidos durante  
**C)** Ambos sÃ£o aprendidos durante o treino  
**D)** Ambos sÃ£o definidos antes do treino  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) HiperparÃ¢metros sÃ£o definidos antes do treino, parÃ¢metros sÃ£o aprendidos durante**

**Exemplos:**

| Modelo | ParÃ¢metros (aprendidos) | HiperparÃ¢metros (definidos antes) |
|--------|------------------------|-----------------------------------|
| Linear Regression | Coeficientes (w, b) | Taxa de aprendizado |
| Random Forest | Splits das Ã¡rvores | `max_depth`, `n_estimators` |
| Neural Network | Pesos das conexÃµes | Arquitetura, learning rate |

**Analogia:** 
- **HiperparÃ¢metros:** ConfiguraÃ§Ãµes do treino (quantos dias treinar por semana)
- **ParÃ¢metros:** O que vocÃª aprende durante o treino (tÃ©cnica de chute)

**Conceito-chave:** HiperparÃ¢metros controlam COMO o modelo aprende, parÃ¢metros sÃ£o O QUE ele aprende.
</details>

---

### QuestÃ£o 2: Grid Search vs Random Search
VocÃª tem 5 hiperparÃ¢metros, cada um com 4 possÃ­veis valores, e tempo limitado (30 minutos). O que fazer?

**A)** Grid Search com todos os valores (4^5 = 1024 combinaÃ§Ãµes)  
**B)** Random Search com n_iter=50  
**C)** Testar manualmente 10 combinaÃ§Ãµes  
**D)** Usar valores padrÃ£o (nÃ£o fazer tuning)  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Random Search com n_iter=50**

**Por quÃª:**

**Grid Search:**
- 1024 combinaÃ§Ãµes Ã— CV=5 = **5120 treinos**
- Tempo: InviÃ¡vel em 30 minutos

**Random Search (n_iter=50):**
- 50 combinaÃ§Ãµes Ã— CV=5 = **250 treinos**
- Tempo: ViÃ¡vel em 30 minutos
- Explora melhor o espaÃ§o (pode encontrar combinaÃ§Ãµes nÃ£o Ã³bvias)

**Manual (10 combinaÃ§Ãµes):**
- Muito poucas para espaÃ§o grande
- Depende de intuiÃ§Ã£o (pode errar)

**Valores padrÃ£o:**
- RÃ¡pido mas pode ter performance ruim

**Regra prÃ¡tica:**
```
HiperparÃ¢metros â‰¤ 3 â†’ Grid Search
HiperparÃ¢metros > 3 â†’ Random Search
Tempo muito limitado â†’ Valores padrÃ£o + tuning simples
```

**Conceito-chave:** Random Search Ã© mais eficiente para espaÃ§os grandes de hiperparÃ¢metros.
</details>

---

### QuestÃ£o 3: Cross-Validation
VocÃª tem um dataset com 1000 amostras (700 classe 0, 300 classe 1). Qual estratÃ©gia de CV usar?

**A)** K-Fold com k=5 (sem stratify)  
**B)** Stratified K-Fold com k=5  
**C)** Leave-One-Out CV  
**D)** Holdout (train/test Ãºnico)  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Stratified K-Fold com k=5**

**Por quÃª:**

Dataset desbalanceado (70% vs 30%) precisa de **Stratified K-Fold**.

**ComparaÃ§Ã£o:**

| MÃ©todo | Classe 0 | Classe 1 | Problema |
|--------|----------|----------|----------|
| K-Fold normal | Varia por fold | Varia por fold | âŒ Alguns folds podem ter poucos exemplos da classe 1 |
| **Stratified K-Fold** | **~70% em cada fold** | **~30% em cada fold** | âœ… ProporÃ§Ã£o mantida |
| Leave-One-Out | - | - | âŒ Muito lento (1000 iteraÃ§Ãµes!) |
| Holdout | 70% | 30% | âŒ NÃ£o valida variÃ¢ncia |

**CÃ³digo correto:**
```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf)
```

**Conceito-chave:** Use SEMPRE Stratified K-Fold em classificaÃ§Ã£o, especialmente se desbalanceado.
</details>

---

### QuestÃ£o 4: Learning Curves
VocÃª treinou um modelo e viu estas curvas:

```
Train accuracy: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95%
Test accuracy:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’ 60%
```

O que fazer?

**A)** Aumentar complexidade (mais features, modelo maior)  
**B)** Reduzir complexidade (regularizaÃ§Ã£o, menos features)  
**C)** Coletar mais dados  
**D)** B ou C (ambas podem ajudar)  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: D) B ou C (ambas podem ajudar)**

**DiagnÃ³stico: OVERFITTING**
- Train >> Test (35% gap)
- Modelo decorou os dados de treino

**SoluÃ§Ãµes:**

**1. Reduzir complexidade (B):**
```python
# Random Forest
RandomForestClassifier(
    max_depth=5,           # â†“ profundidade
    min_samples_split=50,  # â†‘ amostras mÃ­nimas
    min_samples_leaf=20    # â†‘ amostras por folha
)

# XGBoost
XGBClassifier(
    max_depth=3,           # â†“ profundidade
    learning_rate=0.01,    # â†“ taxa aprendizado
    reg_alpha=1.0,         # â†‘ regularizaÃ§Ã£o L1
    reg_lambda=1.0         # â†‘ regularizaÃ§Ã£o L2
)
```

**2. Mais dados (C):**
- Mais exemplos â†’ modelo generaliza melhor
- Se possÃ­vel, sempre ajuda

**3. Feature Selection:**
```python
from sklearn.feature_selection import RFE
selector = RFE(model, n_features_to_select=10)
```

**CenÃ¡rios de Learning Curves:**

```
1. Overfitting:
   Train: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta)
   Test:  â”€â”€â”€â”€â–â–â–â–â–â–  (baixa)
   â†’ SoluÃ§Ã£o: Regularizar ou mais dados

2. Underfitting:
   Train: â”€â”€â”€â”€â”€â”€â”€â”€ (baixa)
   Test:  â”€â”€â”€â”€â”€â”€â”€â”€ (baixa)
   â†’ SoluÃ§Ã£o: Modelo mais complexo

3. Ideal:
   Train: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta)
   Test:  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (alta, prÃ³xima)
   â†’ SoluÃ§Ã£o: Nenhuma, estÃ¡ Ã³timo! âœ…
```

**Conceito-chave:** Gap grande entre train/test = Overfitting â†’ Regularizar ou mais dados.
</details>

---

## ğŸ–¥ï¸ Parte 2: PrÃ¡tica (cÃ³digo)

### Desafio: Pipeline com Grid Search

Complete o cÃ³digo abaixo corrigindo os **4 erros**:

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
import pandas as pd

# Carregar dados
df = pd.read_csv('titanic.csv')
X = df.drop('survived', axis=1)
y = df['survived']

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Pipeline para SVM
pipeline_svm = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', SVC())
])

# Grid Search para SVM
param_grid_svm = {
    'C': [0.1, 1, 10],              # âŒ ERRO 1: Falta prefixo
    'kernel': ['linear', 'rbf']
}

grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=5)
grid_svm.fit(X_train, y_train)

# Pipeline para Random Forest
pipeline_rf = Pipeline([
    ('scaler', StandardScaler()),   # âŒ ERRO 2: RF nÃ£o precisa
    ('classifier', RandomForestClassifier())
])

# Treinar Random Forest
pipeline_rf.fit(X_train, y_train)

# Avaliar SVM
y_pred_svm = grid_svm.predict(X_test)
print(f"SVM Accuracy: {grid_svm.score(X_test, y_test)}")

# Avaliar Random Forest
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test)  # âŒ ERRO 3: Data leakage
y_pred_rf = pipeline_rf.predict(X_test_scaled)  # âŒ ERRO 4: Dados errados

print(f"RF Accuracy: {pipeline_rf.score(X_test, y_test)}")
```

<details>
<summary>ğŸ’¡ Ver resposta completa</summary>

### âŒ Erros e CorreÃ§Ãµes:

#### ERRO 1: ParÃ¢metros do Grid sem prefixo do pipeline
```python
# ERRADO
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# CERTO
param_grid_svm = {
    'classifier__C': [0.1, 1, 10],           # Prefixo: nome_do_passo__
    'classifier__kernel': ['linear', 'rbf']
}
```
**Por quÃª:** No Pipeline, parÃ¢metros sÃ£o acessados com `nome_passo__parametro`.

---

#### ERRO 2: Random Forest nÃ£o precisa de normalizaÃ§Ã£o
```python
# ERRADO
pipeline_rf = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier())
])

# CERTO
pipeline_rf = Pipeline([
    ('classifier', RandomForestClassifier())  # Sem scaler
])

# OU SIMPLESMENTE
rf_model = RandomForestClassifier()
```
**Por quÃª:** RF Ã© baseado em Ã¡rvores (thresholds), nÃ£o Ã© afetado por escala.

---

#### ERRO 3: `fit_transform` no test set
```python
# ERRADO
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test)  # âŒ Data leakage!

# CERTO
# NÃ£o precisa escalar separadamente, o pipeline jÃ¡ faz isso!
# Mas se fosse necessÃ¡rio:
scaler = StandardScaler()
scaler.fit(X_train)  # Fit apenas no train
X_test_scaled = scaler.transform(X_test)  # Transform no test
```

---

#### ERRO 4: Passar dados errados para o pipeline
```python
# ERRADO
y_pred_rf = pipeline_rf.predict(X_test_scaled)

# CERTO
y_pred_rf = pipeline_rf.predict(X_test)  # Dados originais
```
**Por quÃª:** O pipeline jÃ¡ faz o preprocessamento internamente (se tivesse scaler).

---

### âœ… CÃ³digo Correto Completo:

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
import pandas as pd

# Carregar dados
df = pd.read_csv('titanic.csv')
X = df.drop('survived', axis=1)
y = df['survived']

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y  # âœ… Bonus: stratify
)

# ====== SVM (precisa de normalizaÃ§Ã£o) ======
pipeline_svm = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', SVC(random_state=42))
])

# Grid Search para SVM
param_grid_svm = {
    'classifier__C': [0.1, 1, 10],              # âœ… Com prefixo
    'classifier__kernel': ['linear', 'rbf'],
    'classifier__gamma': ['scale', 'auto']      # âœ… Bonus: testar gamma
}

grid_svm = GridSearchCV(
    pipeline_svm, 
    param_grid_svm, 
    cv=5,
    scoring='accuracy',
    n_jobs=-1  # âœ… Paralelizar
)
grid_svm.fit(X_train, y_train)

print(f"Melhores hiperparÃ¢metros SVM: {grid_svm.best_params_}")
print(f"SVM CV Score: {grid_svm.best_score_:.3f}")

# ====== Random Forest (NÃƒO precisa normalizaÃ§Ã£o) ======
pipeline_rf = Pipeline([
    ('classifier', RandomForestClassifier(random_state=42))  # âœ… Sem scaler
])

# Grid Search para Random Forest
param_grid_rf = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [5, 10, None],
    'classifier__min_samples_split': [2, 5, 10]
}

grid_rf = GridSearchCV(
    pipeline_rf,
    param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_rf.fit(X_train, y_train)

print(f"Melhores hiperparÃ¢metros RF: {grid_rf.best_params_}")
print(f"RF CV Score: {grid_rf.best_score_:.3f}")

# ====== AvaliaÃ§Ã£o Final ======
svm_test_score = grid_svm.score(X_test, y_test)  # âœ… Pipeline faz tudo
rf_test_score = grid_rf.score(X_test, y_test)    # âœ… Pipeline faz tudo

print(f"\n=== Resultados no Test Set ===")
print(f"SVM Test Accuracy: {svm_test_score:.3f}")
print(f"RF Test Accuracy: {rf_test_score:.3f}")

# ComparaÃ§Ã£o
if svm_test_score > rf_test_score:
    print("\nğŸ† Melhor modelo: SVM")
else:
    print("\nğŸ† Melhor modelo: Random Forest")
```

### ğŸ“Š Checklist de ValidaÃ§Ã£o:

| Modelo | Precisa NormalizaÃ§Ã£o? | Pipeline Correto |
|--------|-----------------------|------------------|
| Random Forest | âŒ NÃ£o | `Pipeline([('classifier', RF())])` |
| XGBoost | âŒ NÃ£o | `Pipeline([('classifier', XGB())])` |
| SVM | âœ… Sim | `Pipeline([('scaler', SS()), ('classifier', SVC())])` |
| MLP | âœ… Sim | `Pipeline([('scaler', SS()), ('classifier', MLP())])` |

### ğŸ¯ Conceitos Aplicados:

1. âœ… Pipeline previne data leakage
2. âœ… `classifier__param` para acessar hiperparÃ¢metros
3. âœ… Grid Search otimiza automaticamente
4. âœ… CV valida durante otimizaÃ§Ã£o
5. âœ… Test set apenas para avaliaÃ§Ã£o final

</details>

---

## ğŸ“Š Parte 3: DiagnÃ³stico de Learning Curves

Analise as 3 curvas abaixo e identifique o problema:

### Modelo A:
```
Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 99%
Test:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’ 65%
```

### Modelo B:
```
Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’ 68%
Test:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’ 65%
```

### Modelo C:
```
Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’ 87%
Test:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’ 84%
```

**Qual modelo escolher e por quÃª?**

<details>
<summary>ğŸ’¡ Ver anÃ¡lise completa</summary>

### ğŸ“Š DiagnÃ³stico:

#### Modelo A: OVERFITTING SEVERO âŒ
```
Train: 99% | Test: 65% | Gap: 34%
```

**Problema:** Modelo decorou os dados de treino  
**Sintomas:** 
- Train muito alta
- Test muito baixa
- Gap enorme (>30%)

**SoluÃ§Ãµes:**
```python
# 1. Reduzir complexidade
RandomForestClassifier(
    max_depth=5,           # Limitar profundidade
    min_samples_split=50,  # Mais restritivo
    n_estimators=50        # Menos Ã¡rvores
)

# 2. RegularizaÃ§Ã£o
XGBClassifier(
    learning_rate=0.01,    # Menor learning rate
    reg_alpha=1.0,         # L1 regularization
    reg_lambda=1.0         # L2 regularization
)

# 3. Feature Selection
from sklearn.feature_selection import RFE
selector = RFE(model, n_features_to_select=10)
```

---

#### Modelo B: UNDERFITTING âš ï¸
```
Train: 68% | Test: 65% | Gap: 3%
```

**Problema:** Modelo muito simples, nÃ£o aprende padrÃµes  
**Sintomas:**
- Train e Test ambas baixas
- Gap pequeno (bom sinal)
- Performance geral ruim (<70%)

**SoluÃ§Ãµes:**
```python
# 1. Aumentar complexidade
RandomForestClassifier(
    max_depth=None,        # Sem limite
    n_estimators=200,      # Mais Ã¡rvores
    min_samples_split=2    # Menos restritivo
)

# 2. Mais features
# - Feature engineering
# - Interaction terms
# - Polynomial features

# 3. Modelo mais poderoso
from xgboost import XGBClassifier
model = XGBClassifier(n_estimators=200, max_depth=6)
```

---

#### Modelo C: IDEAL âœ… ğŸ†
```
Train: 87% | Test: 84% | Gap: 3%
```

**AnÃ¡lise:**
- âœ… Test accuracy boa (>80%)
- âœ… Gap pequeno (<5%)
- âœ… Generaliza bem
- âœ… NÃ£o overfitting nem underfitting

**Por que escolher:**
1. **GeneralizaÃ§Ã£o:** Test prÃ³ximo de Train
2. **Performance:** 84% Ã© boa para maioria dos problemas
3. **Estabilidade:** Gap pequeno indica modelo robusto
4. **Confiabilidade:** FuncionarÃ¡ bem em produÃ§Ã£o

**Quando NÃƒO escolher:**
- Se o problema exige >90% accuracy
- Se hÃ¡ classes desbalanceadas (olhar outras mÃ©tricas)

---

### ğŸ¯ Regras de DecisÃ£o:

| Train | Test | Gap | DiagnÃ³stico | AÃ§Ã£o |
|-------|------|-----|-------------|------|
| >95% | <70% | >25% | **Overfitting severo** | âŒ Regularizar forte |
| >90% | <80% | >10% | **Overfitting** | âš ï¸ Regularizar |
| >85% | >80% | <5% | **Ideal** | âœ… Usar! |
| <75% | <75% | <5% | **Underfitting** | âš ï¸ Aumentar complexidade |

### ğŸ’¡ Dica Profissional:

```python
# Visualizar Learning Curves no cÃ³digo
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, 
    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring='accuracy'
)

plt.plot(train_sizes, train_scores.mean(axis=1), label='Train')
plt.plot(train_sizes, test_scores.mean(axis=1), label='Test')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Learning Curves')
plt.show()
```

**O que procurar:**
- Curvas convergindo â†’ Bom sinal âœ…
- Curvas divergindo â†’ Overfitting âŒ
- Ambas baixas â†’ Underfitting âš ï¸

</details>

---

## ğŸ“ Gabarito de Auto-AvaliaÃ§Ã£o - Dia 2

### PontuaÃ§Ã£o:

- **Parte 1 (Conceitos):** 4 questÃµes Ã— 2.5 pontos = **10 pontos**
- **Parte 2 (CÃ³digo):** 4 erros Ã— 2.5 pontos = **10 pontos**  
- **Parte 3 (Learning Curves):** 1 questÃ£o Ã— 10 pontos = **10 pontos**

**TOTAL:** 30 pontos

---

### ğŸ“Š InterpretaÃ§Ã£o da sua nota (Dia 2):

#### ğŸ† 27-30 pontos: EXCELENTE!
**VocÃª domina Hyperparameter Tuning!**

âœ… Entende Grid vs Random Search  
âœ… Aplica Cross-Validation corretamente  
âœ… Diagnostica overfitting/underfitting  
âœ… Usa Pipelines sem data leakage  

**PrÃ³ximos passos:**
- AvanÃ§ar para Dia 3 (Dashboard React)
- Aplicar tuning em projeto pessoal

---

#### ğŸ’ª 20-26 pontos: BOM!
**VocÃª entende os conceitos principais.**

âœ… Conceitos gerais claros  
âš ï¸ Pode ter dÃºvidas em implementaÃ§Ã£o  

**PrÃ³ximos passos:**
- Revisar Pipeline e prefixos (`classifier__param`)
- Praticar mais Grid/Random Search
- Pode avanÃ§ar, mas mantenha material Ã  mÃ£o

---

#### ğŸ”„ 15-19 pontos: PARCIAL
**Recomendado praticar mais antes de avanÃ§ar.**

âš ï¸ Alguns conceitos nÃ£o estÃ£o sÃ³lidos  
âš ï¸ Pode ter dificuldade no Dia 3  

**PrÃ³ximos passos:**
- Refazer notebook `02-hyperparameter-tuning.ipynb`
- Focar em Pipeline e Cross-Validation
- Ler material lÃºdico (`14-arvores-decisao-explicacao-ludica.md`)
- Repetir teste em 2-3 dias

---

#### ğŸ“š 0-14 pontos: REVISAR
**Ã‰ importante refazer o Dia 2.**

âŒ Conceitos fundamentais precisam de atenÃ§Ã£o  

**PrÃ³ximos passos:**
1. **NÃ£o desanime!** Hyperparameters sÃ£o complexos
2. Releia `docs/15-dia2-semana3-hyperparameter-tuning.md`
3. Execute notebook cÃ©lula por cÃ©lula
4. Foque em entender O PORQUÃŠ de cada decisÃ£o
5. Retome este teste em 1 semana

---

## ğŸ§© Teste BÃ´nus: CenÃ¡rio Real - OtimizaÃ§Ã£o Completa

### ğŸ’¼ Desafio Profissional

VocÃª estÃ¡ otimizando um modelo de detecÃ§Ã£o de fraude:

**Dataset:**
- 50.000 transaÃ§Ãµes
- 20 features numÃ©ricas
- Target: `is_fraud` (desbalanceado: 98% legÃ­timo, 2% fraude)

**Requisitos:**
- Recall mÃ­nimo: 85% (nÃ£o perder fraudes)
- Tempo de inferÃªncia: <100ms por prediÃ§Ã£o
- Explicabilidade: Importante (stakeholders nÃ£o-tÃ©cnicos)

**Seu plano:**

```markdown
## Escolha do Modelo
Modelo principal: __________
Por quÃª: __________

Modelo alternativo (backup): __________
Por quÃª: __________

## EstratÃ©gia de OtimizaÃ§Ã£o
[ ] Grid Search ou Random Search? __________
[ ] HiperparÃ¢metros prioritÃ¡rios: __________, __________, __________
[ ] Cross-Validation: K-Fold ou Stratified K-Fold? __________

## Preprocessing
[ ] NormalizaÃ§Ã£o necessÃ¡ria? __________
[ ] Feature Selection? __________
[ ] Balanceamento de classes? __________

## ValidaÃ§Ã£o
MÃ©trica principal: __________
Por quÃª: __________
```

<details>
<summary>ğŸ’¡ Ver exemplo de plano profissional</summary>

## âœ… Exemplo de Plano:

### Escolha do Modelo

**Modelo principal: Random Forest**

**Por quÃª:**
- âœ… Explicabilidade: Feature importance clara
- âœ… Performance: Boa em dados tabulares
- âœ… Velocidade: <100ms factÃ­vel
- âœ… Lida bem com desbalanceamento (ajustando `class_weight`)

**Modelo alternativo: XGBoost**

**Por quÃª:**
- âœ… Melhor performance (backup se RF nÃ£o atingir 85% recall)
- âœ… Feature importance disponÃ­vel
- âš ï¸ Menos interpretÃ¡vel que RF
- âš ï¸ Pode ser mais lento

---

### EstratÃ©gia de OtimizaÃ§Ã£o

**Random Search (n_iter=50)**

**Por quÃª:**
- 5+ hiperparÃ¢metros para otimizar
- Tempo limitado
- EspaÃ§o de busca grande

**HiperparÃ¢metros prioritÃ¡rios:**

```python
param_distributions = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [10, 20, 50],
    'min_samples_leaf': [5, 10, 20],
    'class_weight': ['balanced', 'balanced_subsample'],  # CRUCIAL para desbalanceamento
    'max_features': ['sqrt', 'log2', None]
}
```

**Cross-Validation: Stratified K-Fold (k=5)**

**Por quÃª:**
- Dataset desbalanceado (98/2)
- Garante 2% fraude em cada fold
- ValidaÃ§Ã£o mais confiÃ¡vel

---

### Preprocessing

**NormalizaÃ§Ã£o: NÃƒO**
- Random Forest nÃ£o precisa
- Se usar XGBoost: tambÃ©m nÃ£o precisa

**Feature Selection: SIM (se performance permitir)**
```python
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(f_classif, k=15)  # Reduzir de 20 para 15
```
**Por quÃª:**
- Melhora velocidade de inferÃªncia
- Reduz ruÃ­do
- Aumenta explicabilidade

**Balanceamento: SIM**
```python
RandomForestClassifier(
    class_weight='balanced',  # Penaliza erro em classe minoritÃ¡ria
    ...
)

# OU SMOTE (se necessÃ¡rio)
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

---

### ValidaÃ§Ã£o

**MÃ©trica principal: RECALL (classe fraude)**

**Por quÃª:**
- Requisito: 85% recall mÃ­nimo
- DetecÃ§Ã£o de fraude: **Falso Negativo Ã© MUITO pior** que Falso Positivo
- Melhor perder 10 transaÃ§Ãµes legÃ­timas (FP) que deixar passar 1 fraude (FN)

**MÃ©tricas secundÃ¡rias:**
- **Precision:** Evitar muitos falsos alarmes
- **F1-Score:** Balancear recall e precision
- **ROC-AUC:** SeparaÃ§Ã£o geral das classes

**Pipeline completo:**

```python
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold

# Pipeline
pipeline = Pipeline([
    ('selector', SelectKBest(f_classif, k=15)),
    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))
])

# ParÃ¢metros
param_distributions = {
    'selector__k': [10, 15, 20],
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [10, 20, None],
    'classifier__min_samples_split': [10, 20, 50],
    'classifier__class_weight': ['balanced', 'balanced_subsample']
}

# Stratified K-Fold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Random Search
random_search = RandomizedSearchCV(
    pipeline,
    param_distributions,
    n_iter=50,
    cv=cv,
    scoring='recall',  # MÃ‰TRICA PRINCIPAL
    n_jobs=-1,
    random_state=42,
    verbose=2
)

# Treinar
random_search.fit(X_train, y_train)

# Melhor modelo
best_model = random_search.best_estimator_

# Avaliar no test
from sklearn.metrics import classification_report, recall_score

y_pred = best_model.predict(X_test)
recall = recall_score(y_test, y_pred, pos_label=1)  # Classe fraude

print(f"Recall (fraude): {recall:.3f}")
print(classification_report(y_test, y_pred))

# Validar requisito
if recall >= 0.85:
    print("âœ… Requisito de recall atingido!")
else:
    print(f"âŒ Recall abaixo do mÃ­nimo. Falta: {0.85 - recall:.3f}")
```

---

### ğŸ¯ DecisÃ£o Final

**SE recall < 85%:**
1. Ajustar threshold de decisÃ£o
```python
# Reduzir threshold para aumentar recall
y_proba = best_model.predict_proba(X_test)[:, 1]
y_pred_adjusted = (y_proba > 0.3).astype(int)  # Em vez de 0.5
```

2. Tentar XGBoost (modelo alternativo)

3. Aplicar SMOTE (oversampling)

**SE recall >= 85%:**
âœ… Modelo pronto para produÃ§Ã£o!

---

### ğŸ“Š EntregÃ¡veis

1. Modelo salvo (`fraud_detector.pkl`)
2. Pipeline de preprocessamento
3. RelatÃ³rio de performance
4. Feature importance (top 10)
5. DocumentaÃ§Ã£o para stakeholders

</details>

---

## ğŸ’¬ Mensagem Final - Dia 2

> **"Eu entendi, mas nÃ£o faÃ§o sozinho sem consultar"**  
> **Isso Ã© NORMAL e ESPERADO!** ğŸ‘

Profissionais consultam:
- DocumentaÃ§Ã£o do Scikit-learn
- Stack Overflow
- Notebooks anteriores
- Papers acadÃªmicos

**A diferenÃ§a Ã©:** eles sabem **o que procurar** e **como aplicar**.

Se vocÃª chegou atÃ© aqui, executou tudo, e entende os conceitos quando lÃª as explicaÃ§Ãµes, **vocÃª estÃ¡ no caminho certo!** ğŸ¯

---

# ğŸ“… DIA 3: Dashboard React + API FastAPI

## â±ï¸ Tempo estimado: 30 minutos

---

## ğŸ“‹ Parte 1: Conceitos Fundamentais (mÃºltipla escolha)

### QuestÃ£o 1: REST API Basics
Qual mÃ©todo HTTP vocÃª usaria para fazer uma prediÃ§Ã£o enviando dados de um passageiro para a API?

**A)** GET  
**B)** POST  
**C)** PUT  
**D)** DELETE  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) POST**

**Por quÃª:**
- **GET:** Apenas buscar dados, sem enviar corpo (body)
- **POST:** Enviar dados no corpo da requisiÃ§Ã£o âœ…
- **PUT:** Atualizar recurso existente
- **DELETE:** Remover recurso

**Conceito-chave:** POST Ã© usado quando vocÃª envia dados para o servidor processar e retornar um resultado.

**Exemplo prÃ¡tico:**
```python
# âŒ ERRADO - GET nÃ£o tem body
response = requests.get('http://localhost:8000/predict', 
                        data={'age': 22})

# âœ… CERTO - POST envia dados no body
response = requests.post('http://localhost:8000/predict',
                         json={'pclass': 3, 'age': 22, ...})
```
</details>

---

### QuestÃ£o 2: CORS (Cross-Origin Resource Sharing)
Seu React estÃ¡ em `localhost:5173` e a API em `localhost:8000`. VocÃª recebe erro "CORS policy blocked". O que fazer?

**A)** Desabilitar CORS no navegador  
**B)** Configurar CORS na API FastAPI  
**C)** Mudar a porta do React  
**D)** Usar HTTPS  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Configurar CORS na API FastAPI**

**Por quÃª:**
- Frontend (5173) e Backend (8000) sÃ£o origens diferentes
- Navegador bloqueia por seguranÃ§a
- SoluÃ§Ã£o: API deve permitir explicitamente

**CÃ³digo correto (FastAPI):**
```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # Frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**Conceito-chave:** CORS Ã© configurado no SERVIDOR (backend), nÃ£o no cliente.
</details>

---

### QuestÃ£o 3: Pydantic Validation
Na API, vocÃª definiu:
```python
class PassengerInput(BaseModel):
    age: float = Field(..., ge=0, le=100)
```

O que acontece se enviar `age: -5`?

**A)** API retorna prediÃ§Ã£o com age=-5  
**B)** FastAPI retorna erro 422 (Validation Error)  
**C)** Python lanÃ§a ValueError  
**D)** Pydantic ajusta para 0  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) FastAPI retorna erro 422 (Validation Error)**

**Por quÃª:**
- `ge=0` significa "greater or equal" (â‰¥ 0)
- Pydantic valida ANTES de processar
- FastAPI retorna automaticamente status 422

**Resposta da API:**
```json
{
  "detail": [
    {
      "loc": ["body", "age"],
      "msg": "ensure this value is greater than or equal to 0",
      "type": "value_error.number.not_ge"
    }
  ]
}
```

**Conceito-chave:** Pydantic faz validaÃ§Ã£o automÃ¡tica, sem precisar de if/else manual.
</details>

---

### QuestÃ£o 4: React Hooks - useState
VocÃª tem:
```tsx
const [prediction, setPrediction] = useState<PredictionResponse | null>(null)
```

Quando vocÃª deve chamar `setPrediction()`?

**A)** Imediatamente apÃ³s o componente renderizar  
**B)** Depois de receber a resposta da API  
**C)** Antes de enviar a requisiÃ§Ã£o  
**D)** No evento onClick do botÃ£o  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Depois de receber a resposta da API**

**Por quÃª:**
- Estado sÃ³ muda quando temos dados novos
- Aguardamos resposta assÃ­ncrona
- SÃ³ entÃ£o atualizamos UI

**Fluxo correto:**
```tsx
const handleSubmit = async (e: FormEvent) => {
  e.preventDefault()
  setLoading(true)  // Antes: mostra loading
  
  try {
    const response = await axios.post('/predict', data)
    setPrediction(response.data)  // âœ… Depois: atualiza estado
  } catch (error) {
    // Tratar erro
  } finally {
    setLoading(false)  // Sempre: remove loading
  }
}
```

**Conceito-chave:** Estado muda DEPOIS de receber dados, nÃ£o antes ou durante.
</details>

---

### QuestÃ£o 5: useEffect Dependencies
VocÃª quer carregar informaÃ§Ãµes do modelo quando o componente montar. Qual Ã© o correto?

**A)** `useEffect(() => { fetchData() })`  
**B)** `useEffect(() => { fetchData() }, [])`  
**C)** `useEffect(() => { fetchData() }, [fetchData])`  
**D)** `useEffect(fetchData, [])`  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) `useEffect(() => { fetchData() }, [])`**

**Por quÃª:**
- `[]` vazio = executa APENAS na montagem
- Sem `[]` = executa a cada render (âŒ loop infinito!)
- `[fetchData]` = desnecessÃ¡rio se fetchData nÃ£o muda

**CÃ³digo completo:**
```tsx
useEffect(() => {
  const fetchModelInfo = async () => {
    const response = await axios.get('/model/info')
    setMetadata(response.data)
  }
  
  fetchModelInfo()
}, [])  // âœ… Array vazio = roda uma vez
```

**Conceito-chave:** Array de dependÃªncias controla QUANDO o effect executa.
</details>

---

### QuestÃ£o 6: Axios vs Fetch
Qual a principal vantagem do Axios sobre o Fetch nativo?

**A)** Axios Ã© mais rÃ¡pido  
**B)** Axios transforma JSON automaticamente  
**C)** Axios usa menos memÃ³ria  
**D)** Axios funciona apenas no React  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Axios transforma JSON automaticamente**

**ComparaÃ§Ã£o:**
```tsx
// âŒ FETCH - Precisa de .json()
const response = await fetch('/model/info')
const data = await response.json()  // Passo extra!

// âœ… AXIOS - JÃ¡ retorna objeto
const response = await axios.get('/model/info')
const data = response.data  // Direto!
```

**Outras vantagens do Axios:**
- Interceptors (modificar requisiÃ§Ãµes)
- Timeout automÃ¡tico
- Cancelamento de requisiÃ§Ãµes
- Melhor tratamento de erros

**Conceito-chave:** Axios simplifica trabalho com APIs REST.
</details>

---

### QuestÃ£o 7: TypeScript Interfaces
Por que definimos interfaces como `PredictionResponse`?

```tsx
interface PredictionResponse {
  survived: number
  probability: number
  message: string
}
```

**A)** Para fazer a API funcionar  
**B)** Para garantir type safety no cÃ³digo  
**C)** Para melhorar performance  
**D)** Porque React exige  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Para garantir type safety no cÃ³digo**

**Por quÃª:**
- TypeScript verifica tipos em tempo de desenvolvimento
- Previne erros de digitaÃ§Ã£o
- Autocomplete no VS Code

**Exemplo de erro prevenido:**
```tsx
// âŒ Sem interface - erro sÃ³ em runtime
<div>{prediction.probabilty}</div>  // Typo: probabilty

// âœ… Com interface - erro em desenvolvimento
<div>{prediction.probability}</div>  // TypeScript avisa do erro
```

**Conceito-chave:** Interfaces documentam e validam a estrutura de dados.
</details>

---

### QuestÃ£o 8: Async/Await
Qual o problema neste cÃ³digo?

```tsx
const handleSubmit = (e: FormEvent) => {
  e.preventDefault()
  const response = axios.post('/predict', data)
  setPrediction(response.data)  // âš ï¸
}
```

**A)** Falta try/catch  
**B)** Falta await (ou .then)  
**C)** Falta async no parÃ¢metro  
**D)** Falta setLoading  

<details>
<summary>ğŸ’¡ Ver resposta</summary>

**Resposta: B) Falta await (ou .then)**

**Por quÃª:**
- `axios.post()` retorna uma **Promise**
- Sem `await`, `response` Ã© a Promise, nÃ£o os dados
- `response.data` Ã© undefined!

**CÃ³digo correto:**
```tsx
// âœ… OpÃ§Ã£o 1: async/await
const handleSubmit = async (e: FormEvent) => {
  e.preventDefault()
  const response = await axios.post('/predict', data)
  setPrediction(response.data)
}

// âœ… OpÃ§Ã£o 2: .then()
const handleSubmit = (e: FormEvent) => {
  e.preventDefault()
  axios.post('/predict', data)
    .then(response => setPrediction(response.data))
}
```

**Conceito-chave:** RequisiÃ§Ãµes HTTP sÃ£o assÃ­ncronas - sempre use await ou .then.
</details>

---

## ğŸ“‹ Parte 2: DepuraÃ§Ã£o e Troubleshooting (cenÃ¡rios prÃ¡ticos)

### CenÃ¡rio 1: API nÃ£o responde
VocÃª iniciou o dashboard React, mas vÃª este erro no console:

```
Error: Network Error
    at createError (axios.js:123)
```

**O que vocÃª faz PRIMEIRO?**

**A)** Reinstala o Axios  
**B)** Verifica se a API estÃ¡ rodando em localhost:8000  
**C)** Muda a porta do React  
**D)** Limpa o cache do navegador  

<details>
<summary>ğŸ’¡ Ver resposta e diagnÃ³stico</summary>

**Resposta: B) Verifica se a API estÃ¡ rodando**

**Checklist de diagnÃ³stico:**

1. **Verificar se API estÃ¡ ativa:**
```bash
# Terminal 1 - API deve estar rodando
cd python-api
python app.py
```

2. **Testar API manualmente:**
```bash
curl http://localhost:8000/health
# Ou acessar no navegador
```

3. **Verificar porta correta:**
```tsx
// No cÃ³digo React
const API_URL = 'http://localhost:8000'  // âœ… Porta certa?
```

4. **Ver logs da API:**
```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

**Conceito-chave:** "Network Error" quase sempre significa backend offline.
</details>

---

### CenÃ¡rio 2: Modelo nÃ£o carregado
A API retorna:

```json
{
  "detail": "Modelo nÃ£o carregado. Execute train_model.py primeiro."
}
```

**Qual o problema e soluÃ§Ã£o?**

**A)** API estÃ¡ desatualizada â†’ reinstalar FastAPI  
**B)** Arquivo model.pkl nÃ£o existe â†’ treinar modelo  
**C)** Python estÃ¡ na versÃ£o errada â†’ atualizar Python  
**D)** CORS bloqueado â†’ configurar CORS  

<details>
<summary>ğŸ’¡ Ver resposta e soluÃ§Ã£o</summary>

**Resposta: B) Arquivo model.pkl nÃ£o existe â†’ treinar modelo**

**SoluÃ§Ã£o passo a passo:**

```bash
# 1. Ir para diretÃ³rio da API
cd python-api

# 2. Treinar o modelo
python train_model.py

# 3. Verificar se arquivos foram criados
ls -la
# Deve mostrar:
# model.pkl
# model_metadata.json

# 4. Reiniciar API
python app.py
```

**O que acontece no treino:**
```
â¬‡ï¸ Baixando dataset Titanic...
âœ… Dataset carregado: (891, 12)
ğŸ“Š Preparando dados...
ğŸ¤– Treinando modelo Random Forest...
âœ… Modelo treinado!
ğŸ“Š Avaliando modelo...
âœ… Accuracy: 0.8268 (82.68%)
ğŸ’¾ Modelo salvo: model.pkl
ğŸ’¾ Metadata salva: model_metadata.json
```

**Conceito-chave:** API precisa do modelo treinado para fazer prediÃ§Ãµes.
</details>

---

### CenÃ¡rio 3: Dashboard carrega, mas nÃ£o mostra dados do modelo
O componente `ModelInfo` mostra apenas o spinner infinito. O que investigar?

**A)** O CSS do spinner estÃ¡ quebrado  
**B)** O useEffect nÃ£o estÃ¡ sendo chamado  
**C)** A API retornou erro (verificar try/catch)  
**D)** O React estÃ¡ desatualizado  

<details>
<summary>ğŸ’¡ Ver resposta e debugging</summary>

**Resposta: C) A API retornou erro (verificar try/catch)**

**Debugging passo a passo:**

1. **Abrir DevTools (F12) â†’ Console:**
```
GET http://localhost:8000/model/info 503
```

2. **Verificar Network tab:**
```json
{
  "detail": "Metadata nÃ£o carregada"
}
```

3. **Analisar cÃ³digo React:**
```tsx
useEffect(() => {
  const fetchModelInfo = async () => {
    try {
      const response = await axios.get('/model/info')
      setMetadata(response.data)
    } catch (err) {
      setError(err.response?.data?.detail)  // âœ… Captura erro
    } finally {
      setLoading(false)  // âœ… Para spinner
    }
  }
  fetchModelInfo()
}, [])
```

**Problemas comuns:**
- âŒ Faltou `setLoading(false)` no catch â†’ spinner infinito
- âŒ Faltou `finally` â†’ spinner fica se der erro
- âŒ NÃ£o tratou erro â†’ usuÃ¡rio nÃ£o sabe o que aconteceu

**Conceito-chave:** Sempre trate erros e pare loading states.
</details>

---

### CenÃ¡rio 4: PrediÃ§Ã£o retorna resultado errado
VocÃª envia uma passageira de 1Âª classe (alta chance) mas recebe "NÃ£o sobreviveu". PossÃ­veis causas?

**A)** Modelo foi treinado com dados errados  
**B)** Encoding errado no frontend (sex: "female" â†’ 0 ou 1?)  
**C)** API usa modelo diferente do treinado  
**D)** Todas as anteriores  

<details>
<summary>ğŸ’¡ Ver resposta e validaÃ§Ã£o</summary>

**Resposta: D) Todas as anteriores (mas B Ã© mais comum)**

**Checklist de validaÃ§Ã£o:**

1. **Verificar encoding do frontend:**
```tsx
// âŒ ERRADO - String literal
const data = { sex: "female" }

// âœ… CERTO - Backend espera string mesmo
// (Pydantic faz validaÃ§Ã£o)
const data = { sex: "female" }  // API processa
```

2. **Verificar o que API recebe:**
```python
# Em app.py, adicionar log temporÃ¡rio
@app.post("/predict")
async def predict(passenger: PassengerInput):
    print(f"DEBUG: Recebido {passenger}")  # Ver no terminal
    ...
```

3. **Testar prediÃ§Ã£o manualmente:**
```python
# Em Python
import joblib
model = joblib.load('model.pkl')

# Dados de teste (1Âª classe, mulher)
import pandas as pd
test = pd.DataFrame([{
    'Pclass': 1, 'Sex': 0,  # female=0, male=1
    'Age': 38, 'SibSp': 1, 'Parch': 0,
    'Fare': 71, 'Embarked': 0  # C=0, Q=1, S=2
}])

pred = model.predict(test)
proba = model.predict_proba(test)
print(f"PrediÃ§Ã£o: {pred[0]}, Prob: {proba[0][1]:.2%}")
# Esperado: PrediÃ§Ã£o: 1, Prob: 85.30%
```

**Conceito-chave:** Sempre valide o pipeline completo (frontend â†’ API â†’ modelo).
</details>

---

## ğŸ“‹ Parte 3: ImplementaÃ§Ã£o PrÃ¡tica (anÃ¡lise de cÃ³digo)

### QuestÃ£o 9: Componente PredictionForm
Analise este cÃ³digo:

```tsx
const [formData, setFormData] = useState({
  pclass: 3,
  sex: 'male',
  age: 22
})

const handleChange = (e: React.ChangeEvent<HTMLInputElement>) => {
  const { name, value } = e.target
  setFormData(prev => ({
    ...prev,
    [name]: value  // âš ï¸ PossÃ­vel problema aqui
  }))
}
```

**Qual o problema potencial?**

**A)** NÃ£o tem problema  
**B)** `value` Ã© sempre string, mas `pclass` e `age` devem ser number  
**C)** Falta validaÃ§Ã£o de input  
**D)** `...prev` nÃ£o funciona com useState  

<details>
<summary>ğŸ’¡ Ver resposta e correÃ§Ã£o</summary>

**Resposta: B) `value` Ã© sempre string, mas precisamos de number**

**Problema:**
- HTML inputs sempre retornam **string**
- `pclass: "3"` em vez de `pclass: 3`
- API pode rejeitar ou comportar incorretamente

**CÃ³digo correto:**
```tsx
const handleChange = (e: React.ChangeEvent<HTMLInputElement | HTMLSelectElement>) => {
  const { name, value } = e.target
  setFormData(prev => ({
    ...prev,
    [name]: name === 'sex' || name === 'embarked' 
      ? value  // String
      : Number(value)  // âœ… Converter para number
  }))
}

// OU mais robusto:
const isNumericField = ['pclass', 'age', 'sibsp', 'parch', 'fare'].includes(name)
setFormData(prev => ({
  ...prev,
  [name]: isNumericField ? Number(value) : value
}))
```

**Conceito-chave:** HTML inputs retornam strings - sempre converta tipos quando necessÃ¡rio.
</details>

---

### QuestÃ£o 10: Loading State Pattern
Qual padrÃ£o Ã© MELHOR para exibir loading?

**A)**
```tsx
{loading && <div>Loading...</div>}
{!loading && <div>{prediction.message}</div>}
```

**B)**
```tsx
{loading ? <div>Loading...</div> : <div>{prediction?.message}</div>}
```

**C)**
```tsx
if (loading) return <div>Loading...</div>
return <div>{prediction.message}</div>
```

**D)** Todos sÃ£o equivalentes

<details>
<summary>ğŸ’¡ Ver resposta e boas prÃ¡ticas</summary>

**Resposta: B) TernÃ¡rio Ã© mais limpo para um componente**

**ComparaÃ§Ã£o:**

```tsx
// âœ… MELHOR - TernÃ¡rio (conciso e claro)
{loading ? (
  <div className="loading">
    <div className="spinner"></div>
    <p>Analisando dados...</p>
  </div>
) : (
  prediction && (
    <div className="result">
      {prediction.message}
    </div>
  )
)}

// âš ï¸ OK - Early return (para componentes inteiros)
function PredictionResult({ loading, prediction }) {
  if (loading) {
    return <div className="loading">Loading...</div>
  }
  
  if (!prediction) {
    return null
  }
  
  return <div>{prediction.message}</div>
}

// âŒ EVITAR - MÃºltiplos && (confuso)
{loading && <Loading />}
{!loading && prediction && <Result />}
{!loading && !prediction && <Empty />}
```

**Conceito-chave:** Use ternÃ¡rio para JSX inline, early return para lÃ³gica de componente.
</details>

---

## ğŸ† AvaliaÃ§Ã£o Final - Dia 3

### PontuaÃ§Ã£o:
- **Parte 1 (Conceitos):** 8 questÃµes Ã— 1 ponto = **8 pontos**
- **Parte 2 (Troubleshooting):** 4 cenÃ¡rios Ã— 2 pontos = **8 pontos**
- **Parte 3 (CÃ³digo):** 2 questÃµes Ã— 2 pontos = **4 pontos**

**Total:** 20 pontos

---

### ğŸ¯ InterpretaÃ§Ã£o da PontuaÃ§Ã£o:

#### ğŸŒŸ 18-20 pontos: EXCELENTE!
**VocÃª domina desenvolvimento full-stack ML!**

âœ… Entende REST APIs profundamente  
âœ… Domina React Hooks e TypeScript  
âœ… Sabe debugar problemas reais  
âœ… Pronto para projetos profissionais  

**PrÃ³ximos passos:**
- Deploy em produÃ§Ã£o (Vercel + Railway)
- Adicionar features avanÃ§adas (upload CSV, histÃ³rico)
- Iniciar portfÃ³lio GitHub
- Aplicar em projetos pessoais

---

#### ğŸ’ª 14-17 pontos: BOM!
**Conceitos sÃ³lidos, precisa de mais prÃ¡tica.**

âœ… Conceitos gerais compreendidos  
âš ï¸ Pode ter dificuldade em debugging avanÃ§ado  

**PrÃ³ximos passos:**
- Refazer dashboard do zero (sem copiar cÃ³digo)
- Praticar debugging com DevTools
- Estudar async/await e Promises
- Experimentar modificar o dashboard

---

#### ğŸ”„ 10-13 pontos: PARCIAL
**Entende teoria, mas implementaÃ§Ã£o precisa de atenÃ§Ã£o.**

âš ï¸ Conceitos de API/React nÃ£o estÃ£o sÃ³lidos  
âš ï¸ Pode ter dificuldade em projetos independentes  

**PrÃ³ximos passos:**
- Revisar documentaÃ§Ã£o FastAPI e React
- Executar dashboard cÃ©lula por cÃ©lula
- Praticar com exemplos menores
- Focar em um conceito por vez (primeiro API, depois React)
- Repetir teste em 1 semana

---

#### ğŸ“š 0-9 pontos: REVISAR
**Conceitos fundamentais precisam de reforÃ§o.**

âŒ Full-stack requer base sÃ³lida  

**PrÃ³ximos passos:**
1. **NÃ£o desanime!** Full-stack Ã© complexo
2. Dividir em etapas menores:
   - Primeiro: Dominar apenas o backend (API)
   - Depois: Dominar apenas frontend (React estÃ¡tico)
   - Por Ãºltimo: IntegraÃ§Ã£o
3. Seguir tutoriais passo a passo
4. Copiar cÃ³digo, depois modificar aos poucos
5. Retomar teste em 2 semanas

---

## ğŸ§ª Teste PrÃ¡tico BÃ´nus: Crie Seu PrÃ³prio Endpoint!

### ğŸ’¼ Desafio Final (30 minutos)

**Tarefa:** Adicione um novo endpoint `/predict/explain` que retorna:
- PrediÃ§Ã£o (survived)
- Probabilidade
- **3 fatores principais** que influenciaram a decisÃ£o

**Requisitos:**
1. Backend (FastAPI): Criar endpoint novo
2. Backend: Usar `model.feature_importances_` ou SHAP
3. Frontend: Novo botÃ£o "Explicar PrediÃ§Ã£o"
4. Frontend: Mostrar os 3 fatores

**Dicas:**
```python
# Backend - app.py
@app.post("/predict/explain")
async def predict_explain(passenger: PassengerInput):
    prediction = model.predict(input_data)[0]
    probability = model.predict_proba(input_data)[0][1]
    
    # Feature importance
    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
    importances = model.feature_importances_
    
    # Top 3
    top_indices = importances.argsort()[-3:][::-1]
    top_features = [
        {"feature": features[i], "importance": float(importances[i])}
        for i in top_indices
    ]
    
    return {
        "survived": int(prediction),
        "probability": float(probability),
        "explanation": top_features
    }
```

**AvaliaÃ§Ã£o:**
- âœ… Implementou backend: +5 pontos
- âœ… Integrou frontend: +5 pontos
- âœ… Funcionou sem erros: +5 pontos
- âœ… Interface bonita: +5 pontos

**BONUS TOTAL:** 20 pontos extras!

---

## ğŸ’¬ Mensagem Final - Semana 3 Completa

> **ParabÃ©ns por chegar atÃ© aqui!** ğŸ‰

VocÃª percorreu:
- **Dia 1:** 5 modelos ML diferentes (RF, SVM, XGBoost, MLP, DT)
- **Dia 2:** OtimizaÃ§Ã£o completa (Grid/Random Search, CV, Pipeline)
- **Dia 3:** Dashboard full-stack (React + FastAPI + ML)

**Isso Ã© MUITO conteÃºdo!** A maioria das pessoas leva MESES para absorver isso.

### ğŸ“ˆ Seu Progresso Real:

```
Semana 1: "O que Ã© Machine Learning?" ğŸ¤”
           â†“
Semana 2: "Consigo treinar um modelo!" ğŸ’ª
           â†“
Semana 3: "Tenho um sistema ML funcionando!" ğŸš€
```

### ğŸ¯ PrÃ³ximos Passos Profissionais:

**NÃ­vel JÃºnior (vocÃª estÃ¡ aqui!):**
- âœ… Consegue treinar e comparar modelos
- âœ… Entende mÃ©tricas e otimizaÃ§Ã£o
- âœ… Cria APIs para servir modelos
- â­ï¸ PrÃ³ximo: Deploy e monitoramento

**NÃ­vel Pleno (prÃ³xima meta):**
- ğŸ“¦ Deploy em produÃ§Ã£o (Docker, Cloud)
- ğŸ“Š Monitoramento (Prometheus, Grafana)
- ğŸ”„ CI/CD (GitHub Actions)
- ğŸ“ˆ A/B testing e model retraining

**NÃ­vel SÃªnior (futuro):**
- ğŸ—ï¸ Arquitetura de sistemas ML
- ğŸ¯ MLOps completo
- ğŸ“ Model governance e compliance
- ğŸ‘¥ Liderar equipes tÃ©cnicas

---

**Continue praticando e construindo! ğŸš€**

_Lembre-se: Consultar documentaÃ§Ã£o Ã© sinal de profissionalismo, nÃ£o de fraqueza._

**Sucesso na sua jornada de Machine Learning! ğŸ“**


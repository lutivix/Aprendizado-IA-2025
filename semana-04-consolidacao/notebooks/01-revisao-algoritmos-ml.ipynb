{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bc2889",
   "metadata": {},
   "source": [
    "# ğŸ“š Semana 4 - Dia 1: RevisÃ£o Comparativa de Algoritmos ML\n",
    "\n",
    "**Data:** 3 Dezembro 2025  \n",
    "**Objetivo:** ğŸ§˜â€â™€ï¸ Consolidar conhecimento sobre QUANDO usar cada algoritmo ML\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ O que vocÃª vai praticar hoje:\n",
    "\n",
    "1. âœ… Comparar 6 algoritmos lado-a-lado no mesmo dataset\n",
    "2. âœ… Visualizar diferenÃ§as de performance\n",
    "3. âœ… Entender quando cada um funciona melhor\n",
    "4. âœ… Criar sua prÃ³pria \"Ã¡rvore de decisÃ£o\" mental para escolher modelos\n",
    "5. âœ… Praticar cenÃ¡rios reais de decisÃ£o\n",
    "\n",
    "---\n",
    "\n",
    "**Datasets:** Titanic (classificaÃ§Ã£o) + Dados sintÃ©ticos (regressÃ£o)  \n",
    "**Algoritmos:** Linear/Logistic Regression, Random Forest, XGBoost, SVM, Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b7ed7",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. ConfiguraÃ§Ã£o Inicial - Importar Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc4fa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bibliotecas importadas com sucesso!\n",
      "ğŸ“Š Pandas: 2.2.3\n",
      "ğŸ”¢ NumPy: 2.1.3\n",
      "ğŸ¤– Scikit-learn importado\n"
     ]
    }
   ],
   "source": [
    "# ManipulaÃ§Ã£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# VisualizaÃ§Ã£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Modelos\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Machine Learning - PrÃ©-processamento e AvaliaÃ§Ã£o\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, mean_squared_error, r2_score)\n",
    "\n",
    "# ConfiguraÃ§Ãµes de visualizaÃ§Ã£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Ignorar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Bibliotecas importadas com sucesso!\")\n",
    "print(f\"ğŸ“Š Pandas: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ NumPy: {np.__version__}\")\n",
    "print(f\"ğŸ¤– Scikit-learn importado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab6560",
   "metadata": {},
   "source": [
    "## ğŸ“Š 2. Carregar Dataset Titanic (ClassificaÃ§Ã£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c7f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando dataset local: ../../semana-02-data-science/titanic.csv\n",
      "âœ… Dataset carregado do arquivo local!\n",
      "\n",
      "ğŸ” DEBUG - Colunas originais: ['429: Too Many Requests']\n",
      "ğŸ” DEBUG - Colunas apÃ³s capitalize: ['429: too many requests']\n",
      "\n",
      "ğŸ“‹ Dataset Titanic carregado!\n",
      "   Formato: (1, 1)\n",
      "   Colunas finais: ['429: too many requests']\n",
      "\n",
      "âš ï¸ AVISO: Coluna 'Survived' nÃ£o encontrada!\n",
      "   Colunas disponÃ­veis: ['429: too many requests']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>429: too many requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>For more on scraping GitHub and how it may affect your rights</th>\n",
       "      <td>please review our Terms of Service (https://d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               429: too many requests\n",
       "For more on scraping GitHub and how it may affe...   please review our Terms of Service (https://d..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar dataset Titanic - FORÃ‡AR DOWNLOAD ONLINE\n",
    "import ssl\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Caminho para salvar CSV\n",
    "csv_path = '../../semana-02-data-science/titanic.csv'\n",
    "\n",
    "# APAGAR CSV CORROMPIDO se existir\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"ğŸ—‘ï¸ Apagando CSV corrompido: {csv_path}\")\n",
    "    os.remove(csv_path)\n",
    "\n",
    "# FORÃ‡AR download online\n",
    "print(\"ğŸŒ Baixando dataset online (forÃ§ado)...\")\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv',\n",
    "    'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv',\n",
    "    'https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv',\n",
    "]\n",
    "\n",
    "df = None\n",
    "for url in urls:\n",
    "    try:\n",
    "        print(f\"ğŸ”„ Tentando: {url[:60]}...\")\n",
    "        \n",
    "        # Baixar e carregar\n",
    "        df = pd.read_csv(url)\n",
    "        \n",
    "        print(f\"âœ… Download bem-sucedido!\")\n",
    "        print(f\"   Colunas baixadas: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Salvar localmente para prÃ³ximas execuÃ§Ãµes\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"ğŸ’¾ CSV salvo em: {csv_path}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Falhou: {str(e)[:80]}\")\n",
    "        continue\n",
    "\n",
    "# Se todas as URLs falharem, criar dataset COMPLETO de exemplo\n",
    "if df is None:\n",
    "    print(\"\\nâš ï¸ Todas URLs falharam. Criando dataset de exemplo...\")\n",
    "    csv_data = \"\"\"PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
    "1,0,3,Braund Mr. Owen Harris,male,22.0,1,0,A/5 21171,7.25,,S\n",
    "2,1,1,Cumings Mrs. John Bradley,female,38.0,1,0,PC 17599,71.2833,C85,C\n",
    "3,1,3,Heikkinen Miss. Laina,female,26.0,0,0,STON/O2. 3101282,7.925,,S\n",
    "4,1,1,Futrelle Mrs. Jacques Heath,female,35.0,1,0,113803,53.1,C123,S\n",
    "5,0,3,Allen Mr. William Henry,male,35.0,0,0,373450,8.05,,S\n",
    "6,0,3,Moran Mr. James,male,27.0,0,0,330877,8.4583,,Q\n",
    "7,0,1,McCarthy Mr. Timothy J,male,54.0,0,0,17463,51.8625,E46,S\n",
    "8,0,3,Palsson Master. Gosta Leonard,male,2.0,3,1,349909,21.075,,S\n",
    "9,1,3,Johnson Mrs. Oscar W,female,27.0,0,2,347742,11.1333,,S\n",
    "10,1,2,Nasser Mrs. Nicholas,female,14.0,1,0,237736,30.0708,,C\"\"\"\n",
    "    df = pd.read_csv(io.StringIO(csv_data))\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"ğŸ’¾ Dataset de exemplo salvo em: {csv_path}\")\n",
    "\n",
    "# Padronizar nomes das colunas (primeira letra maiÃºscula)\n",
    "df.columns = [col.capitalize() for col in df.columns]\n",
    "\n",
    "print(\"\\nğŸ“‹ Dataset Titanic carregado!\")\n",
    "print(f\"   Formato: {df.shape}\")\n",
    "print(f\"   Colunas: {df.columns.tolist()}\")\n",
    "print(f\"\\nğŸ¯ Target: Survived (0 = NÃ£o sobreviveu, 1 = Sobreviveu)\")\n",
    "print(f\"   DistribuiÃ§Ã£o: {df['Survived'].value_counts().to_dict()}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7ed767",
   "metadata": {},
   "source": [
    "## ğŸ§¹ 3. Preparar Dados (Feature Engineering)\n",
    "\n",
    "Vamos usar as mesmas transformaÃ§Ãµes que vocÃª jÃ¡ domina da Semana 2 e 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar cÃ³pia para manipulaÃ§Ã£o\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1. Preencher valores faltantes\n",
    "df_clean['Age'].fillna(df_clean['Age'].median(), inplace=True)\n",
    "if 'Embarked' in df_clean.columns:\n",
    "    df_clean['Embarked'].fillna(df_clean['Embarked'].mode()[0], inplace=True)\n",
    "df_clean['Fare'].fillna(df_clean['Fare'].median(), inplace=True)\n",
    "\n",
    "# 2. Feature Engineering\n",
    "df_clean['Familysize'] = df_clean['Sibsp'] + df_clean['Parch'] + 1\n",
    "df_clean['Isalone'] = (df_clean['Familysize'] == 1).astype(int)\n",
    "df_clean['Title'] = df_clean['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Simplificar tÃ­tulos\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare', \n",
    "    'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',\n",
    "    'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',\n",
    "    'Capt': 'Rare', 'Sir': 'Rare'\n",
    "}\n",
    "df_clean['Title'] = df_clean['Title'].map(title_mapping)\n",
    "# Preencher tÃ­tulos NaN com 'Mr' (mais comum)\n",
    "df_clean['Title'].fillna('Mr', inplace=True)\n",
    "\n",
    "# 3. Converter categÃ³ricas em numÃ©ricas\n",
    "df_clean['Sex'] = df_clean['Sex'].map({'male': 0, 'female': 1})\n",
    "if 'Embarked' in df_clean.columns:\n",
    "    df_clean['Embarked'] = df_clean['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "    df_clean['Embarked'].fillna(0, inplace=True)  # Preencher NaN com S (mais comum)\n",
    "\n",
    "# Label encoding para Title\n",
    "le_title = LabelEncoder()\n",
    "df_clean['Title'] = le_title.fit_transform(df_clean['Title'])\n",
    "\n",
    "# 4. Selecionar features para o modelo\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Familysize', 'Isalone', 'Title']\n",
    "if 'Embarked' in df_clean.columns:\n",
    "    features.insert(4, 'Embarked')\n",
    "\n",
    "X = df_clean[features]\n",
    "y = df_clean['Survived']\n",
    "\n",
    "print(\"âœ… Dados preparados!\")\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "print(f\"   Features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101b544",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ 4. Split Train/Test e Escalamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Escalar dados (importante para SVM e Neural Networks!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Dados divididos e escalados!\")\n",
    "print(f\"   Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"   Train scaled: {X_train_scaled.shape}\")\n",
    "print(f\"\\nğŸ“Š DistribuiÃ§Ã£o do target no treino:\")\n",
    "print(f\"   {pd.Series(y_train).value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d5500",
   "metadata": {},
   "source": [
    "## ğŸ¤– 5. ComparaÃ§Ã£o de Algoritmos ML - ClassificaÃ§Ã£o\n",
    "\n",
    "Vamos treinar 6 algoritmos diferentes e comparar seus resultados!\n",
    "\n",
    "### ğŸ¯ Por que comparar lado-a-lado?\n",
    "- Ver qual funciona melhor para ESTE problema\n",
    "- Entender diferenÃ§as de performance\n",
    "- Identificar overfitting/underfitting\n",
    "- Aprender quando cada um Ã© mais apropriado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DicionÃ¡rio para armazenar resultados\n",
    "results = {}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1ï¸âƒ£ LOGISTIC REGRESSION - Baseline Linear\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"ğŸ”¹ 1. Logistic Regression (Baseline)\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "results['Logistic Regression'] = lr_acc\n",
    "print(f\"   âœ… Accuracy: {lr_acc:.4f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2ï¸âƒ£ RANDOM FOREST - Ensemble Poderoso\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸŒ² 2. Random Forest\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "results['Random Forest'] = rf_acc\n",
    "print(f\"   âœ… Accuracy: {rf_acc:.4f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3ï¸âƒ£ XGBOOST - Gradient Boosting Champion\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸš€ 3. XGBoost\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "    random_state=42, eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "results['XGBoost'] = xgb_acc\n",
    "print(f\"   âœ… Accuracy: {xgb_acc:.4f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4ï¸âƒ£ SVM - Support Vector Machine (com dados escalados!)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nâš¡ 4. SVM (RBF Kernel)\")\n",
    "svm = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)  # IMPORTANTE: dados escalados!\n",
    "svm_pred = svm.predict(X_test_scaled)\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "results['SVM'] = svm_acc\n",
    "print(f\"   âœ… Accuracy: {svm_acc:.4f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5ï¸âƒ£ NEURAL NETWORK (MLP) - Rede Neural (com dados escalados!)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ§  5. Neural Network (MLP)\")\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "mlp.fit(X_train_scaled, y_train)  # IMPORTANTE: dados escalados!\n",
    "mlp_pred = mlp.predict(X_test_scaled)\n",
    "mlp_acc = accuracy_score(y_test, mlp_pred)\n",
    "results['Neural Network'] = mlp_acc\n",
    "print(f\"   âœ… Accuracy: {mlp_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“Š RESUMO GERAL:\")\n",
    "for model, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   {model:20s}: {acc:.4f} ({acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d12c3e",
   "metadata": {},
   "source": [
    "## ğŸ“Š 6. VisualizaÃ§Ã£o Comparativa de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar figura com 2 subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Barplot de Accuracy\n",
    "models = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "\n",
    "axes[0].barh(models, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'])\n",
    "axes[0].set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('ComparaÃ§Ã£o de Accuracy dos Modelos', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim(0.7, 0.9)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(v + 0.005, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# Subplot 2: Accuracy com Cross-Validation (mÃ©dia de 5 folds)\n",
    "cv_results = {}\n",
    "\n",
    "print(\"ğŸ”„ Calculando Cross-Validation (5-fold) para cada modelo...\")\n",
    "\n",
    "models_to_test = [\n",
    "    ('Logistic Regression', lr),\n",
    "    ('Random Forest', rf),\n",
    "    ('XGBoost', xgb_model),\n",
    "]\n",
    "\n",
    "for name, model in models_to_test:\n",
    "    if 'SVM' in name or 'Neural' in name:\n",
    "        # Para SVM e MLP, usar dados escalados\n",
    "        scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    print(f\"   {name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "\n",
    "# Plotar boxplot do CV\n",
    "cv_data = [cv_results[model] for model in cv_results.keys()]\n",
    "axes[1].boxplot(cv_data, labels=list(cv_results.keys()), vert=True)\n",
    "axes[1].set_ylabel('Accuracy (5-Fold CV)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('DistribuiÃ§Ã£o de Accuracy com Cross-Validation', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… GrÃ¡ficos gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9890079b",
   "metadata": {},
   "source": [
    "## ğŸ¯ 7. Feature Importance (Modelos Tree-based)\n",
    "\n",
    "Uma das grandes vantagens de Random Forest e XGBoost Ã© que eles nos dizem **quais features sÃ£o mais importantes** para a prediÃ§Ã£o!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance para Random Forest e XGBoost\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "axes[0].barh(rf_importance['feature'], rf_importance['importance'], color='#4ECDC4')\n",
    "axes[0].set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Random Forest - Feature Importance', fontsize=14, fontweight='bold')\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "axes[1].barh(xgb_importance['feature'], xgb_importance['importance'], color='#45B7D1')\n",
    "axes[1].set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('XGBoost - Feature Importance', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Top 3 Features mais importantes:\")\n",
    "print(\"\\nğŸŒ² Random Forest:\")\n",
    "for i, row in rf_importance.tail(3).iterrows():\n",
    "    print(f\"   {row['feature']:15s}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nğŸš€ XGBoost:\")\n",
    "for i, row in xgb_importance.tail(3).iterrows():\n",
    "    print(f\"   {row['feature']:15s}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357a33a",
   "metadata": {},
   "source": [
    "## ğŸ§  8. Guia de DecisÃ£o: Quando Usar Cada Algoritmo?\n",
    "\n",
    "### ğŸ“‹ Resumo PrÃ¡tico baseado nos Resultados\n",
    "\n",
    "Com base nos resultados acima e suas 3 semanas de experiÃªncia, aqui estÃ¡ seu guia de decisÃ£o:\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ **Logistic Regression** - Baseline RÃ¡pido\n",
    "âœ… **Use quando:**\n",
    "- Precisa de baseline rÃ¡pido (< 1 minuto treino)\n",
    "- Quer interpretar coeficientes das features\n",
    "- Dados sÃ£o aproximadamente lineares\n",
    "- Tem poucas features (< 20)\n",
    "\n",
    "âŒ **NÃƒO use quando:**\n",
    "- RelaÃ§Ãµes nÃ£o-lineares complexas\n",
    "- Precisa da melhor accuracy possÃ­vel\n",
    "\n",
    "**Resultado Titanic:** ~78-80% accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Random Forest** ğŸŒ² - Primeira Escolha\n",
    "âœ… **Use quando:**\n",
    "- EstÃ¡ comeÃ§ando um projeto novo\n",
    "- Quer bom resultado sem muito tuning\n",
    "- Precisa de feature importance\n",
    "- Dados tÃªm outliers\n",
    "- NÃ£o sabe se Ã© linear ou nÃ£o-linear\n",
    "\n",
    "âŒ **NÃƒO use quando:**\n",
    "- Precisa de modelo muito leve para produÃ§Ã£o\n",
    "- Tem MUITAS features (> 100)\n",
    "\n",
    "**Resultado Titanic:** ~82-84% accuracy â­\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **XGBoost** ğŸš€ - MÃ¡xima Performance\n",
    "âœ… **Use quando:**\n",
    "- Quer a melhor accuracy possÃ­vel\n",
    "- EstÃ¡ em competiÃ§Ã£o (Kaggle)\n",
    "- Tem tempo para hyperparameter tuning\n",
    "- Dados tabulares estruturados\n",
    "\n",
    "âŒ **NÃƒO use quando:**\n",
    "- NÃ£o tem tempo para tuning\n",
    "- Baseline rÃ¡pido Ã© suficiente\n",
    "- Interpretabilidade Ã© prioridade\n",
    "\n",
    "**Resultado Titanic:** ~83-85% accuracy ğŸ†\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **SVM** âš¡ - Dados Pequenos e Dimensionais\n",
    "âœ… **Use quando:**\n",
    "- Dataset pequeno (< 5k amostras)\n",
    "- Alta dimensionalidade\n",
    "- ClassificaÃ§Ã£o binÃ¡ria clara\n",
    "- **IMPORTANTE:** Dados escalados!\n",
    "\n",
    "âŒ **NÃƒO use quando:**\n",
    "- Dataset muito grande (> 10k)\n",
    "- NÃ£o quer escalar dados\n",
    "- Precisa de probabilidades\n",
    "\n",
    "**Resultado Titanic:** ~80-82% accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### 5ï¸âƒ£ **Neural Network (MLP)** ğŸ§  - RelaÃ§Ãµes Complexas\n",
    "âœ… **Use quando:**\n",
    "- RelaÃ§Ãµes MUITO complexas\n",
    "- Muitos dados (> 10k amostras)\n",
    "- **IMPORTANTE:** Dados escalados!\n",
    "- Tem GPU disponÃ­vel\n",
    "\n",
    "âŒ **NÃƒO use quando:**\n",
    "- Poucos dados (< 1k)\n",
    "- Precisa de interpretabilidade\n",
    "- Baseline ou prototipagem\n",
    "\n",
    "**Resultado Titanic:** ~79-82% accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Seu Fluxo de Trabalho Recomendado:\n",
    "\n",
    "```python\n",
    "# ETAPA 1: Baseline (5 minutos)\n",
    "baseline = LogisticRegression()\n",
    "baseline.fit(X_train, y_train)\n",
    "# â†’ Accuracy: ~78%\n",
    "\n",
    "# ETAPA 2: Random Forest (10 minutos)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "# â†’ Accuracy: ~82% âœ… JÃ MUITO BOM!\n",
    "\n",
    "# ETAPA 3: XGBoost (se precisa melhorar)\n",
    "xgb = XGBClassifier(...)\n",
    "xgb.fit(X_train, y_train)\n",
    "# â†’ Accuracy: ~84% ğŸ†\n",
    "\n",
    "# ETAPA 4: Hyperparameter Tuning (se ainda precisa melhorar)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# ... tuning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Insights das suas 3 Semanas:\n",
    "\n",
    "1. **Semana 1:** LinearRegression foi Ã“TIMO para aprender (96.5% RÂ²)\n",
    "2. **Semana 2:** LogisticRegression deu 79% no Titanic (bom baseline!)\n",
    "3. **Semana 3:** XGBoost chegou a 85.1% (melhor resultado com tuning!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b33dad",
   "metadata": {},
   "source": [
    "## ğŸ§ª 9. ExercÃ­cios PrÃ¡ticos - Teste seu Conhecimento!\n",
    "\n",
    "### ExercÃ­cio 1: Escolha o Algoritmo Correto\n",
    "\n",
    "Para cada cenÃ¡rio abaixo, escolha o melhor algoritmo INICIAL (antes de tuning):\n",
    "\n",
    "**CenÃ¡rio A:**  \n",
    "Dataset: 500 amostras, 8 features numÃ©ricas, target binÃ¡rio  \n",
    "Objetivo: Modelo interpretÃ¡vel para apresentaÃ§Ã£o ao cliente  \n",
    "Tempo: 30 minutos disponÃ­veis  \n",
    "**Resposta:** `_________________`\n",
    "\n",
    "**CenÃ¡rio B:**  \n",
    "Dataset: 50.000 amostras, 25 features (mix categÃ³rico/numÃ©rico)  \n",
    "Objetivo: MÃ¡xima accuracy para competiÃ§Ã£o Kaggle  \n",
    "Tempo: 2 dias disponÃ­veis para tuning  \n",
    "**Resposta:** `_________________`\n",
    "\n",
    "**CenÃ¡rio C:**  \n",
    "Dataset: 2.000 amostras, 100 features (dados de sensores)  \n",
    "Objetivo: ClassificaÃ§Ã£o binÃ¡ria com margem clara  \n",
    "Tempo: 1 dia, dados jÃ¡ escalados  \n",
    "**Resposta:** `_________________`\n",
    "\n",
    "**CenÃ¡rio D:**  \n",
    "Dataset: 15.000 amostras, 15 features bem balanceadas  \n",
    "Objetivo: Prototipagem rÃ¡pida, bom resultado \"out of the box\"  \n",
    "Tempo: 2 horas  \n",
    "**Resposta:** `_________________`\n",
    "\n",
    "---\n",
    "\n",
    "### ExercÃ­cio 2: Debug de Modelo\n",
    "\n",
    "VocÃª treinou um modelo e obteve estes resultados:\n",
    "```\n",
    "Train Accuracy: 99.2%\n",
    "Test Accuracy: 68.5%\n",
    "```\n",
    "\n",
    "**Pergunta 1:** O que estÃ¡ acontecendo?  \n",
    "**Resposta:** `_________________`\n",
    "\n",
    "**Pergunta 2:** Qual das seguintes soluÃ§Ãµes NÃƒO ajudaria?\n",
    "- A) Usar Cross-Validation\n",
    "- B) Aumentar complexidade do modelo\n",
    "- C) Reduzir nÃºmero de features\n",
    "- D) Coletar mais dados\n",
    "\n",
    "**Resposta:** `_________________`\n",
    "\n",
    "---\n",
    "\n",
    "### ExercÃ­cio 3: Escalamento de Dados\n",
    "\n",
    "**Pergunta:** VocÃª PRECISA escalar dados para quais modelos?\n",
    "\n",
    "- [ ] Linear Regression\n",
    "- [ ] Logistic Regression  \n",
    "- [ ] Random Forest\n",
    "- [ ] XGBoost\n",
    "- [ ] SVM\n",
    "- [ ] Neural Network (MLP)\n",
    "\n",
    "**Dica:** Volte nos resultados acima e veja quais usamos `X_train_scaled`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc74e72",
   "metadata": {},
   "source": [
    "## ğŸ¯ 10. Respostas dos ExercÃ­cios\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘† Clique para ver as respostas (tente resolver primeiro!)</summary>\n",
    "\n",
    "### ExercÃ­cio 1: Escolha o Algoritmo\n",
    "\n",
    "**CenÃ¡rio A (500 amostras, interpretÃ¡vel):**  \n",
    "âœ… **Logistic Regression** - Pequeno dataset, precisa interpretar, tempo curto\n",
    "\n",
    "**CenÃ¡rio B (50k amostras, Kaggle):**  \n",
    "âœ… **XGBoost** - Muitos dados, precisa mÃ¡xima accuracy, tem tempo para tuning\n",
    "\n",
    "**CenÃ¡rio C (2k amostras, 100 features):**  \n",
    "âœ… **SVM** - Pequeno dataset com alta dimensionalidade, jÃ¡ escalado\n",
    "\n",
    "**CenÃ¡rio D (15k amostras, prototipagem):**  \n",
    "âœ… **Random Forest** - Bom resultado rÃ¡pido, robusto, nÃ£o precisa escalar\n",
    "\n",
    "---\n",
    "\n",
    "### ExercÃ­cio 2: Debug\n",
    "\n",
    "**Pergunta 1:** O modelo estÃ¡ com **OVERFITTING** - decorou o treino mas nÃ£o generaliza!\n",
    "\n",
    "**Pergunta 2:** **B) Aumentar complexidade do modelo** - Isso PIORA o overfitting!  \n",
    "As outras opÃ§Ãµes ajudam:\n",
    "- Cross-Validation detecta overfitting\n",
    "- Reduzir features diminui complexidade\n",
    "- Mais dados ajudam a generalizar\n",
    "\n",
    "---\n",
    "\n",
    "### ExercÃ­cio 3: Escalamento\n",
    "\n",
    "âœ… **PRECISA escalar:**\n",
    "- **SVM** - Muito sensÃ­vel a escala das features\n",
    "- **Neural Network (MLP)** - ConvergÃªncia mais rÃ¡pida e melhor com dados escalados\n",
    "\n",
    "âŒ **NÃƒO precisa escalar:**\n",
    "- Linear/Logistic Regression - Funciona, mas nÃ£o Ã© crÃ­tico\n",
    "- Random Forest - Baseado em Ã¡rvores (nÃ£o sensÃ­vel a escala)\n",
    "- XGBoost - Baseado em Ã¡rvores (nÃ£o sensÃ­vel a escala)\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Como vocÃª foi nos exercÃ­cios?**\n",
    "- 3/3 corretos: VocÃª domina! â­\n",
    "- 2/3 corretos: EstÃ¡ no caminho! ğŸ‘\n",
    "- 1/3 corretos: Revise a seÃ§Ã£o 8! ğŸ“š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d137f2f",
   "metadata": {},
   "source": [
    "## ğŸ“ 11. ConclusÃµes e PrÃ³ximos Passos\n",
    "\n",
    "### ğŸ“ O que VocÃª Aprendeu Hoje:\n",
    "\n",
    "1. âœ… **Comparou 5 algoritmos** lado-a-lado no mesmo dataset\n",
    "2. âœ… **Entendeu quando usar cada um** atravÃ©s de exemplos prÃ¡ticos\n",
    "3. âœ… **Viu feature importance** para modelos tree-based\n",
    "4. âœ… **Praticou decisÃµes reais** com exercÃ­cios de cenÃ¡rios\n",
    "5. âœ… **Aprendeu sobre overfitting** e como evitar\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Principais Insights:\n",
    "\n",
    "1. **NÃ£o existe \"melhor algoritmo\"** - depende do problema!\n",
    "2. **Random Forest Ã© sua primeira escolha** - bom resultado sem muito esforÃ§o\n",
    "3. **XGBoost para competiÃ§Ãµes** - mÃ¡xima accuracy com tuning\n",
    "4. **Sempre comece com baseline simples** (Logistic Regression)\n",
    "5. **SVM e Neural Networks PRECISAM de dados escalados!**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Seu Checklist Mental (Cole no seu monitor!):\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ESCOLHENDO MEU ALGORITMO ML                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                             â”‚\n",
    "â”‚  1. Quantos dados tenho?                    â”‚\n",
    "â”‚     < 1k   â†’ Logistic/SVM                   â”‚\n",
    "â”‚     1k-10k â†’ Random Forest                  â”‚\n",
    "â”‚     > 10k  â†’ XGBoost/Neural Network         â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚  2. Preciso interpretar?                    â”‚\n",
    "â”‚     Sim â†’ Linear/Logistic Regression        â”‚\n",
    "â”‚     NÃ£o â†’ Random Forest ou XGBoost          â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚  3. Quanto tempo tenho?                     â”‚\n",
    "â”‚     Pouco  â†’ Logistic/Random Forest         â”‚\n",
    "â”‚     Muito  â†’ XGBoost com tuning             â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚  4. Escalei os dados?                       â”‚\n",
    "â”‚     NÃ£o â†’ Random Forest ou XGBoost          â”‚\n",
    "â”‚     Sim â†’ Posso usar SVM ou Neural Network  â”‚\n",
    "â”‚                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š RevisÃ£o das Semanas 1-3:\n",
    "\n",
    "| Semana | Algoritmo Principal | Accuracy | Aprendizado-Chave |\n",
    "|--------|-------------------|----------|-------------------|\n",
    "| **Semana 1** | Linear Regression | 96.5% RÂ² | Fundamentos sÃ³lidos |\n",
    "| **Semana 2** | Logistic Regression | 79% | EDA + Feature Engineering |\n",
    "| **Semana 3** | XGBoost | 85.1% | Hyperparameter Tuning |\n",
    "| **Semana 4** | **Todos!** | - | **Quando usar cada um!** ğŸ¯ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ PrÃ³ximos Passos:\n",
    "\n",
    "- [ ] **Dia 2:** Feature Engineering na prÃ¡tica\n",
    "- [ ] **Dia 3:** Projeto prÃ¡tico com dados financeiros\n",
    "- [ ] Revisar este notebook quando tiver dÃºvida sobre qual algoritmo usar!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’­ ReflexÃ£o Final:\n",
    "\n",
    "*\"O melhor modelo nÃ£o Ã© o mais complexo, Ã© aquele que vocÃª entende profundamente e resolve seu problema de forma confiÃ¡vel.\"*\n",
    "\n",
    "**Perguntas para auto-reflexÃ£o:**\n",
    "1. Consigo explicar quando usar Random Forest vs XGBoost?\n",
    "2. Sei por que SVM precisa de dados escalados?\n",
    "3. Entendo o tradeoff entre interpretabilidade e accuracy?\n",
    "4. Sei como detectar overfitting nos meus modelos?\n",
    "\n",
    "Se respondeu SIM para 3+ perguntas, vocÃª estÃ¡ **pronto para o prÃ³ximo nÃ­vel!** ğŸ¯\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ ParabÃ©ns por completar este notebook de revisÃ£o!**  \n",
    "**Tempo estimado:** ~60-90 minutos  \n",
    "**PrÃ³ximo:** [Dia 2 - Feature Engineering](../docs/18-dia2-semana4-feature-engineering.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
